{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221ff96b1fbbb464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:27:59.814153400Z",
     "start_time": "2024-01-14T19:27:59.806397400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')\n",
    "parquet_path = os.path.join(local_path, 'parquet_scaling')\n",
    "result_folder_path = os.path.join(os.getcwd(), 'test_results')\n",
    "\n",
    "spark_rest_api_url = \"http://localhost:4040/api/v1/applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2416bbad8d173cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:27:59.983067200Z",
     "start_time": "2024-01-14T19:27:59.969621200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Choose time period for which to download the data\n",
    "start_date = datetime.strptime('2015-07-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2023-12-31', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28da7b24aeff3418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:28:01.479777200Z",
     "start_time": "2024-01-14T19:28:00.587029700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "url_list = []\n",
    "index = 0\n",
    "url_list.append([])\n",
    "month = date_list[0].month\n",
    "\n",
    "# Create a nested list containing a list of months with the corresponding download urls\n",
    "for date in date_list:\n",
    "    if date.month != month:\n",
    "        month = date.month\n",
    "        index += 1\n",
    "        url_list.append([])\n",
    "\n",
    "    # Create the url and append it to the month list\n",
    "    for x in range(0, 24):\n",
    "        for y in range(0, 60, 15):\n",
    "            date_tmp = date + timedelta(hours=x, minutes=y)\n",
    "            url = base_url + date_tmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "            url_list[index].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966a9a0250206cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:28:02.765753800Z",
     "start_time": "2024-01-14T19:28:02.742099400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directory for the data if it doesn't exist\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)\n",
    "\n",
    "if not os.path.isdir(parquet_path):\n",
    "    os.mkdir(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5837aac79f353d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:28:06.662591500Z",
     "start_time": "2024-01-14T19:28:02.949968300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a spark session (see config folder for spark config)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb63d2c2c61662b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:28:06.709048900Z",
     "start_time": "2024-01-14T19:28:06.660401600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define original data schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), True),\n",
    "    StructField(\"Day\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad726d18d2771288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:28:06.743627400Z",
     "start_time": "2024-01-14T19:28:06.700992600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    folder_location = os.path.join(local_path, fname[:4], fname[4:6])\n",
    "\n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(folder_location, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(folder_location, fname))\n",
    "\n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(folder_location, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(folder_location)\n",
    "\n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(folder_location, fname))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "cf453f8e79a0608a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred with file 20150713230000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20150804161500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20150809034500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151020010000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021041500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021043000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021044500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021050000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021051500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021053000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021060000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021054500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021061500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021063000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021064500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021070000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021071500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021073000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021074500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021081500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021080000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021083000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021084500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021090000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021093000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021094500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021091500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021100000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021101500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021104500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021103000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021113000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021114500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021110000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021120000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021111500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021121500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021123000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021124500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021130000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021131500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021140000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021133000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021134500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021143000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021141500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021144500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021153000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021150000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021154500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021160000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021151500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021161500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021164500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021163000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021170000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021171500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021173000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021180000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021174500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021181500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021184500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021183000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021190000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021194500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021191500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021201500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021203000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021193000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021200000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021204500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021211500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021210000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021213000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021214500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021220000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021221500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021223000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021224500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021230000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021231500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021234500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151021233000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022000000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022001500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022003000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022010000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022004500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022011500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022020000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022013000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022014500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022021500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022024500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022023000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022030000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022031500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022040000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022034500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022041500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022033000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022050000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022044500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022043000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022051500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022054500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022060000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022053000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022061500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022063000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022064500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022070000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022071500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022073000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022080000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022074500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022081500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022084500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022083000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022093000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022090000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022094500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022091500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022101500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022100000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022103000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022104500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022111500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022110000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022113000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022114500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022120000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022121500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022123000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022131500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022130000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022124500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022133000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022140000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022134500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022141500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022143000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022144500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022150000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022151500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022154500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022160000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022161500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022163000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022170000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022171500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022164500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022174500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022181500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022173000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022184500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022190000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022193000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022191500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022200000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022201500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022210000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022203000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022213000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022211500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022214500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022204500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022153000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022183000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151023213000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022194500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151022180000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151115211500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151118021500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151127053000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151129213000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151202230000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151203001500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151216230000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151217010000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151217011500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151226234500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151228024500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20151230023000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160107203000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160108014500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160112001500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160112014500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160122013000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160124234500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160129231500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160129233000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160312040000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160313034500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160330230000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160408234500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160412023000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160414014500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160421231500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160423103000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160425153000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160508013000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160508030000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160508020000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160620051500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160627021500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160628021500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160629023000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160629024500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160629030000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160629034500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160629031500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160711200000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160721011500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160728231500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20160729150000.export.CSV.zip: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download files and write them to parquet files in parallel for each month\n",
    "# This is done in batches to allow simple addition of new months to already existing data\n",
    "i = 0\n",
    "for month_list in url_list:\n",
    "    # Skip month if parquet file already exists\n",
    "    if os.path.exists(os.path.join(parquet_path, str(i) + \".parquet\")):\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    year_folder = os.path.join(local_path, month_list[0].split('/')[-1][:4])\n",
    "    month_folder = os.path.join(year_folder, month_list[0].split('/')[-1][4:6])\n",
    "\n",
    "    if not os.path.isdir(year_folder):\n",
    "        os.mkdir(year_folder)\n",
    "\n",
    "    if not os.path.isdir(month_folder):\n",
    "        os.mkdir(month_folder)\n",
    "\n",
    "    # Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_file, month_list)\n",
    "\n",
    "    # Read all csv files of one month into a spark dataframe\n",
    "    df = spark.read.csv(month_folder, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "\n",
    "    # Write the data of one month into a parquet file\n",
    "    df.write.parquet(os.path.join(parquet_path, str(i) + \".parquet\"), mode='overwrite')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    # Delete the csv files to free up disk space\n",
    "    shutil.rmtree(month_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9e285222dfa5b51",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Get the csv and parquet file sizes via the input/output bytes of the stages from the spark rest api\n",
    "result_file_path = os.path.join(result_folder_path, 'data_size.csv')\n",
    "\n",
    "if not os.path.isdir(result_folder_path):\n",
    "    os.mkdir(result_folder_path)\n",
    "\n",
    "# Fetch the list of applications to get the application id\n",
    "apps_response = requests.get(spark_rest_api_url)\n",
    "apps = apps_response.json()\n",
    "app_id = apps[0]['id']\n",
    "\n",
    "# Get stages information for the application (1 stage per parquet file write)\n",
    "stages_response = requests.get(f\"{spark_rest_api_url}/{app_id}/stages\")\n",
    "stages_data = stages_response.json()\n",
    "\n",
    "stage_result = {}\n",
    "\n",
    "# Get necessary information of each stage\n",
    "for stage in stages_data:\n",
    "    stage_result[stage['stageId']] = {\n",
    "        'status': stage['status'],\n",
    "        'input_data': stage['inputBytes'],\n",
    "        'output_data': stage['outputBytes']\n",
    "    }\n",
    "    \n",
    "df_result = pd.DataFrame.from_dict(stage_result, orient='index', columns=['status','input_data', 'output_data'])\n",
    "\n",
    "# Write the result to a csv file to use them later\n",
    "if not os.path.isfile(result_file_path):\n",
    "    df_result.to_csv(result_file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f9ccbf5639cd6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:28:12.742925800Z",
     "start_time": "2024-01-14T19:28:12.741288500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Method to run the non aggregated version of the program repeatedly in the test loop\n",
    "def run_non_aggregated(df_base):\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_non_aggregated = df_base.join(broadcast(df_mapping), df_base['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                     'left_outer')\n",
    "\n",
    "    df_non_aggregated = df_non_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_non_aggregated.cache()\n",
    "    df_non_aggregated.count()\n",
    "    df_non_aggregated.createOrReplaceGlobalTempView(\"GDELT\")\n",
    "    \n",
    "    return df_non_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33308ce9115c6ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:28:15.071712600Z",
     "start_time": "2024-01-14T19:28:15.067533800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to run the aggregated version of the program repeatedly in the test loop\n",
    "def run_aggregated(df_base):\n",
    "    # Select only relevant columns for the aggregation\n",
    "    df_selection = df_base.select(\n",
    "        F.col('Day'),\n",
    "        F.col('ActionGeo_CountryCode'),\n",
    "        F.col('GoldsteinScale')\n",
    "    )\n",
    "\n",
    "    # Remove rows that contain null values, which would distort the aggregation results \n",
    "    df_selection = df_selection.na.drop()\n",
    "    \n",
    "    # Aggregate the values by date and country so there is only one value per country per day\n",
    "    df_aggregated = df_selection.groupBy('Day', 'ActionGeo_CountryCode').agg(\n",
    "        F.sum('GoldsteinScale').alias('GoldsteinScaleSum'),\n",
    "        F.count('*').alias('EventCount')\n",
    "    )\n",
    "\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_aggregated = df_aggregated.join(broadcast(df_mapping),\n",
    "                                       df_aggregated['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                       'left_outer')\n",
    "\n",
    "    df_aggregated = df_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_aggregated.cache()\n",
    "    df_aggregated.count()\n",
    "    df_aggregated.createOrReplaceGlobalTempView(\"GDELT_AGGR\")\n",
    "    \n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "395334747b2bf948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:28:39.895587300Z",
     "start_time": "2024-01-14T19:28:39.836417800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Method to get the current cache information from the spark rest api\n",
    "def get_cache_information():\n",
    "    \n",
    "    # Fetch the list of applications to get the application id\n",
    "    apps_response = requests.get(spark_rest_api_url)\n",
    "    apps = apps_response.json()\n",
    "    app_id = apps[0]['id']\n",
    "    \n",
    "    # Get storage information for the application\n",
    "    storage_response = requests.get(f\"{spark_rest_api_url}/{app_id}/storage/rdd\")\n",
    "    storage_data = storage_response.json()\n",
    "    \n",
    "    # Only one dataframe is cached at a time, so only the first entry is relevant\n",
    "    return storage_data[0]['memoryUsed'], storage_data[0]['diskUsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8440c03e9b407fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:28:42.111978700Z",
     "start_time": "2024-01-14T19:28:41.850105Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "\n",
    "def average_sql_request_execution_time(sql, n=10):\n",
    "    with hive.connect(host='localhost', port=10000, username='spark') as connection:\n",
    "        request_start_time = time.time()\n",
    "\n",
    "        for _ in range(n):\n",
    "            pd.read_sql(sql=sql, con=connection)\n",
    "\n",
    "        request_end_time = time.time()\n",
    "\n",
    "    return (request_end_time - request_start_time) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93339d3044765fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:29:02.114610600Z",
     "start_time": "2024-01-14T19:28:58.555151200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d9be6de28b6c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:29:32.732373700Z",
     "start_time": "2024-01-14T19:29:32.719035300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of all parquet file paths\n",
    "parquet_path_list = []\n",
    "\n",
    "for i in range(0, len(os.listdir(parquet_path))):\n",
    "    parquet_path_list.append(os.path.join(parquet_path, str(i) + \".parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f61114596cbab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-14T19:29:35.435828600Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7676/3974149025.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql(sql=sql, con=connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: {'duration': 170.5504674911499, 'memory_usage': 5993606440, 'disk_usage': 1042432428, 'avg_sql_time': 3.129914879798889}}\n",
      "{6: {'duration': 170.5504674911499, 'memory_usage': 5993606440, 'disk_usage': 1042432428, 'avg_sql_time': 3.129914879798889}, 12: {'duration': 311.4626443386078, 'memory_usage': 5876227744, 'disk_usage': 6974715762, 'avg_sql_time': 6.637240314483643}}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "result_non_aggregated = {}\n",
    "\n",
    "# Run the non aggregated version of main logic (loading & pre-processing) in a loop to test the scalability of the program\n",
    "# With every run 6 more parquet files (each representing a month) are loaded and processed\n",
    "for i in range(6, len(os.listdir(parquet_path)) + 1, 6):\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read in the parquet files relevant for the current run\n",
    "    df = spark.read.parquet(*parquet_path_list[:i])\n",
    "    \n",
    "    # Run the non aggregated version of the main logic\n",
    "    df_non_aggregated = run_non_aggregated(df)\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate duration of the run\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Get cache information\n",
    "    memory_usage, disk_usage = get_cache_information()\n",
    "    \n",
    "    # Get the average execution time of the sql query used by superset to display the heatmap\n",
    "    avg_sql_time = average_sql_request_execution_time(\"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               AVG(GoldsteinScale) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT\n",
    "           WHERE ActionGeo_CountryCode IS NOT NULL\n",
    "             AND GoldsteinScale IS NOT NULL) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\")\n",
    "    \n",
    "    result_non_aggregated[i] = {\n",
    "        'duration': duration,\n",
    "        'memory_usage': memory_usage,\n",
    "        'disk_usage': disk_usage,\n",
    "        'avg_sql_time': avg_sql_time\n",
    "    }\n",
    "    \n",
    "    print(result_non_aggregated)\n",
    "    df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f880bf27194ef36",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_file_path = os.path.join(result_folder_path, 'test_result_non_aggregated.csv')\n",
    "\n",
    "df_result_non_aggregated = pd.DataFrame.from_dict(result_non_aggregated, orient='index', columns=['duration','memory_usage', 'disk_usage'])\n",
    "\n",
    "# Write the test result of the aggregated version to a csv file\n",
    "if not os.path.isfile(result_file_path):\n",
    "    df_result.to_csv(result_file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e6e395c751cd15e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "result_aggregated = {}\n",
    "\n",
    "# Same test run with the aggregated version of the loading & preprocessing logic\n",
    "for i in range(6, len(os.listdir(parquet_path)) + 1, 6):\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read in the parquet files relveant for the current run\n",
    "    df = spark.read.parquet(*parquet_path_list[:i])\n",
    "    \n",
    "    # Run the aggregated version of the main logic\n",
    "    df_aggregated = run_aggregated(df)\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate duration of the run\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Get cache information\n",
    "    memory_usage, disk_usage = get_cache_information()\n",
    "    \n",
    "    # Get the average execution time of the sql query used by superset to display the heatmap\n",
    "    avg_sql_time = average_sql_request_execution_time(\"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               SUM(GoldsteinScaleSum)/SUM(EventCount) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT_AGGR) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\")\n",
    "\n",
    "    \n",
    "    result_aggregated[i] = {\n",
    "        'duration': duration,\n",
    "        'memory_usage': memory_usage,\n",
    "        'disk_usage': disk_usage,\n",
    "        'avg_sql_time': avg_sql_time\n",
    "    }\n",
    "    \n",
    "    df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "id": "4523ebbb2d220097",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_file_path = os.path.join(result_folder_path, 'test_result_aggregated.csv')\n",
    "\n",
    "df_result_aggregated = pd.DataFrame.from_dict(result_aggregated, orient='index', columns=['duration','memory_usage', 'disk_usage'])\n",
    "\n",
    "# Write the test result of the aggregated version to a csv file\n",
    "if not os.path.isfile(result_file_path):\n",
    "    df_result.to_csv(result_file_path, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
