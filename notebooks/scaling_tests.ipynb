{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e300a42e26201cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:05.483081100Z",
     "start_time": "2024-01-14T10:41:05.461746500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')\n",
    "parquet_path = os.path.join(local_path, 'parquet_scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c3437f24a37734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:05.486836100Z",
     "start_time": "2024-01-14T10:41:05.464386200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Choose time period for which to download the data\n",
    "start_date = datetime.strptime('2015-03-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2023-12-31', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da92f98b4fd81836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:05.498352700Z",
     "start_time": "2024-01-14T10:41:05.482576700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "url_list = []\n",
    "index = 0\n",
    "url_list.append([])\n",
    "month = date_list[0].month\n",
    "\n",
    "# Create a nested list containing a list of months with the corresponding download urls\n",
    "for date in date_list:\n",
    "    if date.month != month:\n",
    "        month = date.month\n",
    "        index += 1\n",
    "        url_list.append([])\n",
    "\n",
    "    # Create the url and append it to the month list\n",
    "    for x in range(0, 24):\n",
    "        for y in range(0, 60, 15):\n",
    "            date_tmp = date + timedelta(hours=x, minutes=y)\n",
    "            url = base_url + date_tmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "            url_list[index].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e26abae53da1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:05.500463300Z",
     "start_time": "2024-01-14T10:41:05.496270100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directory for the data if it doesn't exist\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)\n",
    "\n",
    "if not os.path.isdir(parquet_path):\n",
    "    os.mkdir(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0378b19ae3e14b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:08.715303200Z",
     "start_time": "2024-01-14T10:41:05.503079200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a spark session (see config folder for spark config)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a0282333bb29ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:08.763849Z",
     "start_time": "2024-01-14T10:41:08.732294800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define original data schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), True),\n",
    "    StructField(\"Day\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21f4f9364f7a8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:08.765949600Z",
     "start_time": "2024-01-14T10:41:08.742306100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    folder_location = os.path.join(local_path, fname[:4], fname[4:6])\n",
    "\n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(folder_location, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(folder_location, fname))\n",
    "\n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(folder_location, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(folder_location)\n",
    "\n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(folder_location, fname))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1252f586fe05269c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:08.779954700Z",
     "start_time": "2024-01-14T10:41:08.763300300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download files and write them to parquet files in parallel for each month\n",
    "# This is done in batches to allow simple addition of new months to already existing data\n",
    "i = 0\n",
    "for month_list in url_list:\n",
    "    # Skip month if parquet file already exists\n",
    "    if os.path.exists(os.path.join(local_path, month_list[0].split('/')[-1][:6] + \".parquet\")):\n",
    "        continue\n",
    "\n",
    "    year_folder = os.path.join(local_path, month_list[0].split('/')[-1][:4])\n",
    "    month_folder = os.path.join(year_folder, month_list[0].split('/')[-1][4:6])\n",
    "\n",
    "    if not os.path.isdir(year_folder):\n",
    "        os.mkdir(year_folder)\n",
    "\n",
    "    if not os.path.isdir(month_folder):\n",
    "        os.mkdir(month_folder)\n",
    "\n",
    "    # Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_file, month_list)\n",
    "\n",
    "    # Read all csv files of one month into a spark dataframe\n",
    "    df = spark.read.csv(month_folder, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "\n",
    "    # Write the data of one month into a parquet file\n",
    "    df.write.parquet(os.path.join(parquet_path, i + \".parquet\"), mode='overwrite')\n",
    "    i += 1\n",
    "\n",
    "    # Delete the csv files to free up disk space\n",
    "    shutil.rmtree(month_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f783fa625d680d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:16.420077Z",
     "start_time": "2024-01-14T10:41:08.776137900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load all parquet files from the parquet folder into a multiple dataframes. the parquets are in the format 0.parquet, 1.parquet, ...\n",
    "# Each dataframe should contain 6 more parquets than the previous one so that the first has 6, the second 12, the third 18, ... until all parquets are loaded\n",
    "df_list = []\n",
    "parquet_path_list = []\n",
    "for i in range(0, len(os.listdir(parquet_path)), 6):\n",
    "    for d in range(0, 6):\n",
    "        parquet_path_list.append(os.path.join(parquet_path, str(i + d) + \".parquet\"))\n",
    "    df_list.append(spark.read.parquet(*parquet_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    # Do scaling tests here e.g. call method etc.\n",
    "    df.count()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1352d744deefa676"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86412e44e23053c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:19.750962400Z",
     "start_time": "2024-01-14T10:41:16.423735300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "# Load mapping file outside of spark (small dataset)\n",
    "df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True)\n",
    "\n",
    "df_mapping = df_mapping.select(\n",
    "    F.col('FIPS 10-4'),\n",
    "    F.col('ISO 3166-1')\n",
    ")\n",
    "\n",
    "# Map the country codes\n",
    "df_non_aggregated = df_base.join(broadcast(df_mapping), df_base['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                 'left_outer')\n",
    "\n",
    "df_non_aggregated = df_non_aggregated \\\n",
    "    .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "    .drop('ISO 3166-1') \\\n",
    "    .drop('FIPS 10-4') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3460f85c88c70e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:58.632766400Z",
     "start_time": "2024-01-14T10:41:19.751474300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events: 5915973\n"
     ]
    }
   ],
   "source": [
    "# Cache the dataframe to prevent re-doing data loading & country code mapping\n",
    "df_non_aggregated.cache()\n",
    "\n",
    "# Load data & trigger caching\n",
    "event_count = df_non_aggregated.count()\n",
    "\n",
    "print(\"Total number of events:\", event_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6adf06bed33507ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:41:59.640283Z",
     "start_time": "2024-01-14T10:41:58.631714500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 6, Status: SUCCEEDED, Duration: 0.147 seconds\n",
      "Job ID: 5, Status: SUCCEEDED, Duration: 0.424 seconds\n",
      "Job ID: 4, Status: SUCCEEDED, Duration: 36.843 seconds\n",
      "Job ID: 3, Status: SUCCEEDED, Duration: 0.881 seconds\n",
      "Job ID: 2, Status: SUCCEEDED, Duration: 1.467 seconds\n",
      "Job ID: 1, Status: SUCCEEDED, Duration: 0.437 seconds\n",
      "Job ID: 0, Status: SUCCEEDED, Duration: 0.815 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import dateutil.parser\n",
    "\n",
    "# Replace with your Spark driver node and port\n",
    "spark_rest_api_url = \"http://localhost:4040/api/v1/applications\"\n",
    "\n",
    "# Fetch the list of applications\n",
    "apps_response = requests.get(spark_rest_api_url)\n",
    "apps = apps_response.json()\n",
    "\n",
    "# Assuming you are interested in the first application\n",
    "app_id = apps[0]['id']\n",
    "\n",
    "# Get job information for the application\n",
    "jobs_response = requests.get(f\"{spark_rest_api_url}/{app_id}/jobs?status=succeeded\")\n",
    "jobs_data = jobs_response.json()\n",
    "\n",
    "# Print job information (or process it as needed)\n",
    "for job in jobs_data:\n",
    "    if 'submissionTime' in job and 'completionTime' in job and job['completionTime']:\n",
    "        start = dateutil.parser.parse(job['submissionTime'])\n",
    "        end = dateutil.parser.parse(job['completionTime'])\n",
    "        duration = (end - start).total_seconds()\n",
    "        print(f\"Job ID: {job['jobId']}, Status: {job['status']}, Duration: {duration} seconds\")\n",
    "    else:\n",
    "        print(f\"Job ID: {job['jobId']}, Status: {job['status']}, Duration: Not available\")\n",
    " # \n",
    " # # Get input data size from stages\n",
    " #    input_data_size = 0\n",
    " #    for stage_id in job['stageIds']:\n",
    " #        stage_response = requests.get(f\"{spark_rest_api_url}/{app_id}/stages/{stage_id}?status=succeeded\")\n",
    " #        stage_data = stage_response.json()\n",
    " # \n",
    " #        # Summing input data size from all attempts of this stage\n",
    " #        input_data_size += stage_data['inputBytes']\n",
    " # \n",
    " #    print(f\"Total Input Data Size for Job {job['jobId']}: {input_data_size} bytes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4b9d94bb354187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:51:37.114577200Z",
     "start_time": "2024-01-14T10:51:37.108667500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app-20240114104107-0007'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "196d41e654e1998c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T10:56:30.928066700Z",
     "start_time": "2024-01-14T10:56:30.917428600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache use: Memory: 1234299688, Disk: 0\n"
     ]
    }
   ],
   "source": [
    "storage_response = requests.get(f\"{spark_rest_api_url}/{app_id}/storage/rdd\")\n",
    "storage_data = jobs_response.json()\n",
    "for storage in storage_data:\n",
    "    print(f\"Cache use: Memory: {storage['memoryUsed']}, Disk: {storage['diskUsed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "163b9bf2af47bae5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-14T10:41:59.804187Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
