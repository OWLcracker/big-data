{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e300a42e26201cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:47:00.372388Z",
     "start_time": "2024-01-13T10:47:00.354208Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c3437f24a37734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:47:00.372763Z",
     "start_time": "2024-01-13T10:47:00.359461Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Choose time period for which to download the data\n",
    "start_date = datetime.strptime('2022-02-24', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2022-03-1', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da92f98b4fd81836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:47:00.374907Z",
     "start_time": "2024-01-13T10:47:00.362782Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "urllist = []\n",
    "index = 0\n",
    "\n",
    "for date in date_list:\n",
    "    # Create the url and append it to the list\n",
    "    for x in range(0, 24):\n",
    "        for y in range(0, 60, 15):\n",
    "            datetmp = date + timedelta(hours=x, minutes=y)\n",
    "            url = base_url + datetmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "            urllist.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e26abae53da1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:47:00.389898Z",
     "start_time": "2024-01-13T10:47:00.375090Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directory if it doesn't exist\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f21f4f9364f7a8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:47:00.390285Z",
     "start_time": "2024-01-13T10:47:00.379103Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "\n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(local_path, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(local_path, fname))\n",
    "\n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(local_path, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(os.path.join(local_path, fname[:4], fname[4:6]))\n",
    "\n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(local_path, fname))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce0378b19ae3e14b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:47:02.145379Z",
     "start_time": "2024-01-13T10:47:00.391677Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29a0282333bb29ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:47:02.162377Z",
     "start_time": "2024-01-13T10:47:02.152203Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define original data schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), True),\n",
    "    StructField(\"Day\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1252f586fe05269c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:43.707914Z",
     "start_time": "2024-01-13T10:47:02.161807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "monthlist = []\n",
    "month = urllist[0].split('/')[-1][:6]\n",
    "urllist.append(\"/\")\n",
    "for url in urllist:\n",
    "    if url.split('/')[-1][:6] != month:\n",
    "        if os.path.exists(os.path.join(local_path, month[:6]+\".parquet\")):\n",
    "            month = url.split('/')[-1][:6]\n",
    "            monthlist = []\n",
    "            continue\n",
    "        # download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(download_file, monthlist)\n",
    "        df = spark.read.csv(os.path.join(local_path, month[:4], month[4:6]), sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "        df.write.parquet(os.path.join(local_path, month[:6]+\".parquet\"), mode='overwrite')\n",
    "        shutil.rmtree(os.path.join(local_path, month[:4], month[4:6]))\n",
    "        month = url.split('/')[-1][:6]\n",
    "        monthlist = []\n",
    "    if os.path.exists(os.path.join(local_path, month[:6]+\".parquet\")):\n",
    "        continue\n",
    "    monthlist.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e436a590b3f515d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:43.898769Z",
     "start_time": "2024-01-13T10:48:43.709935Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date, CountryCode: string, GoldsteinScale: float]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load all csv files from the data directory into spark\n",
    "df_base = spark.read.parquet(local_path+'/*.parquet')\n",
    "\n",
    "# Select only relevant columns\n",
    "df_base = df_base.select(\n",
    "    F.col('Day').alias('Date'),\n",
    "    F.col('ActionGeo_CountryCode').alias('CountryCode'),\n",
    "    F.col('GoldsteinScale')\n",
    ")\n",
    "\n",
    "# Cache the base dataframe to prevent re-loading of the data in the following data analysis/cleaning steps\n",
    "df_base.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47409b8151da472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:45.268434Z",
     "start_time": "2024-01-13T10:48:43.899321Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events (before removing null values): 814106\n"
     ]
    }
   ],
   "source": [
    "# Total number of rows -> Triggers loading & caching of the data\n",
    "row_count = df_base.count()\n",
    "\n",
    "print(\"Total number of events (before removing null values):\", row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fe6514e1505dfbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:46.037259Z",
     "start_time": "2024-01-13T10:48:45.269315Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------------+\n",
      "|Date|CountryCode|GoldsteinScale|\n",
      "+----+-----------+--------------+\n",
      "|   0|      14639|             0|\n",
      "+----+-----------+--------------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    " \n",
    "# Number of null values in each column\n",
    "df_base.select([count(when(col(c).isNull(), c)).alias(c) for c in df_base.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a21fcd7000a45b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:46.507917Z",
     "start_time": "2024-01-13T10:48:46.038872Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events removed from dataset: 14639\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with any null value\n",
    "df_no_null = df_base.na.drop()\n",
    "\n",
    "# Number of rows not considering null values\n",
    "row_count_without_null = df_no_null.count()\n",
    "# \n",
    "print(\"Events removed from dataset:\", row_count - row_count_without_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8800d58fcf645fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:46.525139Z",
     "start_time": "2024-01-13T10:48:46.510565Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregate the values by date and country so there is only one value per country per day\n",
    "df_grouped = df_no_null.groupBy('Date', 'CountryCode').agg(\n",
    "    F.sum('GoldsteinScale').alias('GoldsteinScaleSum'),\n",
    "    F.count('*').alias('EventCount')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94554800e46d9b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:47.333212Z",
     "start_time": "2024-01-13T10:48:46.527384Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date, CountryCode: string, GoldsteinScaleSum: double, EventCount: bigint, CountryName: string, FIPS 10-4: string, ISO 3166-1: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "# Load mapping file\n",
    "df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True)\n",
    "\n",
    "# Map from FIPS10-4 country code to ISO 3166-1 alpha-2 country code\n",
    "df_joined = df_grouped.join(df_mapping, df_grouped['CountryCode'] == df_mapping['FIPS 10-4'], 'left_outer')\n",
    "\n",
    "# Cache agregated & joined dataframe to prevent re-evaluation of these steps\n",
    "df_joined.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e26b3bca10ae0a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:48.497894Z",
     "start_time": "2024-01-13T10:48:47.335667Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|CountryCode|EventCount|\n",
      "+-----------+----------+\n",
      "|         PF|         2|\n",
      "|         YI|         4|\n",
      "|         NT|         1|\n",
      "|         RB|       332|\n",
      "|         TE|        16|\n",
      "|         OS|      1094|\n",
      "|         OC|        28|\n",
      "+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "# Check for country codes where there is no coresponing ISO 3166-1 alpha-2 country code\n",
    "df_joined.filter(F.col('ISO 3166-1').isNull()) \\\n",
    "    .groupBy('CountryCode') \\\n",
    "    .agg(F.sum('EventCount').alias('EventCount')) \\\n",
    "    .show()\n",
    "\n",
    "# For example:\n",
    "# PF (Paracel Islands) -> no equivalent\n",
    "# NT (Netherlands Antilles) -> no equivalent \n",
    "# PG (Spratly Islands) -> no equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead2f0b17da0b57d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:48.546467Z",
     "start_time": "2024-01-13T10:48:48.492759Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date, GoldsteinScaleSum: double, EventCount: bigint, CountryName: string, CountryCode: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exchange country code columns\n",
    "df_joined = df_joined.drop('FIPS 10-4', 'CountryCode') \\\n",
    "    .withColumnRenamed('ISO 3166-1', 'CountryCode')\n",
    "\n",
    "# Remove rows with no corresponding country code\n",
    "df_joined_no_null = df_joined.na.drop()\n",
    "df_joined_no_null.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303e5e3c53fbd6b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:49.763912Z",
     "start_time": "2024-01-13T10:48:48.544092Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Virtual table which can be accessed by the thrift server\n",
    "df_joined_no_null.createOrReplaceGlobalTempView(\"GDELT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "255eff20511c9c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:49.869866Z",
     "start_time": "2024-01-13T10:48:49.761950Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date, GoldsteinScaleSum: double, EventCount: bigint, CountryName: string, CountryCode: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigger caching of the final dataframe\n",
    "df_joined_no_null.count()\n",
    "\n",
    "# Unpersist dataframes which are no longer needed\n",
    "df_base.unpersist()\n",
    "df_joined.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac5d604a854c50cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:49.933379Z",
     "start_time": "2024-01-13T10:48:49.870464Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf15600f848b5d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:49.938387Z",
     "start_time": "2024-01-13T10:48:49.934019Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
