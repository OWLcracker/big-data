{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3271cc46c6ac00b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GDELT Crisis Analysis\n",
    "\n",
    "The following notebook contains the code for the collection and pre-processing of data from the GDELT 2.0 Event Database by using Apache Spark. The resulting data is then provided via a Thrift Server to be accessed by Apache Superset for visualization and analysis. \n",
    "The example use case, which is implemented in this notebook, aims at visualizing global crises in a heatmap based on the average Goldstein Scale of the events, which occurred in the respective countries in a certain time period. As described in the provided documentation, the use case is implemented by following two different strategies:\n",
    "- **Non-aggregated**: The data is collected, pre-processed and provided without prior aggregation.\n",
    "- **Aggregated**: The data is collected, pre-processed and provided after aggregating the data by date and country.\n",
    "\n",
    "It's important to note that in this notebook the different approaches are not strictly seperated, but are rather built on top of each other to optimize the performance. The reason for this is that the notebook is only intended for demonstration purposes, showcasing the different steps involved. Nevertheless, during testing, the two approaches are implemented and executed in a separate manner, to accurately measure the characteristics of both approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47cd282055f14e4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "The data is collected by downloading the CSV files containing the events from the GDELT 2.0 Event Database. This is necessary because the Event Database doesn't offer the possibility to export a complete dump of the data.\n",
    "\n",
    "After the CSV files have been downloaded, they are then converted to Parquet files, to minimize the required storage space and to optimize the performance when reading the data into Apache Spark. \n",
    "\n",
    "The download and conversion process is executed in batches of months. This is done to minimize the storage space required at any given time, as every batch is compressed, which frees up storage space. Additionaly this allows for the simple addition of new months to already existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7c232210215cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:10.922526Z",
     "start_time": "2024-02-02T13:39:10.799144Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')\n",
    "parquet_path = os.path.join(local_path, 'parquet_main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470da507cf1f4066",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The time period for which the data should be downloaded is specified by the `start_date` and `end_date`.\n",
    "\n",
    "If you want to change the time period, make sure to include full months, because the month will be skipped, if the notebook is executed again and the parquet file for the month already exists (e.g. with only half of the data)\n",
    "\n",
    "We recommend keeping the default time period, to ensure fast execution of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272a874513fc2c10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:10.974786900Z",
     "start_time": "2024-02-02T13:39:10.807059200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Choose time period for which to download the data\n",
    "start_date = datetime.strptime('2021-12-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2021-12-31', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0993ec3598b0f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A list is created containing the download urls of all 15 minute interval CSV files for the specified time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0c757000cdfb85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:10.980011900Z",
     "start_time": "2024-02-02T13:39:10.812221Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "url_list = []\n",
    "index = 0\n",
    "url_list.append([])\n",
    "month = date_list[0].month\n",
    "\n",
    "# Create a nested list containing a list of months with the corresponding download urls\n",
    "for date in date_list:\n",
    "    if date.month != month:\n",
    "        month = date.month\n",
    "        index += 1\n",
    "        url_list.append([])\n",
    "\n",
    "    # Create the url and append it to the month list\n",
    "    for x in range(0, 24):\n",
    "        for y in range(0, 60, 15):\n",
    "            date_tmp = date + timedelta(hours=x, minutes=y)\n",
    "            url = base_url + date_tmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "            url_list[index].append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c3258134b8d47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Creation of directories where the data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30de261fe7016835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:10.981716Z",
     "start_time": "2024-02-02T13:39:10.871691800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directory for the data if it doesn't exist\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)\n",
    "    \n",
    "if not os.path.isdir(parquet_path):\n",
    "    os.mkdir(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceef3e61edc1596",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Intialization of the Spark Session to utilize Apache Spark for the conversion of the CSV files to Parquet files and later for the pre-processing of the data.\n",
    "\n",
    "Hive support is enabled to allow running the Thrift Server to provide the data to Apache Superset.\n",
    "\n",
    "The configuration for the Spark Session (e.g. the IP of the master) is stored in the `config` folder in the `spark_defaults.conf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f131781cb24636e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:14.653515700Z",
     "start_time": "2024-02-02T13:39:10.871691800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a spark session (see config folder for spark config)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2700c8221f97b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The complete schema for the GDELT event data is defined to ensure that the CSV files are read correctly into Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6faac416e73632b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:14.677988600Z",
     "start_time": "2024-02-02T13:39:14.652987100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define original data schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), True),\n",
    "    StructField(\"Day\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a053952a7752053",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following method is defined to download the CSV files from the GDELT website. \n",
    "\n",
    "Because the files are compressed, they are unzipped after being downloaded. The unzipped files are then deleted to free up storage space again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7381daf9ac12242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:14.826981400Z",
     "start_time": "2024-02-02T13:39:14.675059400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    folder_location = os.path.join(local_path, fname[:4], fname[4:6])\n",
    "\n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(folder_location, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(folder_location, fname))\n",
    "\n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(folder_location, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(folder_location)\n",
    "\n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(folder_location, fname))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb18a31ec387c00",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The data is downloaded in batches of months. \n",
    "\n",
    "For each month the list of urls is split and distirbuted to multiple threads, which download and unzip the individual files in parallel, to reduce the time required for the download process. \n",
    "\n",
    "If there is already an existing parquet file for a month, the month is skipped, to prevent downloading the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ca503bab6677a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:14.838056800Z",
     "start_time": "2024-02-02T13:39:14.711539900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download files and write them to parquet files in parallel for each month\n",
    "# This is done in batches to allow simple addition of new months to already existing data\n",
    "for month_list in url_list:\n",
    "    # Skip month if parquet file already exists\n",
    "    if os.path.exists(os.path.join(parquet_path, month_list[0].split('/')[-1][:6] + \".parquet\")):\n",
    "        continue\n",
    "\n",
    "    year_folder = os.path.join(local_path, month_list[0].split('/')[-1][:4])\n",
    "    month_folder = os.path.join(year_folder, month_list[0].split('/')[-1][4:6])\n",
    "\n",
    "    if not os.path.isdir(year_folder):\n",
    "        os.mkdir(year_folder)\n",
    "\n",
    "    if not os.path.isdir(month_folder):\n",
    "        os.mkdir(month_folder)\n",
    "\n",
    "    # Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_file, month_list)\n",
    "\n",
    "    # Read all csv files of one month into a spark dataframe\n",
    "    df = spark.read.csv(month_folder, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "\n",
    "    # Write the data of one month into a parquet file\n",
    "    df.write.parquet(os.path.join(parquet_path, month_list[0].split('/')[-1][:6] + \".parquet\"), mode='overwrite')\n",
    "\n",
    "    # Delete the csv files to free up disk space\n",
    "    shutil.rmtree(month_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8afc3812844c89",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Non-aggregated Approach\n",
    "\n",
    "The non-aggregated approach loads the data, pre-processes it and provides it to be used by Superset. \n",
    "\n",
    "The necessary aggregation of the avereage Goldstein Scale per country over a specific time period is triggered by the SQL queries, which are sent from Superset to the Thrift Server and are then processed by Spark. Therefore the aggregation is done on-demand and must be re-calculated every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623bbfd99596dc9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "All parquet files, which are stored in the `parquet_main` directory, are loaded into a Spark dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be03d2fe38895087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:22.332387700Z",
     "start_time": "2024-02-02T13:39:14.724477600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load all parquet files from the data directory into spark\n",
    "df_base = spark.read.parquet(parquet_path + '/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e52a038e8a8301",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The country codes in the GDELT data are chosen based on the FIPS10-4 standard. To be able to visualize the data in Apache Superset, the country codes need to be mapped to ISO 3166-1 alpha-2 country codes, because Superset uses these codes to identify the countries and display them on the heatmap.\n",
    "\n",
    "A CSV file containing the mapping is loaded into Spark. The country codes are then mapped by joining the GDELT dataframe with the mapping dataframe.\n",
    "\n",
    "To boost the performance of the expensive operation, the mapping dataframe is broadcasted, which means that the dataframe is copied to each worker node in the cluster. This is possible because the mapping dataframe is small in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7395e1c915ceaa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:28.130544800Z",
     "start_time": "2024-02-02T13:39:22.268894100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "# Load mapping file outside of spark (small dataset)\n",
    "df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "    F.col('FIPS 10-4'),\n",
    "    F.col('ISO 3166-1')\n",
    ")\n",
    "\n",
    "# Map the country codes\n",
    "df_non_aggregated = df_base.join(broadcast(df_mapping), df_base['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                 'left_outer')\n",
    "\n",
    "df_non_aggregated = df_non_aggregated \\\n",
    "    .withColumn('FIPS 10-4', F.col('ActionGeo_CountryCode')) \\\n",
    "    .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "    .drop('ISO 3166-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7fa7180aed4c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The dataframe is then cached to prevent re-doing the data loading and all of the previous operations. The data is primarily cached in memory, but spills over to disk if not enough memory space is available.\n",
    "\n",
    "Due to Spark's lazy evaluation, a count operation is executed to trigger the caching of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d837fcbb3d6d993c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:53.408856600Z",
     "start_time": "2024-02-02T13:39:28.061342700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events: 2954730\n"
     ]
    }
   ],
   "source": [
    "# Cache the dataframe to prevent re-doing data loading & country code mapping\n",
    "df_non_aggregated.cache()\n",
    "\n",
    "# Load data & trigger caching\n",
    "event_count = df_non_aggregated.count()\n",
    "\n",
    "print(\"Total number of events:\", event_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48894358253c18ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The resulting dataframe is registered as a global temporary view, which assigns the specified name to the logical plan, which describes how the dataframe is created. Because the underlying data is cached, the transformation steps in the logical plan don't have to be re-executed every time the view is accessed.\n",
    "\n",
    "Subsequently, SQL queries sent to the Thrift Server can access the cached dataframe by using the specified name, in this case \"global_temp.GDELT\".\n",
    "\n",
    "The data is not materialized on disk. Instead the data is kept in memory, which is why lifetime of the view is also bound to the lifetime of the Spark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0f9455da0fca36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:56.440205600Z",
     "start_time": "2024-02-02T13:39:53.402094600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Virtual table which can be accessed by the thrift server\n",
    "df_non_aggregated.createOrReplaceGlobalTempView(\"GDELT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c983a8c294855",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following code checks, for which FIPS 10-4 country codes in the dataset there is no corresponding ISO 3166-1 alpha-2 country code and counts how many events are affected by this.\n",
    "For example the following FIPS 10-4 country codes don't have a corresponding ISO 3166-1 alpha-2 country code:\n",
    "- PF (Paracel Islands)\n",
    "- NT (Netherlands Antilles)\n",
    "- PG (Spratly Islands)\n",
    "- ...\n",
    "\n",
    "The defined SQL query, which is sent to Spark to aggregate the data on-demand to visualize the result in Superset, ignores these events, as they can't be visualized on the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9243705817fd0a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:58.422320600Z",
     "start_time": "2024-02-02T13:39:56.445566500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|FIPS 10-4|EventCount|\n",
      "+---------+----------+\n",
      "|       OS|      1501|\n",
      "|       RB|      1300|\n",
      "|       OC|       321|\n",
      "|       YI|        48|\n",
      "|       WQ|        27|\n",
      "|       NT|         8|\n",
      "|       LQ|         7|\n",
      "|       PG|         5|\n",
      "|       JN|         1|\n",
      "|       PF|         1|\n",
      "|       CR|         1|\n",
      "|       JQ|         1|\n",
      "+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "# Check for country codes where there is no coresponing ISO 3166-1 alpha-2 country code\n",
    "df_non_aggregated.filter((F.col('ActionGeo_CountryCode').isNull()) & (F.col('FIPS 10-4').isNotNull())) \\\n",
    "    .groupBy('FIPS 10-4') \\\n",
    "    .agg(F.count('*').alias('EventCount')) \\\n",
    "    .sort('EventCount', ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4faa22033751c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Aggregated Approach\n",
    "\n",
    "The aggregated approach loads the data, selects only the relevant columns, pre-processes it and pre-aggregates it before providing it to be used by Superset.\n",
    "\n",
    "The pre-aggregation calculates the sum of the Goldstein Scale values and the number of events for each country per day, so the average can still be calculated for a specific time period dynamically. \n",
    "\n",
    "The caluclation of the average Goldstein Scale per country over a specific time period is then triggered by the SQL query, which is sent from Superset to the Thrift Server. Spark calculates the average Goldstein Scale by utilizing the pre-aggregated data, which reduces the amount of data that needs to be processed when the data is visualized in Superset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c00c66bb8e58b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Only the necessary columns are selected from the dataframe to reduce the amount of data that needs to be processed. These columns include the date, the country code and the Goldstein Scale value.\n",
    "\n",
    "This is done using the datframe, in which the country codes are already mapped to ISO 3166-1 alpha-2 country codes, to reduce the execution time of this notebook. In the tests, the mapping of country codes is done separately after the data has been aggregated already, to ensure that the performance of the two approaches can be compared accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db4398ea00d89ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:58.423407700Z",
     "start_time": "2024-02-02T13:39:58.345211800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select only relevant columns for the aggregation\n",
    "df_selection = df_non_aggregated.select(\n",
    "    F.col('Day'),\n",
    "    F.col('ActionGeo_CountryCode'),\n",
    "    F.col('GoldsteinScale')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55cb5fb8544364",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The number of null values in each of the relevant columns are shown to check if there are any null values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338d33aeb009bd03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:39:59.505834400Z",
     "start_time": "2024-02-02T13:39:58.373607100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+--------------+\n",
      "|Day|ActionGeo_CountryCode|GoldsteinScale|\n",
      "+---+---------------------+--------------+\n",
      "|  0|                88375|            49|\n",
      "+---+---------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Number of null values in each column\n",
    "df_selection.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_selection.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3a12a34e5e223",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The rows with null values are dropped, as they can't be used for the aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a81bddb0a27cd23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:40:00.100815600Z",
     "start_time": "2024-02-02T13:39:59.313087700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events removed from dataset: 88424\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with null values as they would distort the aggregation\n",
    "df_selection = df_selection.na.drop()\n",
    "\n",
    "# Number of rows not considering null values\n",
    "event_count_without_null = df_selection.count()\n",
    "\n",
    "print(\"Events removed from dataset:\", event_count - event_count_without_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461173a966ff95d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The sum of the Goldstein Scale values and the number of events for each country per day are calculated.\n",
    "\n",
    "This pre-aggregation allows the average Goldstein Scale to be calculated dynamically for a specific time period, while reducing the amount of data that needs to be processed significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d110fbe2ff08c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:40:00.248437800Z",
     "start_time": "2024-02-02T13:40:00.049808500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregate the values by date and country so there is only one value per country per day\n",
    "df_aggregated = df_selection.groupBy('Day', 'ActionGeo_CountryCode').agg(\n",
    "    F.sum('GoldsteinScale').alias('GoldsteinScaleSum'),\n",
    "    F.count('*').alias('EventCount')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46b21b2445ecb1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The result of the aggregation is then cached to prevent re-doing the loading, pre-processing and aggregation every time an SQL query is sent to the Thrift Server.\n",
    "\n",
    "Again, the caching is triggered by executing a count operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68572d50b83dc4f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:40:01.759207300Z",
     "start_time": "2024-02-02T13:40:00.101593100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in aggregation: 11268\n"
     ]
    }
   ],
   "source": [
    "# Cache the dataframe to prevent re-doing the aggregation\n",
    "df_aggregated.cache()\n",
    "\n",
    "# Trigger caching of the final aggregated dataframe\n",
    "aggregation_count = df_aggregated.count()\n",
    "\n",
    "print(\"Number of rows in aggregation:\", aggregation_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a806d010f56c4fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The resulting dataframe is also registered as a global temporary view, so it can be accessed through the Thrift Server by using the specified name \"global_temp.GDELT_AGGR\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dd3040ac7da4e53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:40:02.009613900Z",
     "start_time": "2024-02-02T13:40:01.760255300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Virtual table which can be accessed by the thrift server\n",
    "df_aggregated.createOrReplaceGlobalTempView(\"GDELT_AGGR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407bcdddd5bc354",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As last step, the Thrift Server is started. The Thrift Server provides an interface for the execution of SQL queries on the Spark data using JDBC/ODBC. Therefore the data in Spark can be accessed by clients (e.g. Apache Superset) using SQL queries. The queries are then processed by Spark, which acts as a distributed SQL query engine.\n",
    "\n",
    "Normally the Thrift Server would be started by running the `start-thriftserver.sh` script in the `sbin` directory of the Spark installation. For the given use case, this is not a suitable approach, because the Thrift Server would be started in a separate process, without access to the context of the Spark application. Thereby the Thrift Server wouldn't be able to access the unmaterilized datasets, which are registered as global temporary views.\n",
    "\n",
    "The following workaround to this problem has been identified and was implemented in this project. The Thrift Server is started by utilizing the Py4J library, which allows the execution of Java code from Python. The Java class of the Thrift Server `HiveThriftServer2` is imported and the main method is called using the JVM instance of the Spark context. Consequently, the Thrift Server is started in the same JVM as the Spark application and uses the same Spark context, which allows the Thrift Server to access the registered global temporary views. As a result the cached data can be accessed by clients using SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe2fef5cdf6dae5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T13:40:02.122081600Z",
     "start_time": "2024-02-02T13:40:01.788007700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
