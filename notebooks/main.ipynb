{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e300a42e26201cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:47.568496600Z",
     "start_time": "2024-01-07T10:44:47.558451300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c3437f24a37734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:47.594281200Z",
     "start_time": "2024-01-07T10:44:47.567398700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "start_date = datetime.strptime('2020-02-24', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2023-02-24', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date-start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da92f98b4fd81836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:47.599254800Z",
     "start_time": "2024-01-07T10:44:47.572890100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "urllist = []\n",
    "index = 0\n",
    "\n",
    "for date in date_list:\n",
    "    index += 1\n",
    "    # Set seed to get the same results with every execution\n",
    "    random.seed(1234 + index)\n",
    "    \n",
    "    # Get random number between 0 and 23\n",
    "    hours = random.randint(0,23)\n",
    "    \n",
    "    # Get random number between 1 and 4\n",
    "    minutes = random.randint(0,3)*15\n",
    "    \n",
    "    # Format the date\n",
    "    datetmp = date.replace(hour=hours, minute=minutes)\n",
    "    \n",
    "    # Replace result to date_list\n",
    "    date_list[date_list.index(date)] = datetmp\n",
    "    \n",
    "    # Create the url and append it to the list\n",
    "    url = base_url + datetmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "    urllist.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e26abae53da1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:47.618222Z",
     "start_time": "2024-01-07T10:44:47.596633100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directory if it doesn't exist\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f21f4f9364f7a8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:47.618759Z",
     "start_time": "2024-01-07T10:44:47.601937Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    \n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(local_path, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(local_path, fname))\n",
    "            \n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(local_path, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(local_path)\n",
    "                \n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(local_path, fname))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1252f586fe05269c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:46:18.662919300Z",
     "start_time": "2024-01-07T10:44:47.608605200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred with file 20200925030000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201004041500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201007010000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201015011500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201017041500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201016204500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201020014500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201021031500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201022170000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201024181500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201023224500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201025013000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201026220000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201028024500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201029191500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201027004500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201030043000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201102053000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201106070000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201107191500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201108061500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201109050000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201110131500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201112020000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201111020000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201113044500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201114114500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201116201500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20201117054500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20210731043000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20210803024500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20210806060000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20210809234500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20210810024500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20210807061500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20210811004500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20210814064500.export.CSV.zip: HTTP Error 404: Not Found\n",
      "An error occurred with file 20210813050000.export.CSV.zip: HTTP Error 404: Not Found\n",
      "File 20220626184500.export.CSV.zip already exists\n",
      "An error occurred with file 20221111020000.export.CSV.zip: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(download_file, urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0378b19ae3e14b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:46:22.080118100Z",
     "start_time": "2024-01-07T10:46:18.648408400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a0282333bb29ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:46:22.096192600Z",
     "start_time": "2024-01-07T10:46:22.093687400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define Schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GLOBALEVENTID\", IntegerType(), True),\n",
    "    StructField(\"SQLDATE\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e436a590b3f515d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:55:44.457322100Z",
     "start_time": "2024-01-07T10:55:38.084372300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(local_path, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "df = df.select('SQLDATE', 'GoldsteinScale', 'AvgTone', 'ActionGeo_CountryCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e904d30ae45c799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:55:44.678669900Z",
     "start_time": "2024-01-07T10:55:44.458954700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "# Load mapping file\n",
    "df_mapping = spark.read.csv(mapping_path, sep=';', header=True, inferSchema=True)\n",
    "\n",
    "# Map from FIPS10-4 country code to ISO 3166-1 alpha-2 country code\n",
    "df = df.join(df_mapping, df['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'], 'left_outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f3a57d118bf432a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:57:47.277356800Z",
     "start_time": "2024-01-07T10:57:40.276818200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ActionGeo_CountryCode|\n",
      "+---------------------+\n",
      "|                   PG|\n",
      "|                   RB|\n",
      "|                   OS|\n",
      "|                   OC|\n",
      "|                   PF|\n",
      "|                   YI|\n",
      "|                   WQ|\n",
      "|                   JQ|\n",
      "|                   LQ|\n",
      "|                 NULL|\n",
      "+---------------------+\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['ISO 3166-1'].isNull()).select(['ActionGeo_CountryCode']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8bae23070b4fe82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns and assign original name to new country code column\n",
    "df = df.drop('FIPS 10-4', 'ActionGeo_CountryCode').withColumnRenamed('ISO 3166-1', 'ActionGeo_CountryCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a444bab521d560e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:51:35.769166600Z",
     "start_time": "2024-01-07T10:51:35.761655300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "# null_values_df = df.filter(col('ISO 3166-1').isNull())\n",
    "# \n",
    "# null_values_df.select(['ActionGeo_CountryCode']).distinct().show()\n",
    "\n",
    "# from pyspark.sql.functions import col,isnan, when, count\n",
    "# df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "#    ).show()\n",
    "\n",
    "# WE & GZ (West Bank & Gaza Strip) -> PS (Palestine)\n",
    "# PF (Paracel Islands) -> no equivalent\n",
    "# NT (Netherlands Antilles) -> no equivalent \n",
    "# PG (Spratly Islands) -> no equivalent\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a21fcd7000a45b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:51:51.457331700Z",
     "start_time": "2024-01-07T10:51:35.764831700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null values: 45098\n"
     ]
    }
   ],
   "source": [
    "# TODO: See how many values are null per column\n",
    "\n",
    "row_count = df.count()\n",
    "\n",
    "# Remove rows with null values and show how many values were null per column\n",
    "df = df.na.drop()\n",
    "\n",
    "row_count_without_null = df.count()\n",
    "\n",
    "# Number fo rows with null values\n",
    "print(\"Rows with null values:\" ,row_count-row_count_without_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61264193bf1cf25f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:46:53.130175100Z",
     "start_time": "2024-01-07T10:46:53.081044900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Virtual table which can be accessed by the thrift server\n",
    "df.createOrReplaceTempView(\"GDELT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac5d604a854c50cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:46:55.314727Z",
     "start_time": "2024-01-07T10:46:53.128065900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c736fd60c6195809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:46:55.326603500Z",
     "start_time": "2024-01-07T10:46:55.315804100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
