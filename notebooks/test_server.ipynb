{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:33.962954500Z",
     "start_time": "2024-01-14T19:02:33.958854400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5aef0a70879774a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:33.974676300Z",
     "start_time": "2024-01-14T19:02:33.966137400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')\n",
    "parquet_path = os.path.join(local_path, 'parquet_main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2dd934e5146e98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:34.014392100Z",
     "start_time": "2024-01-14T19:02:33.970724800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Choose time period for which to download the data\n",
    "start_date = datetime.strptime('2021-12-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2021-12-31', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4754d20be51aadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:34.014917300Z",
     "start_time": "2024-01-14T19:02:33.989319600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "url_list = []\n",
    "index = 0\n",
    "url_list.append([])\n",
    "month = date_list[0].month\n",
    "\n",
    "# Create a nested list containing a list of months with the corresponding download urls\n",
    "for date in date_list:\n",
    "    if date.month != month:\n",
    "        month = date.month\n",
    "        index += 1\n",
    "        url_list.append([])\n",
    "\n",
    "    # Create the url and append it to the month list\n",
    "    for x in range(0, 24):\n",
    "        for y in range(0, 60, 15):\n",
    "            date_tmp = date + timedelta(hours=x, minutes=y)\n",
    "            url = base_url + date_tmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "            url_list[index].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64d1bf5c90ff9d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:34.015467600Z",
     "start_time": "2024-01-14T19:02:34.003947500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directory for the data if it doesn't exist\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)\n",
    "    \n",
    "if not os.path.isdir(parquet_path):\n",
    "    os.mkdir(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e2b8ed5dc46fcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:36.661077Z",
     "start_time": "2024-01-14T19:02:34.009966500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a spark session (see config folder for spark config)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e51f843fe7e61bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:36.672314Z",
     "start_time": "2024-01-14T19:02:36.661077Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define original data schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), True),\n",
    "    StructField(\"Day\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "505d9548366549c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:36.711286200Z",
     "start_time": "2024-01-14T19:02:36.664953700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    folder_location = os.path.join(local_path, fname[:4], fname[4:6])\n",
    "\n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(folder_location, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(folder_location, fname))\n",
    "\n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(folder_location, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(folder_location)\n",
    "\n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(folder_location, fname))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25b99bbf3facd31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:36.711884400Z",
     "start_time": "2024-01-14T19:02:36.691733600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download files and write them to parquet files in parallel for each month\n",
    "# This is done in batches to allow simple addition of new months to already existing data\n",
    "for month_list in url_list:\n",
    "    # Skip month if parquet file already exists\n",
    "    if os.path.exists(os.path.join(parquet_path, month_list[0].split('/')[-1][:6] + \".parquet\")):\n",
    "        continue\n",
    "\n",
    "    year_folder = os.path.join(local_path, month_list[0].split('/')[-1][:4])\n",
    "    month_folder = os.path.join(year_folder, month_list[0].split('/')[-1][4:6])\n",
    "\n",
    "    if not os.path.isdir(year_folder):\n",
    "        os.mkdir(year_folder)\n",
    "\n",
    "    if not os.path.isdir(month_folder):\n",
    "        os.mkdir(month_folder)\n",
    "\n",
    "    # Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_file, month_list)\n",
    "\n",
    "    # Read all csv files of one month into a spark dataframe\n",
    "    df = spark.read.csv(month_folder, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "\n",
    "    # Write the data of one month into a parquet file\n",
    "    df.write.parquet(os.path.join(parquet_path, month_list[0].split('/')[-1][:6] + \".parquet\"), mode='overwrite')\n",
    "\n",
    "    # Delete the csv files to free up disk space\n",
    "    shutil.rmtree(month_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64603f52be641635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:41.570100Z",
     "start_time": "2024-01-14T19:02:36.701900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load all parquet files from the data directory into spark\n",
    "df_base = spark.read.parquet(parquet_path + '/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8394cec2d46ee49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:02:45.295569Z",
     "start_time": "2024-01-14T19:02:41.566609500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "# Load mapping file outside of spark (small dataset)\n",
    "df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "    F.col('FIPS 10-4'),\n",
    "    F.col('ISO 3166-1')\n",
    ")\n",
    "\n",
    "# Map the country codes\n",
    "df_non_aggregated = df_base.join(broadcast(df_mapping), df_base['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                 'left_outer')\n",
    "\n",
    "df_non_aggregated = df_non_aggregated \\\n",
    "    .withColumn('FIPS 10-4', F.col('ActionGeo_CountryCode')) \\\n",
    "    .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "    .drop('ISO 3166-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9345cfdb9da0805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:03:07.343399200Z",
     "start_time": "2024-01-14T19:02:45.297118800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events: 2954730\n"
     ]
    }
   ],
   "source": [
    "# Cache the dataframe to prevent re-doing data loading & country code mapping\n",
    "df_non_aggregated.cache()\n",
    "\n",
    "# Load data & trigger caching\n",
    "event_count = df_non_aggregated.count()\n",
    "\n",
    "print(\"Total number of events:\", event_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dda5fc96add3189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:03:08.640044100Z",
     "start_time": "2024-01-14T19:03:07.340270200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|FIPS 10-4|EventCount|\n",
      "+---------+----------+\n",
      "|       OS|      1501|\n",
      "|       RB|      1300|\n",
      "|       OC|       321|\n",
      "|       YI|        48|\n",
      "|       WQ|        27|\n",
      "|       NT|         8|\n",
      "|       LQ|         7|\n",
      "|       PG|         5|\n",
      "|       CR|         1|\n",
      "|       JQ|         1|\n",
      "|       JN|         1|\n",
      "|       PF|         1|\n",
      "+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "# Check for country codes where there is no coresponing ISO 3166-1 alpha-2 country code\n",
    "df_non_aggregated.filter((F.col('ActionGeo_CountryCode').isNull()) & (F.col('FIPS 10-4').isNotNull())) \\\n",
    "    .groupBy('FIPS 10-4') \\\n",
    "    .agg(F.count('*').alias('EventCount')) \\\n",
    "    .sort('EventCount', ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "# For example:\n",
    "# PF (Paracel Islands) -> no equivalent\n",
    "# NT (Netherlands Antilles) -> no equivalent \n",
    "# PG (Spratly Islands) -> no equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24048b197a30b817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:03:08.655254700Z",
     "start_time": "2024-01-14T19:03:08.636008900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select only relevant columns for the aggregation\n",
    "df_selection = df_non_aggregated.select(\n",
    "    F.col('Day'),\n",
    "    F.col('ActionGeo_CountryCode'),\n",
    "    F.col('GoldsteinScale')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb32d15e42733af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:03:09.496614800Z",
     "start_time": "2024-01-14T19:03:08.653604800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+--------------+\n",
      "|Day|ActionGeo_CountryCode|GoldsteinScale|\n",
      "+---+---------------------+--------------+\n",
      "|  0|                88375|            49|\n",
      "+---+---------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Number of null values in each column\n",
    "df_selection.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_selection.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b711fdc581e885c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:03:10.272673700Z",
     "start_time": "2024-01-14T19:03:09.498187300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events removed from dataset: 88424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_selection = df_selection.na.drop()\n",
    "\n",
    "# Number of rows not considering null values\n",
    "event_count_without_null = df_selection.count()\n",
    "\n",
    "print(\"Events removed from dataset:\", event_count - event_count_without_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf579b7bcf7471f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:03:10.318917400Z",
     "start_time": "2024-01-14T19:03:10.264540600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregate the values by date and country so there is only one value per country per day\n",
    "df_aggregated = df_selection.groupBy('Day', 'ActionGeo_CountryCode').agg(\n",
    "    F.sum('GoldsteinScale').alias('GoldsteinScaleSum'),\n",
    "    F.count('*').alias('EventCount')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c421b80aee96a05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:03:11.467628900Z",
     "start_time": "2024-01-14T19:03:10.295175500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in aggregation: 11268\n"
     ]
    }
   ],
   "source": [
    "# Cache the dataframe to prevent re-doing the aggregation\n",
    "df_aggregated.cache()\n",
    "\n",
    "# Trigger caching of the final aggregated dataframe\n",
    "aggregation_count = df_aggregated.count()\n",
    "\n",
    "print(\"Number of rows in aggregation:\", aggregation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0429837781b739d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T19:03:35.262631300Z",
     "start_time": "2024-01-14T19:03:11.469744600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Virtual table which can be accessed by the thrift server\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_non_aggregated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplaceGlobalTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGDELT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_aggregated\u001b[38;5;241m.\u001b[39mcreateOrReplaceGlobalTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGDELT_AGGR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:480\u001b[0m, in \u001b[0;36mDataFrame.createOrReplaceGlobalTempView\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateOrReplaceGlobalTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates or replaces a global temporary view using the given name.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    The lifetime of this temporary view is tied to this Spark application.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m \n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplaceGlobalTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "# Virtual table which can be accessed by the thrift server\n",
    "df_non_aggregated.createOrReplaceGlobalTempView(\"GDELT\")\n",
    "df_aggregated.createOrReplaceGlobalTempView(\"GDELT_AGGR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b2eaa67032d84",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-14T19:03:35.260507200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f391a28133909f2a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-14T19:03:35.261568500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
