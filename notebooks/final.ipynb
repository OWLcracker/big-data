{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e300a42e26201cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:31.064550600Z",
     "start_time": "2024-01-06T20:39:31.033317200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import urllib.request\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c3437f24a37734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:31.084884100Z",
     "start_time": "2024-01-06T20:39:31.040835Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "start_date = datetime.strptime('2023-01-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2023-01-31', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date-start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da92f98b4fd81836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:31.087873Z",
     "start_time": "2024-01-06T20:39:31.074990Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "urllist = []\n",
    "index = 0\n",
    "\n",
    "for date in date_list:\n",
    "    index += 1\n",
    "    # Set seed to get the same results with every execution\n",
    "    random.seed(1234 + index)\n",
    "    \n",
    "    # Get random number between 0 and 23\n",
    "    hours = random.randint(0,23)\n",
    "    \n",
    "    # Get random number between 1 and 4\n",
    "    minutes = random.randint(0,3)*15\n",
    "    \n",
    "    # Format the date\n",
    "    datetmp = date.replace(hour=hours, minute=minutes)\n",
    "    \n",
    "    # Replace result to date_list\n",
    "    date_list[date_list.index(date)] = datetmp\n",
    "    \n",
    "    # Create the url and append it to the list\n",
    "    url = base_url + datetmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "    urllist.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e26abae53da1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:31.128284600Z",
     "start_time": "2024-01-06T20:39:31.092532100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directory if it doesn't exist\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f21f4f9364f7a8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:31.158499200Z",
     "start_time": "2024-01-06T20:39:31.103813900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    \n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(local_path, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(local_path, fname))\n",
    "            \n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(local_path, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(local_path)\n",
    "                \n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(local_path, fname))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1252f586fe05269c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:31.181828300Z",
     "start_time": "2024-01-06T20:39:31.153252400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20230101224500.export.CSV.zip already exists\n",
      "File 20230102174500.export.CSV.zip already exists\n",
      "File 20230103044500.export.CSV.zip already exists\n",
      "File 20230104004500.export.CSV.zip already exists\n",
      "File 20230105023000.export.CSV.zip already exists\n",
      "File 20230106204500.export.CSV.zip already exists\n",
      "File 20230109233000.export.CSV.zip already exists\n",
      "File 20230108023000.export.CSV.zip already exists\n",
      "File 20230107194500.export.CSV.zip already exists\n",
      "File 20230112050000.export.CSV.zip already exists\n",
      "File 20230111171500.export.CSV.zip already exists\n",
      "File 20230113030000.export.CSV.zip already exists\n",
      "File 20230110080000.export.CSV.zip already exists\n",
      "File 20230114181500.export.CSV.zip already exists\n",
      "File 20230115161500.export.CSV.zip already exists\n",
      "File 20230118094500.export.CSV.zip already exists\n",
      "File 20230116044500.export.CSV.zip already exists\n",
      "File 20230117220000.export.CSV.zip already exists\n",
      "File 20230119231500.export.CSV.zip already exists\n",
      "File 20230120131500.export.CSV.zip already exists\n",
      "File 20230121123000.export.CSV.zip already exists\n",
      "File 20230122170000.export.CSV.zip already exists\n",
      "File 20230123164500.export.CSV.zip already exists\n",
      "File 20230124043000.export.CSV.zip already exists\n",
      "File 20230126021500.export.CSV.zip already exists\n",
      "File 20230125121500.export.CSV.zip already exists\n",
      "File 20230127023000.export.CSV.zip already exists\n",
      "File 20230128063000.export.CSV.zip already exists\n",
      "File 20230129051500.export.CSV.zip already exists\n",
      "File 20230130034500.export.CSV.zip already exists\n",
      "File 20230131000000.export.CSV.zip already exists\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(download_file, urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0378b19ae3e14b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:34.235710Z",
     "start_time": "2024-01-06T20:39:31.166759200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a0282333bb29ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:34.252683700Z",
     "start_time": "2024-01-06T20:39:34.250024400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define Schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GLOBALEVENTID\", IntegerType(), True),\n",
    "    StructField(\"SQLDATE\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e436a590b3f515d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:35.953257100Z",
     "start_time": "2024-01-06T20:39:34.251499600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(local_path, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "df = df.select('SQLDATE', 'GoldsteinScale', 'AvgTone', 'ActionGeo_CountryCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e904d30ae45c799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:43.465783700Z",
     "start_time": "2024-01-06T20:39:35.954328800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Find country codes without counterpart\n",
    "\n",
    "mapping_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "# Load mapping file\n",
    "df_mapping = spark.read.csv(mapping_path, sep=';', header=True, inferSchema=True)\n",
    "\n",
    "# Map from FIPS10-4 country code to ISO 3166-1 alpha-2 country code\n",
    "df = df.join(df_mapping, df['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'], 'left_outer').drop('FIPS 10-4', 'ActionGeo_CountryCode')\n",
    "\n",
    "# Rename column\n",
    "df = df.withColumnRenamed('ISO 3166-1', 'ActionGeo_CountryCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21fcd7000a45b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:45.956883500Z",
     "start_time": "2024-01-06T20:39:43.468685100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null values: 1109\n"
     ]
    }
   ],
   "source": [
    "# TODO: See how many values are null per column\n",
    "\n",
    "row_count = df.count()\n",
    "\n",
    "# Remove rows with null values and show how many values were null per column\n",
    "df = df.na.drop()\n",
    "\n",
    "row_count_without_null = df.count()\n",
    "\n",
    "# Number fo rows with null values\n",
    "print(\"Rows with null values:\" ,row_count-row_count_without_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61264193bf1cf25f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:46.020174100Z",
     "start_time": "2024-01-06T20:39:45.943872200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Virtual table which can be accessed by the thrift server\n",
    "df.createOrReplaceTempView(\"GDELT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5d604a854c50cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:48.934920600Z",
     "start_time": "2024-01-06T20:39:46.008370700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c736fd60c6195809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T20:39:48.949299200Z",
     "start_time": "2024-01-06T20:39:48.935627200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
