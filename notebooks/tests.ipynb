{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cd59b29db59877",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tests\n",
    "\n",
    "In this notebook several tests are performed to analyze the scalability and fault tolerance of the application.\n",
    "\n",
    "To achieve comparable test results, all tests (if not stated otherwise) are executed on a local spark cluster with the following allocation of resources:\n",
    "\n",
    "| Item      | Resources        | Total Resources (in cluster) |\n",
    "|-----------|-------------------|-------------------------------|\n",
    "| Workers   | 3                 | 3                             |\n",
    "| Executors | 2 per Worker      | 6                             |\n",
    "| RAM         | 3 GB per Executor | 18 GB                         |\n",
    "| Cores     | 1 per Executor    | 6                             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "221ff96b1fbbb464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:13:36.837732500Z",
     "start_time": "2024-02-01T09:13:36.779698300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')\n",
    "parquet_path = os.path.join(local_path, 'parquet_test')\n",
    "# result_folder_path = os.path.join(os.getcwd(), 'test_results_new')\n",
    "# The following directory was used to store the test results, delivered with the project\n",
    "result_folder_path = os.path.join(os.getcwd(), 'test_results_delivered')\n",
    "\n",
    "spark_rest_api_url = \"http://localhost:4040/api/v1/applications\"\n",
    "\n",
    "# The same sql query, which is used by superset to display the heatmap (aggregated version)\n",
    "SQL_REQUEST_AGGREGATED = \"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               SUM(GoldsteinScaleSum)/SUM(EventCount) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT_AGGR) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\"\n",
    "\n",
    "# The same sql query, which is used by superset to display the heatmap (non-aggregated version)\n",
    "SQL_REQUEST_NON_AGGREGATED = \"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               AVG(GoldsteinScale) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT\n",
    "           WHERE ActionGeo_CountryCode IS NOT NULL\n",
    "             AND GoldsteinScale IS NOT NULL) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2416bbad8d173cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:43.606763900Z",
     "start_time": "2024-02-01T00:23:43.589098Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Choose time period for which to download the data\n",
    "start_date = datetime.strptime('2015-07-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2023-12-31', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28da7b24aeff3418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:45.791461600Z",
     "start_time": "2024-02-01T00:23:45.082099Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "url_list = []\n",
    "index = 0\n",
    "url_list.append([])\n",
    "month = date_list[0].month\n",
    "\n",
    "# Create a nested list containing a list of months with the corresponding download urls\n",
    "for date in date_list:\n",
    "    if date.month != month:\n",
    "        month = date.month\n",
    "        index += 1\n",
    "        url_list.append([])\n",
    "\n",
    "    # Create the url and append it to the month list\n",
    "    for x in range(0, 24):\n",
    "        for y in range(0, 60, 15):\n",
    "            date_tmp = date + timedelta(hours=x, minutes=y)\n",
    "            url = base_url + date_tmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "            url_list[index].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "966a9a0250206cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:13:41.623954100Z",
     "start_time": "2024-02-01T09:13:41.610532500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directories if they don't exist yet\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)\n",
    "\n",
    "if not os.path.isdir(parquet_path):\n",
    "    os.mkdir(parquet_path)\n",
    "    \n",
    "if not os.path.isdir(result_folder_path):\n",
    "    os.mkdir(result_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5837aac79f353d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:49.762954600Z",
     "start_time": "2024-02-01T00:23:46.242671900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a spark session (see config folder for spark config)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb63d2c2c61662b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:49.767462700Z",
     "start_time": "2024-02-01T00:23:49.724918700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define original data schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), True),\n",
    "    StructField(\"Day\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad726d18d2771288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:49.871629300Z",
     "start_time": "2024-02-01T00:23:49.744928700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    folder_location = os.path.join(local_path, fname[:4], fname[4:6])\n",
    "\n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(folder_location, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(folder_location, fname))\n",
    "\n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(folder_location, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(folder_location)\n",
    "\n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(folder_location, fname))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf453f8e79a0608a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:50.015264700Z",
     "start_time": "2024-02-01T00:23:49.772892500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download files and write them to parquet files in parallel for each month\n",
    "# This is done in batches to allow simple addition of new months to already existing data\n",
    "i = 0\n",
    "for month_list in url_list:\n",
    "    # Skip month if parquet file already exists\n",
    "    if os.path.exists(os.path.join(parquet_path, str(i) + \".parquet\")):\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    year_folder = os.path.join(local_path, month_list[0].split('/')[-1][:4])\n",
    "    month_folder = os.path.join(year_folder, month_list[0].split('/')[-1][4:6])\n",
    "\n",
    "    if not os.path.isdir(year_folder):\n",
    "        os.mkdir(year_folder)\n",
    "\n",
    "    if not os.path.isdir(month_folder):\n",
    "        os.mkdir(month_folder)\n",
    "\n",
    "    # Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_file, month_list)\n",
    "\n",
    "    # Read all csv files of one month into a spark dataframe\n",
    "    df = spark.read.csv(month_folder, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "\n",
    "    # Write the data of one month into a parquet file\n",
    "    df.write.parquet(os.path.join(parquet_path, str(i) + \".parquet\"), mode='overwrite')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    # Delete the csv files to free up disk space\n",
    "    shutil.rmtree(month_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e285222dfa5b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:53.320629300Z",
     "start_time": "2024-02-01T00:23:50.019996800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Get the csv and parquet file sizes via the input/output bytes of the stages from the spark rest api\n",
    "result_file_path = os.path.join(result_folder_path, 'data_size.csv')\n",
    "\n",
    "if not os.path.isdir(result_folder_path):\n",
    "    os.mkdir(result_folder_path)\n",
    "\n",
    "# Fetch the list of applications to get the application id\n",
    "apps_response = requests.get(spark_rest_api_url)\n",
    "apps = apps_response.json()\n",
    "app_id = apps[0]['id']\n",
    "\n",
    "# Get stages information for the application (1 stage per parquet file write)\n",
    "stages_response = requests.get(f\"{spark_rest_api_url}/{app_id}/stages\")\n",
    "stages_data = stages_response.json()\n",
    "\n",
    "stage_result = {}\n",
    "\n",
    "# Get necessary information of each stage\n",
    "for stage in stages_data:\n",
    "    stage_result[stage['stageId']] = {\n",
    "        'status': stage['status'],\n",
    "        'input_data': stage['inputBytes'],\n",
    "        'output_data': stage['outputBytes']\n",
    "    }\n",
    "    \n",
    "df_result = pd.DataFrame.from_dict(stage_result, orient='index')\n",
    "\n",
    "# Write the result to a csv file to use them later\n",
    "if not os.path.isfile(result_file_path):\n",
    "    df_result.to_csv(result_file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f9ccbf5639cd6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:53.347516900Z",
     "start_time": "2024-02-01T00:23:53.325363200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Method to run the non-aggregated version of the program repeatedly in the test loop\n",
    "def run_non_aggregated(df_base):\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_non_aggregated = df_base.join(broadcast(df_mapping), df_base['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                     'left_outer')\n",
    "\n",
    "    df_non_aggregated = df_non_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_non_aggregated.cache()\n",
    "    df_non_aggregated.count()\n",
    "    df_non_aggregated.createOrReplaceGlobalTempView(\"GDELT\")\n",
    "    \n",
    "    return df_non_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33308ce9115c6ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:53.348596300Z",
     "start_time": "2024-02-01T00:23:53.334154100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to run the aggregated version of the program repeatedly in the test loop\n",
    "def run_aggregated(df_base):\n",
    "    # Select only relevant columns for the aggregation\n",
    "    df_selection = df_base.select(\n",
    "        F.col('Day'),\n",
    "        F.col('ActionGeo_CountryCode'),\n",
    "        F.col('GoldsteinScale')\n",
    "    )\n",
    "\n",
    "    # Remove rows that contain null values, which would distort the aggregation results \n",
    "    df_selection = df_selection.na.drop()\n",
    "    \n",
    "    # Aggregate the values by date and country so there is only one value per country per day\n",
    "    df_aggregated = df_selection.groupBy('Day', 'ActionGeo_CountryCode').agg(\n",
    "        F.sum('GoldsteinScale').alias('GoldsteinScaleSum'),\n",
    "        F.count('*').alias('EventCount')\n",
    "    )\n",
    "\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_aggregated = df_aggregated.join(broadcast(df_mapping),\n",
    "                                       df_aggregated['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                       'left_outer')\n",
    "\n",
    "    df_aggregated = df_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_aggregated.cache()\n",
    "    df_aggregated.count()\n",
    "    df_aggregated.createOrReplaceGlobalTempView(\"GDELT_AGGR\")\n",
    "    \n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "395334747b2bf948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:53.358454600Z",
     "start_time": "2024-02-01T00:23:53.340084700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Method to get the current cache information from the spark rest api\n",
    "def get_cache_information():\n",
    "    \n",
    "    # Fetch the list of applications to get the application id\n",
    "    apps_response = requests.get(spark_rest_api_url)\n",
    "    apps = apps_response.json()\n",
    "    app_id = apps[0]['id']\n",
    "    \n",
    "    # Get storage information for the application\n",
    "    storage_response = requests.get(f\"{spark_rest_api_url}/{app_id}/storage/rdd\")\n",
    "    storage_data = storage_response.json()\n",
    "    \n",
    "    # Only one dataframe is cached at a time, so only the first entry is relevant\n",
    "    return storage_data[0]['memoryUsed'], storage_data[0]['diskUsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8440c03e9b407fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:53.386570400Z",
     "start_time": "2024-02-01T00:23:53.348060100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "\n",
    "def average_sql_request_response_time(sql, n=10):\n",
    "    # Create connection to thrift server\n",
    "    with hive.connect(host='localhost', port=10000, username='spark') as connection:\n",
    "        request_start_time = time.time()\n",
    "\n",
    "        for _ in range(n):\n",
    "            # Send request to thrift server\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(sql)\n",
    "            result = cursor.fetchall()\n",
    "\n",
    "        request_end_time = time.time()\n",
    "\n",
    "    return (request_end_time - request_start_time) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5089c9a753e4ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:53.387605900Z",
     "start_time": "2024-02-01T00:23:53.375429200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def write_result_to_csv(result_dict, file_name):\n",
    "    \n",
    "    result_file_path = os.path.join(result_folder_path, file_name)\n",
    "    \n",
    "    df_result = pd.DataFrame([result_dict])\n",
    "\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(result_file_path)\n",
    "    \n",
    "    # Write the test result to a csv file, append if file exists, create new if it doesn't\n",
    "    df_result.to_csv(result_file_path, sep=';', mode='a' if file_exists else 'w', header=not file_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93339d3044765fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:56.484006800Z",
     "start_time": "2024-02-01T00:23:53.379667Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc77b63692b1dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling Data Volume\n",
    "\n",
    "The following tests are performed to analyze how increasing the data volume affects the performance of the application.\n",
    "\n",
    "Both versions of the program (aggregated and non-aggregated) are repeatedly executed in a loop with an increasing amount of data. In each test run, performance metrics and cache information are collected and persisted to a csv file, to be used later for analysis.\n",
    "\n",
    "Contrary to `main.py` the aggregated and non-aggregated version are completely separated to test the individual performance of both versions.\n",
    "\n",
    "\n",
    "### Data Volume\n",
    "The data volume is increased by incrementing the number of parquet files (each representing a month of data) in every test run.\n",
    "\n",
    "There a are a total of 102 parquet files (8.5 years of data).\n",
    "\n",
    "Up to a full year of data (12 parquet files), tests are run in increments of 1 month (1 parquet file) to get a more granular view of the performance effects.\n",
    "Afterward, tests are run with increasing increments to analyze the performance effects at scale, while keeping the number of test runs low.\n",
    "The increment starts at 6 months and increases by 6 months with every test run (+6, +12, +18, ... parquet files), until the maximum of 8.5 years of data (102 parquet files) is reached.\n",
    "\n",
    "The tests are therefore conducted with the following numbers of parquet files: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 30, 48, 72, 102.\n",
    "\n",
    "### Performance\n",
    "The effect on the performance is analyzed by measuring different metrics:\n",
    "- **Pre-processing turnaround time**: The amount of time it takes to load the data into spark, conduct the necessary pre-processing & cache the results.\n",
    "- **Query response time**: The amount of time it takes to process and return the results of a single sql query, which is sent to the thrift server. An average of 10 sequential queries is calculated for more stable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1d9be6de28b6c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:23:56.488734700Z",
     "start_time": "2024-02-01T00:23:56.485033500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of all parquet file paths\n",
    "parquet_path_list = []\n",
    "\n",
    "# Number of parquet files in the directory\n",
    "for i in range(0, 102):\n",
    "    parquet_path_list.append(os.path.join(parquet_path, str(i) + \".parquet\"))\n",
    "\n",
    "parquet_file_test_cases = list(range(1, 13)) + [18, 30, 48, 72, 102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e6e395c751cd15e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T00:40:54.746169900Z",
     "start_time": "2024-02-01T00:23:56.490927400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 parquet files: Duration: 14.413100719451904s, memory usage: 212952B, disk usage: 0B, average response time: 0.2546949625015259s\n",
      "2 parquet files: Duration: 12.536877870559692s, memory usage: 392008B, disk usage: 0B, average response time: 0.25118160247802734s\n",
      "3 parquet files: Duration: 14.607834815979004s, memory usage: 570016B, disk usage: 0B, average response time: 0.2640686988830566s\n",
      "4 parquet files: Duration: 15.318191766738892s, memory usage: 739032B, disk usage: 0B, average response time: 0.15903708934783936s\n",
      "5 parquet files: Duration: 16.810499906539917s, memory usage: 902976B, disk usage: 0B, average response time: 0.13215315341949463s\n",
      "6 parquet files: Duration: 19.658199548721313s, memory usage: 1078560B, disk usage: 0B, average response time: 0.11190409660339355s\n",
      "7 parquet files: Duration: 23.625364780426025s, memory usage: 1233000B, disk usage: 0B, average response time: 0.1268925905227661s\n",
      "8 parquet files: Duration: 25.880151510238647s, memory usage: 1387568B, disk usage: 0B, average response time: 0.11033890247344971s\n",
      "9 parquet files: Duration: 31.155808448791504s, memory usage: 1557096B, disk usage: 0B, average response time: 0.11519355773925781s\n",
      "10 parquet files: Duration: 33.858447313308716s, memory usage: 1729096B, disk usage: 0B, average response time: 0.10074267387390137s\n",
      "11 parquet files: Duration: 36.80666637420654s, memory usage: 1911504B, disk usage: 0B, average response time: 0.12038040161132812s\n",
      "12 parquet files: Duration: 41.6138710975647s, memory usage: 2062864B, disk usage: 0B, average response time: 0.1038428783416748s\n",
      "18 parquet files: Duration: 61.616936922073364s, memory usage: 2726136B, disk usage: 0B, average response time: 0.10903618335723878s\n",
      "30 parquet files: Duration: 107.50221300125122s, memory usage: 4167952B, disk usage: 0B, average response time: 0.12674293518066407s\n",
      "48 parquet files: Duration: 117.27163290977478s, memory usage: 6259848B, disk usage: 0B, average response time: 0.12959938049316405s\n",
      "72 parquet files: Duration: 175.56011986732483s, memory usage: 9010864B, disk usage: 0B, average response time: 0.12892916202545165s\n",
      "102 parquet files: Duration: 244.02443599700928s, memory usage: 12413208B, disk usage: 0B, average response time: 0.12293198108673095s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "exception_occurred = False\n",
    "\n",
    "# Run the aggregated version of the main logic (data loading & pre-processing) in a loop with increasing data volume\n",
    "for i in parquet_file_test_cases:\n",
    "    if not exception_occurred:\n",
    "        try:    \n",
    "            # Start timer\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Read in the parquet files relevant for the current run\n",
    "            df = spark.read.parquet(*parquet_path_list[:i])\n",
    "                \n",
    "            # Run the aggregated version of the main logic\n",
    "            df_aggregated = run_aggregated(df)\n",
    "            \n",
    "            # Stop timer\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate duration of the run\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            # Get cache information\n",
    "            memory_usage, disk_usage = get_cache_information()\n",
    "            \n",
    "            # Get the average response time of an sql query\n",
    "            avg_req_time = average_sql_request_response_time(SQL_REQUEST_AGGREGATED)\n",
    "        \n",
    "            # Combine test results of the current run\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': i-1, # This is necessary to calculate the size of the data used for this test run\n",
    "                'duration': duration,\n",
    "                'memory_usage': memory_usage,\n",
    "                'disk_usage': disk_usage,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "            \n",
    "            print(f\"{i} parquet files: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "        \n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occurred = True\n",
    "    \n",
    "    if exception_occurred:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': i-1,\n",
    "            'duration': 0,\n",
    "            'memory_usage': 0,\n",
    "            'disk_usage': 0,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "        \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_data_volume_aggregated.csv')\n",
    "    \n",
    "    # Remove from cache to prevent interference with the next run\n",
    "    df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "188f61114596cbab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T02:55:17.000457700Z",
     "start_time": "2024-02-01T00:40:54.750575200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 parquet files: Duration: 34.120455265045166s, memory usage: 1273540248B, disk usage: 0B, average response time: 0.3191996574401855s\n",
      "2 parquet files: Duration: 49.46705436706543s, memory usage: 2475682816B, disk usage: 0B, average response time: 0.4143723964691162s\n",
      "3 parquet files: Duration: 61.652783155441284s, memory usage: 3727293664B, disk usage: 0B, average response time: 0.5487101793289184s\n",
      "4 parquet files: Duration: 77.62182712554932s, memory usage: 4952817680B, disk usage: 0B, average response time: 0.6659678459167481s\n",
      "5 parquet files: Duration: 96.44000601768494s, memory usage: 6191491184B, disk usage: 0B, average response time: 0.8035871982574463s\n",
      "6 parquet files: Duration: 113.14305400848389s, memory usage: 7367461568B, disk usage: 0B, average response time: 0.934669041633606s\n",
      "7 parquet files: Duration: 130.55651116371155s, memory usage: 8589660192B, disk usage: 0B, average response time: 1.0509275913238525s\n",
      "8 parquet files: Duration: 153.46874284744263s, memory usage: 9717203520B, disk usage: 107571978B, average response time: 1.6853981256484984s\n",
      "9 parquet files: Duration: 176.32402205467224s, memory usage: 9809501320B, disk usage: 1033099269B, average response time: 2.5936631679534914s\n",
      "10 parquet files: Duration: 197.99572944641113s, memory usage: 9757296768B, disk usage: 2057860727B, average response time: 3.4899200439453124s\n",
      "11 parquet files: Duration: 220.1136269569397s, memory usage: 9549503480B, disk usage: 3218142987B, average response time: 3.602662372589111s\n",
      "12 parquet files: Duration: 240.5781979560852s, memory usage: 9632860496B, disk usage: 4129818936B, average response time: 5.201870393753052s\n",
      "18 parquet files: Duration: 365.83278036117554s, memory usage: 9741825640B, disk usage: 9798293786B, average response time: 12.4692862033844s\n",
      "30 parquet files: Duration: 602.385217666626s, memory usage: 9900842376B, disk usage: 20139226367B, average response time: 21.72013363838196s\n",
      "48 parquet files: Duration: 875.6925694942474s, memory usage: 9842528848B, disk usage: 34501112560B, average response time: 30.005757880210876s\n",
      "72 parquet files: Duration: 1189.9346404075623s, memory usage: 9622167424B, disk usage: 49271762140B, average response time: 46.4117698431015s\n",
      "102 parquet files: Duration: 1530.859262228012s, memory usage: 9475923536B, disk usage: 65386724036B, average response time: 62.22336359024048s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "exception_occurred = False\n",
    "\n",
    "# Run the non-aggregated version of the main logic (data loading & pre-processing) in a loop with increasing data volume\n",
    "for i in parquet_file_test_cases:\n",
    "    if not exception_occurred:\n",
    "        try:\n",
    "            # Start timer\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Read in the parquet files relevant for the current run\n",
    "            df = spark.read.parquet(*parquet_path_list[:i])\n",
    "\n",
    "            # Run the non-aggregated version of the main logic\n",
    "            df_non_aggregated = run_non_aggregated(df)\n",
    "\n",
    "            # Stop timer\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate duration of the run\n",
    "            duration = end_time - start_time\n",
    "\n",
    "            # Get cache information\n",
    "            memory_usage, disk_usage = get_cache_information()\n",
    "\n",
    "            # Get the average response time of an sql query\n",
    "            avg_req_time = average_sql_request_response_time(SQL_REQUEST_NON_AGGREGATED)\n",
    "\n",
    "            # Combine test results of the current run\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': i - 1,  # This is necessary to calculate the size of the data used for this test run\n",
    "                'duration': duration,\n",
    "                'memory_usage': memory_usage,\n",
    "                'disk_usage': disk_usage,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "\n",
    "            print(f\"{i} parquet files: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "\n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occurred = True\n",
    "\n",
    "    if exception_occurred:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': i - 1,\n",
    "            'duration': 0,\n",
    "            'memory_usage': 0,\n",
    "            'disk_usage': 0,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "\n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_data_volume_non_aggregated.csv')\n",
    "\n",
    "    # Remove from cache to prevent interference with the next run\n",
    "    df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cb55f2702794b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling Load\n",
    "\n",
    "The following tests are performed to analyze how an increased load influences the performance of the application.\n",
    "\n",
    "In the given use case, an increasing amount of users using the dashboard at the same time or an increasing amount of dashboard elements (e.g. charts, tables, etc.) could be considered an increase in load. In both cases the number of concurrent queries, that are sent from superset to the thrift server, would increase. For that reason, we define load as the number of queries that are sent to the thrift server in parallel.\n",
    "\n",
    "Once the program is executed and the data is cached, an incrementing number of queries are sent to the thrift server in parallel. In each test run performance metrics are measured and persisted to a csv file. The tests are executed for both versions of the program (aggregated and non-aggregated) separately.\n",
    "\n",
    "\n",
    "### Load\n",
    "The number of queries sent to the thrift server in parallel is increased in increments of 10, starting at 10 up to 100. Afterward, it is increased in increments of 50, starting at 150 up to 500. Finally, the number of queries is increased in increments of 100, starting at 600 up to 1000.\n",
    "\n",
    "To simulate, that queries are sent in parallel (e.g. by different users), a separate thread is started for each query. Every thread establishes an isolated connection to the thrift server and sends a single query.\n",
    "\n",
    "\n",
    "### Performance\n",
    "The effect on the performance is analyzed by measuring the following metric:\n",
    "- **Average query response time**: The amount of time it takes to process and return the results of an sql query sent to the thrift server under the given load.\n",
    "\n",
    "### Data Volume\n",
    "The data volume is kept constant to test the effect of additional load in an isolated way. 1 parquet file (1 month of data) is used as a baseline for all test runs to ensure, that the cached data fits completely into the memory of the spark cluster and to reduce the runtime of the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbbb7b579b5dccb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T02:55:17.244616800Z",
     "start_time": "2024-02-01T02:55:16.992505200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import queue\n",
    "\n",
    "\n",
    "def run_query(sql, data_queue, stop_event):\n",
    "    # Send a single query to the thrift server and store the response time\n",
    "    try:\n",
    "        if not stop_event.is_set():\n",
    "            req_time = average_sql_request_response_time(sql, 1)\n",
    "            data_queue.put(req_time)\n",
    "    # Stop other threads, when an exception occurs and save the exception\n",
    "    except Exception as e:\n",
    "        data_queue.put(e)\n",
    "        stop_event.set()\n",
    "\n",
    "\n",
    "def run_queries_parallel(sql, n):\n",
    "    threads = []\n",
    "    data_queue = queue.Queue()\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    # Start n threads to run n queries in parallel\n",
    "    for _ in range(n):\n",
    "        t = threading.Thread(target=run_query, args=(sql, data_queue, stop_event))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    # Wait until all threads are finished\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    # Calculate the average response time\n",
    "    total = 0\n",
    "    while not data_queue.empty():\n",
    "        result = data_queue.get()\n",
    "        if isinstance(result, Exception):\n",
    "            # If an exception occurred in one of the threads, the test run should fail\n",
    "            raise result\n",
    "        total += result\n",
    "\n",
    "    return total / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78ea03ea4e916485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T02:55:17.245614700Z",
     "start_time": "2024-02-01T02:55:17.087155900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of parquet files used in the test\n",
    "files = 1\n",
    "# Number of queries sent in parallel\n",
    "query_test_cases = list(range(10, 101, 10)) + list(range(150, 501, 50)) + list(range(600, 1001, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b82c0191a3b5a0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T02:57:46.429502600Z",
     "start_time": "2024-02-01T02:55:17.115899200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time with 10 parallel queries: 0.21779217720031738s\n",
      "Average response time with 20 parallel queries: 0.36007262468338014s\n",
      "Average response time with 30 parallel queries: 0.29999205271402996s\n",
      "Average response time with 40 parallel queries: 0.4859184265136719s\n",
      "Average response time with 50 parallel queries: 0.5870481777191162s\n",
      "Average response time with 60 parallel queries: 0.7180324633916219s\n",
      "Average response time with 70 parallel queries: 0.6845898219517299s\n",
      "Average response time with 80 parallel queries: 0.7939853012561798s\n",
      "Average response time with 90 parallel queries: 1.063152543703715s\n",
      "Average response time with 100 parallel queries: 1.24916330575943s\n",
      "Average response time with 150 parallel queries: 1.6525200843811034s\n",
      "Average response time with 200 parallel queries: 2.004344297647476s\n",
      "Average response time with 250 parallel queries: 2.4741074285507203s\n",
      "Average response time with 300 parallel queries: 3.037146798769633s\n",
      "Average response time with 350 parallel queries: 4.2066350841522215s\n",
      "Average response time with 400 parallel queries: 4.725031383633613s\n",
      "Average response time with 450 parallel queries: 5.824349455833435s\n",
      "Average response time with 500 parallel queries: 5.758491439819336s\n",
      "An error occurred: unexpected exception\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Day: date, ActionGeo_CountryCode: string, GoldsteinScaleSum: double, EventCount: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the aggregated version of the program as measurement object for the load tests\n",
    "df = spark.read.parquet(*parquet_path_list[:files])\n",
    "df_aggregated = run_aggregated(df)\n",
    "\n",
    "exception_occurred = False\n",
    "\n",
    "for i in query_test_cases:\n",
    "    if not exception_occurred:\n",
    "        # Execute n queries in parallel and calculate the average response time\n",
    "        try:\n",
    "            avg_req_time = run_queries_parallel(SQL_REQUEST_AGGREGATED, i)\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': files-1,\n",
    "                'parallel_queries': i,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "            print(f\"Average response time with {i} parallel queries: {avg_req_time}s\")\n",
    "            \n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occurred = True\n",
    "    \n",
    "    if exception_occurred:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': files-1,\n",
    "            'parallel_queries': i,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "        \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_load_aggregated.csv')\n",
    "    \n",
    "df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4737bdd8e2ac1bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T03:11:07.473516100Z",
     "start_time": "2024-02-01T02:57:46.434782800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time with 10 parallel queries: 1.5785735368728637s\n",
      "Average response time with 20 parallel queries: 3.267963206768036s\n",
      "Average response time with 30 parallel queries: 4.547449461619059s\n",
      "Average response time with 40 parallel queries: 5.626710474491119s\n",
      "Average response time with 50 parallel queries: 6.816288423538208s\n",
      "Average response time with 60 parallel queries: 8.923525830109915s\n",
      "Average response time with 70 parallel queries: 12.24931754044124s\n",
      "Average response time with 80 parallel queries: 14.468654876947403s\n",
      "Average response time with 90 parallel queries: 17.621626607577006s\n",
      "Average response time with 100 parallel queries: 20.29620223760605s\n",
      "Average response time with 150 parallel queries: 38.75385047276815s\n",
      "Average response time with 200 parallel queries: 54.13535201430321s\n",
      "Average response time with 250 parallel queries: 65.63901019477844s\n",
      "Average response time with 300 parallel queries: 97.21682971000672s\n",
      "An error occurred: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: java.lang.OutOfMemoryError: Java heap space:36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:46', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor88:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:568', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'jdk.proxy2.$Proxy99:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:833', '*java.lang.OutOfMemoryError:Java heap space:0:-1'], sqlState=None, errorCode=0, errorMessage='Error running query: java.lang.OutOfMemoryError: Java heap space'), operationHandle=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[GlobalEventID: int, Day: date, MonthYear: int, Year: int, FractionDate: float, Actor1Code: string, Actor1Name: string, Actor1CountryCode: string, Actor1KnownGroupCode: string, Actor1EthnicCode: string, Actor1Religion1Code: string, Actor1Religion2Code: string, Actor1Type1Code: string, Actor1Type2Code: string, Actor1Type3Code: string, Actor2Code: string, Actor2Name: string, Actor2CountryCode: string, Actor2KnownGroupCode: string, Actor2EthnicCode: string, Actor2Religion1Code: string, Actor2Religion2Code: string, Actor2Type1Code: string, Actor2Type2Code: string, Actor2Type3Code: string, IsRootEvent: int, EventCode: string, EventBaseCode: string, EventRootCode: string, QuadClass: int, GoldsteinScale: float, NumMentions: int, NumSources: int, NumArticles: int, AvgTone: float, Actor1Geo_Type: int, Actor1Geo_FullName: string, Actor1Geo_CountryCode: string, Actor1Geo_ADM1Code: string, Actor1Geo_ADM2Code: string, Actor1Geo_Lat: float, Actor1Geo_Long: float, Actor1Geo_FeatureID: string, Actor2Geo_Type: int, Actor2Geo_FullName: string, Actor2Geo_CountryCode: string, Actor2Geo_ADM1Code: string, Actor2Geo_ADM2Code: string, Actor2Geo_Lat: float, Actor2Geo_Long: float, Actor2Geo_FeatureID: string, ActionGeo_Type: int, ActionGeo_FullName: string, ActionGeo_CountryCode: string, ActionGeo_ADM1Code: string, ActionGeo_ADM2Code: string, ActionGeo_Lat: float, ActionGeo_Long: float, ActionGeo_FeatureID: string, DATEADDED: string, SOURCEURL: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the non-aggregated version of the program as measurement objects for the load tests\n",
    "df = spark.read.parquet(*parquet_path_list[:files])\n",
    "df_non_aggregated = run_non_aggregated(df)\n",
    "\n",
    "exception_occurred = False\n",
    "\n",
    "for i in query_test_cases:\n",
    "    if not exception_occurred:\n",
    "        # Execute n queries in parallel and calculate the average response time\n",
    "        try:\n",
    "            avg_req_time = run_queries_parallel(SQL_REQUEST_NON_AGGREGATED, i)\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': files-1,\n",
    "                'parallel_queries': i,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "            print(f\"Average response time with {i} parallel queries: {avg_req_time}s\")\n",
    "            \n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occurred = True\n",
    "    \n",
    "    if exception_occurred:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': files-1,\n",
    "            'parallel_queries': i,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "            \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_load_non_aggregated.csv')\n",
    "\n",
    "df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96620b51c64648",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling Resources\n",
    "\n",
    "The following tests are performed to analyze how increasing the resources of the spark cluster influences the performance of the application.\n",
    "\n",
    "In the given case, the relevant resources are the number of CPU cores and the amount of RAM, which the spark cluster can use to process & cache the data.\n",
    "\n",
    "Scaling resources in the big data context is usually achieved by adding additional nodes consisting of commodity hardware to a cluster (horizontal scaling). To simulate the effect of adding additional nodes to the cluster, both versions of the program (aggregated and non-aggregated) are executed with an increasing number of worker nodes. In each test run, performance metrics and cache information are collected and persisted to a csv file.\n",
    "\n",
    "### Resources\n",
    "The resources are scaled by manually changing the number of worker nodes in `docker-compose.yaml` and adjusting the total number of executors in the cluster accordingly in `spark-defaults.conf`. After changing the configuration the setup is restarted and the tests are executed again with the adapted spark cluster.\n",
    " \n",
    "The tests are conducted with the following resource configurations:\n",
    "\n",
    "\n",
    "| No. Workers               | Total No. Executors (in cluster) | Total RAM (in cluster) | Total No. Cores (in cluster)       |\n",
    "|---------------------------|----------------------------------|------------------------------|------------------------------------|\n",
    "| 1                         | 2                                | 6 GB                         | 2                                  |\n",
    "| 2                         | 4                                | 12 GB                        | 4                                  |\n",
    "| 3                         | 6                                | 18 GB                        | 6                                  |\n",
    "\n",
    "### Performance\n",
    "The measured performance metric are the same as in the data volume scalability tests:\n",
    "- **Pre-processing turnaround time**\n",
    "- **Query response time**\n",
    "\n",
    "### Data Volume\n",
    "The data volume is kept constant to test the effect of adding additional resources in an isolated way.\n",
    " \n",
    "For both versions of the program, a different number of parquet files are included in the test runs, so the amount of included data is aligned with the characteristics of the different versions. This is necessary to ensure that the pre-processed data is partitioned and cached across all nodes, so it can be processed in parallel by the executors (this is not the case for 1 parquet file). Furthermore the resulting data must fit into memory, so the result is not distorted by the additional overhead of swapping data between memory and disk.\n",
    "\n",
    "Therefore, the following number of parquet files are used in the test runs:\n",
    "- **Aggregated version**: 6 parquet files (6 months of data) are used, so the cached result of the pre-processing is partitioned across all nodes.\n",
    "- **Non-aggregated version**: 2 parquet files (2 months of data) are used because it's the maximum amount which fits into memory of 1 worker node with 6 GB of RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2dbc683cb287043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T03:11:07.477140400Z",
     "start_time": "2024-02-01T03:11:07.474044600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Manually adjusted every run\n",
    "workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbf2ca6450e4c906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T03:11:29.180906300Z",
     "start_time": "2024-02-01T03:11:07.490085500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 workers: Duration: 20.550599336624146s, memory usage: 1078536B, disk usage: 0B, average response time: 0.10387980937957764s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Day: date, ActionGeo_CountryCode: string, GoldsteinScaleSum: double, EventCount: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# Number of parquet files used in the test\n",
    "files = 6\n",
    "\n",
    "# Run the aggregated version of the main logic (data loading & pre-processing) with the current number of workers\n",
    "try:\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read in the parquet files\n",
    "    df = spark.read.parquet(*parquet_path_list[:files])\n",
    "\n",
    "    # Run the aggregated version of the main logic\n",
    "    df_aggregated = run_aggregated(df)\n",
    "\n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate duration of the run\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    # Get cache information\n",
    "    memory_usage, disk_usage = get_cache_information()\n",
    "\n",
    "    # Get the average response time of an sql query\n",
    "    avg_req_time = average_sql_request_response_time(SQL_REQUEST_AGGREGATED)\n",
    "\n",
    "    # Combine test results of the current run\n",
    "    result = {\n",
    "        'status': 'COMPLETE',\n",
    "        'workers': workers,\n",
    "        'last_file_index': files,\n",
    "        'duration': duration,\n",
    "        'memory_usage': memory_usage,\n",
    "        'disk_usage': disk_usage,\n",
    "        'avg_req_time': avg_req_time\n",
    "    }\n",
    "\n",
    "    print(f\"{workers} workers: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "\n",
    "# If an exception occurs, the test run should fail \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    result = {\n",
    "        'status': 'FAILED',\n",
    "        'workers': workers,\n",
    "        'last_file_index': files,\n",
    "        'duration': 0,\n",
    "        'memory_usage': 0,\n",
    "        'disk_usage': 0,\n",
    "        'avg_req_time': 0\n",
    "    }\n",
    "\n",
    "# Test results are persisted every run to avoid losing them in case of a crash\n",
    "write_result_to_csv(result, 'test_resources_aggregated.csv')\n",
    "\n",
    "# Remove from cache to prevent interference with the next run\n",
    "df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83df5be09dea548f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T03:12:20.332345500Z",
     "start_time": "2024-02-01T03:11:29.172568600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 workers: Duration: 47.30111622810364s, memory usage: 2475682816B, disk usage: 0B, average response time: 0.3795547723770142s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[GlobalEventID: int, Day: date, MonthYear: int, Year: int, FractionDate: float, Actor1Code: string, Actor1Name: string, Actor1CountryCode: string, Actor1KnownGroupCode: string, Actor1EthnicCode: string, Actor1Religion1Code: string, Actor1Religion2Code: string, Actor1Type1Code: string, Actor1Type2Code: string, Actor1Type3Code: string, Actor2Code: string, Actor2Name: string, Actor2CountryCode: string, Actor2KnownGroupCode: string, Actor2EthnicCode: string, Actor2Religion1Code: string, Actor2Religion2Code: string, Actor2Type1Code: string, Actor2Type2Code: string, Actor2Type3Code: string, IsRootEvent: int, EventCode: string, EventBaseCode: string, EventRootCode: string, QuadClass: int, GoldsteinScale: float, NumMentions: int, NumSources: int, NumArticles: int, AvgTone: float, Actor1Geo_Type: int, Actor1Geo_FullName: string, Actor1Geo_CountryCode: string, Actor1Geo_ADM1Code: string, Actor1Geo_ADM2Code: string, Actor1Geo_Lat: float, Actor1Geo_Long: float, Actor1Geo_FeatureID: string, Actor2Geo_Type: int, Actor2Geo_FullName: string, Actor2Geo_CountryCode: string, Actor2Geo_ADM1Code: string, Actor2Geo_ADM2Code: string, Actor2Geo_Lat: float, Actor2Geo_Long: float, Actor2Geo_FeatureID: string, ActionGeo_Type: int, ActionGeo_FullName: string, ActionGeo_CountryCode: string, ActionGeo_ADM1Code: string, ActionGeo_ADM2Code: string, ActionGeo_Lat: float, ActionGeo_Long: float, ActionGeo_FeatureID: string, DATEADDED: string, SOURCEURL: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# Number of parquet files used in the test\n",
    "files = 2\n",
    "\n",
    "# Run the non-aggregated version of the main logic (data loading & pre-processing) with the current number of workers\n",
    "try:\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read in the parquet files\n",
    "    df = spark.read.parquet(*parquet_path_list[:files])\n",
    "\n",
    "    # Run the non-aggregated version of the main logic\n",
    "    df_non_aggregated = run_non_aggregated(df)\n",
    "\n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate duration of the run\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    # Get cache information\n",
    "    memory_usage, disk_usage = get_cache_information()\n",
    "\n",
    "    # Get the average response time of an sql query\n",
    "    avg_req_time = average_sql_request_response_time(SQL_REQUEST_NON_AGGREGATED)\n",
    "\n",
    "    # Combine test results of the current run\n",
    "    result = {\n",
    "        'status': 'COMPLETE',\n",
    "        'workers': workers,\n",
    "        'last_file_index': files,\n",
    "        'duration': duration,\n",
    "        'memory_usage': memory_usage,\n",
    "        'disk_usage': disk_usage,\n",
    "        'avg_req_time': avg_req_time\n",
    "    }\n",
    "\n",
    "    print(f\"{workers} workers: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "\n",
    "# If an exception occurs, the test run should fail \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    result = {\n",
    "        'status': 'FAILED',\n",
    "        'workers': workers,\n",
    "        'last_file_index': files,\n",
    "        'duration': 0,\n",
    "        'memory_usage': 0,\n",
    "        'disk_usage': 0,\n",
    "        'avg_req_time': 0\n",
    "    }\n",
    "\n",
    "# Test results are persisted every run to avoid losing them in case of a crash\n",
    "write_result_to_csv(result, 'test_resources_non_aggregated.csv')\n",
    "\n",
    "# Remove from cache to prevent interference with the next run\n",
    "df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0dcb5c053ded31",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fault Tolerance\n",
    "\n",
    "The following tests are performed to analyze how the application behaves in case of a failure and how it affects the performance.\n",
    "\n",
    "In the given case, a failure could be caused by a node in the spark cluster failing or by a network partition which prevents the nodes from communicating with each other, effectively making the node unavailable to be used in the cluster. To simulate a node failure, both versions of the program (aggregated and non-aggregated) are executed until the data is cached and then one or more nodes are stopped, so the cached partitions on these nodes are lost and the data has to be re-loaded from disk and re-calculated once it's needed. To trigger this, an sql request is sent to the thrift server, which contains an aggregation that requires processing of the complete dataset. In each test run, performance metrics are collected and persisted to a csv file.\n",
    "\n",
    "### Failures\n",
    "The failure of nodes is simulated by manually stopping the worker nodes in docker once the data has been cached. The following failure scenarios are tested:\n",
    "- **0 Nodes fail**: No nodes are stopped after the data is cached.\n",
    "- **1 Node fails**: One node is stopped after the data is cached.\n",
    "- **2 Nodes fail**: Two nodes are stopped after the data is cached.\n",
    "\n",
    "After every test run, the setup is restarted and the tests are executed again with the next failure scenario.\n",
    "\n",
    "### Performance\n",
    "The effect on the performance is analyzed by measuring the following metric:\n",
    "- **Query response time**: The amount of time it takes to process and return the results of an sql query under the condition that one or more nodes are unavailable.\n",
    "\n",
    "### Data Volume\n",
    "The data volume is kept constant to test the effect of a node failure in an isolated way. \n",
    "\n",
    "The amount of data must is chosen carefully, to ensure that the result of the pre-processing is partitioned and cached across all nodes, so the failure of a single node leads to an actuaL loss of cached data. Furthermore the resulting data must fit into the memory of the remaining nodes, so the result is not distorted by the additional overhead of swapping data between memory and disk.\n",
    "\n",
    "Therefore, the number of parquet files used in the tests are the same as in the resource scalability tests:\n",
    "- **Aggregated version**: 6 parquet files (6 month of data)\n",
    "- **Non-aggregated version**: 2 parquet files (2 month of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e34080aa1ef17b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T03:12:20.333386800Z",
     "start_time": "2024-02-01T03:12:20.321753500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Manually adjusted every run\n",
    "stopped_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8e387b92cd4e0ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T03:12:39.683412400Z",
     "start_time": "2024-02-01T03:12:20.328690700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of parquet files used in the test\n",
    "files = 6\n",
    "\n",
    "# Run the aggregated version of the program as measurement object for the fault tolerance test\n",
    "df = spark.read.parquet(*parquet_path_list[:files])\n",
    "df_aggregated = run_aggregated(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6484ae4dec12797",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stop the number of worker nodes, specified in `stopped_workers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dbf76f82ee91b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T03:12:39.829153500Z",
     "start_time": "2024-02-01T03:12:39.687131100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 stopped workers: Response time: 0.10315108299255371s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Day: date, ActionGeo_CountryCode: string, GoldsteinScaleSum: double, EventCount: bigint]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    # Get the response time of a single sql request -> The following requests would already be processed by using the cached data\n",
    "    avg_req_time = average_sql_request_response_time(SQL_REQUEST_AGGREGATED, 1)\n",
    "\n",
    "    # Combine test results of the current run\n",
    "    result = {\n",
    "        'status': 'COMPLETE',\n",
    "        'stopped_workers': stopped_workers,\n",
    "        'last_file_index': files,\n",
    "        'avg_req_time': avg_req_time\n",
    "    }\n",
    "\n",
    "    print(f\"{stopped_workers} stopped workers: Response time: {avg_req_time}s\")\n",
    "\n",
    "# If an exception occurs, the test run should fail \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    result = {\n",
    "        'status': 'FAILED',\n",
    "        'stopped_workers': stopped_workers,\n",
    "        'last_file_index': files,\n",
    "        'avg_req_time': 0\n",
    "    }\n",
    "\n",
    "# Test results are persisted every run to avoid losing them in case of a crash\n",
    "write_result_to_csv(result, 'test_fault_tolerance_aggregated.csv')\n",
    "\n",
    "# Remove from cache to prevent interference with the next run\n",
    "df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2848c30f40b743",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Restart the setup, so all workers are available again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "243453450cd3168b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T03:13:24.045497400Z",
     "start_time": "2024-02-01T03:12:39.830774400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of parquet files used in the test\n",
    "files = 2\n",
    "\n",
    "# Run the non-aggregated version of the program as measurement object for the fault tolerance test\n",
    "df = spark.read.parquet(*parquet_path_list[:files])\n",
    "df_non_aggregated = run_non_aggregated(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394623f3a0dfe1e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stop the number of worker nodes, specified in `stopped_workers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27fad3c79c9e94a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T03:13:24.498442600Z",
     "start_time": "2024-02-01T03:13:24.049167900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 stopped workers: Response time: 0.4099743366241455s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[GlobalEventID: int, Day: date, MonthYear: int, Year: int, FractionDate: float, Actor1Code: string, Actor1Name: string, Actor1CountryCode: string, Actor1KnownGroupCode: string, Actor1EthnicCode: string, Actor1Religion1Code: string, Actor1Religion2Code: string, Actor1Type1Code: string, Actor1Type2Code: string, Actor1Type3Code: string, Actor2Code: string, Actor2Name: string, Actor2CountryCode: string, Actor2KnownGroupCode: string, Actor2EthnicCode: string, Actor2Religion1Code: string, Actor2Religion2Code: string, Actor2Type1Code: string, Actor2Type2Code: string, Actor2Type3Code: string, IsRootEvent: int, EventCode: string, EventBaseCode: string, EventRootCode: string, QuadClass: int, GoldsteinScale: float, NumMentions: int, NumSources: int, NumArticles: int, AvgTone: float, Actor1Geo_Type: int, Actor1Geo_FullName: string, Actor1Geo_CountryCode: string, Actor1Geo_ADM1Code: string, Actor1Geo_ADM2Code: string, Actor1Geo_Lat: float, Actor1Geo_Long: float, Actor1Geo_FeatureID: string, Actor2Geo_Type: int, Actor2Geo_FullName: string, Actor2Geo_CountryCode: string, Actor2Geo_ADM1Code: string, Actor2Geo_ADM2Code: string, Actor2Geo_Lat: float, Actor2Geo_Long: float, Actor2Geo_FeatureID: string, ActionGeo_Type: int, ActionGeo_FullName: string, ActionGeo_CountryCode: string, ActionGeo_ADM1Code: string, ActionGeo_ADM2Code: string, ActionGeo_Lat: float, ActionGeo_Long: float, ActionGeo_FeatureID: string, DATEADDED: string, SOURCEURL: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    # Get the response time of a single sql request -> The following requests would already be processed by using the cached data\n",
    "    avg_req_time = average_sql_request_response_time(SQL_REQUEST_NON_AGGREGATED, 1)\n",
    "\n",
    "    # Combine test results of the current run\n",
    "    result = {\n",
    "        'status': 'COMPLETE',\n",
    "        'stopped_workers': stopped_workers,\n",
    "        'last_file_index': files,\n",
    "        'avg_req_time': avg_req_time\n",
    "    }\n",
    "\n",
    "    print(f\"{stopped_workers} stopped workers: Response time: {avg_req_time}s\")\n",
    "\n",
    "# If an exception occurs, the test run should fail \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    result = {\n",
    "        'status': 'FAILED',\n",
    "        'stopped_workers': stopped_workers,\n",
    "        'last_file_index': files,\n",
    "        'avg_req_time': 0\n",
    "    }\n",
    "\n",
    "# Test results are persisted every run to avoid losing them in case of a crash\n",
    "write_result_to_csv(result, 'test_fault_tolerance_non_aggregated.csv')\n",
    "\n",
    "# Remove from cache to prevent interference with the next run\n",
    "df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25deb463a34fc809",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Combined Scalability Test"
   ]
  },
  {
   "cell_type": "code",
   "id": "d019377bc4c6aa22",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Combined scalability test\n",
    "# Comment all notebooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
