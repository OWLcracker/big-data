{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cd59b29db59877",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tests\n",
    "\n",
    "In this notebook several tests are performed to analyze the scalability and fault tolerance of the application.\n",
    "\n",
    "To achieve comparable test results, all tests (if not stated otherwise) are executed on a local spark cluster with the following allocation of ressources:\n",
    "\n",
    "| Item      | Ressources        | Total Ressources (in cluster) |\n",
    "|-----------|-------------------|-------------------------------|\n",
    "| Workers   | 3                 | 3                             |\n",
    "| Executors | 2 per Worker      | 6                             |\n",
    "| RAM         | 3 GB per Executor | 18 GB                         |\n",
    "| Cores     | 1 per Executor    | 6                             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221ff96b1fbbb464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:17.523309400Z",
     "start_time": "2024-01-31T10:15:17.513615300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')\n",
    "parquet_path = os.path.join(local_path, 'parquet_test')\n",
    "result_folder_path = os.path.join(os.getcwd(), 'test_results')\n",
    "\n",
    "spark_rest_api_url = \"http://localhost:4040/api/v1/applications\"\n",
    "\n",
    "# The same sql query, which is used by superset to display the heatmap (aggregated version)\n",
    "SQL_REQUEST_AGGREGATED = \"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               SUM(GoldsteinScaleSum)/SUM(EventCount) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT_AGGR) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\"\n",
    "\n",
    "# The same sql query, which is used by superset to display the heatmap (non-aggregated version)\n",
    "SQL_REQUEST_NON_AGGREGATED = \"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               AVG(GoldsteinScale) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT\n",
    "           WHERE ActionGeo_CountryCode IS NOT NULL\n",
    "             AND GoldsteinScale IS NOT NULL) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2416bbad8d173cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:17.536828900Z",
     "start_time": "2024-01-31T10:15:17.517777600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Choose time period for which to download the data\n",
    "start_date = datetime.strptime('2015-07-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2023-12-31', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28da7b24aeff3418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:18.420764600Z",
     "start_time": "2024-01-31T10:15:17.690110900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "url_list = []\n",
    "index = 0\n",
    "url_list.append([])\n",
    "month = date_list[0].month\n",
    "\n",
    "# Create a nested list containing a list of months with the corresponding download urls\n",
    "for date in date_list:\n",
    "    if date.month != month:\n",
    "        month = date.month\n",
    "        index += 1\n",
    "        url_list.append([])\n",
    "\n",
    "    # Create the url and append it to the month list\n",
    "    for x in range(0, 24):\n",
    "        for y in range(0, 60, 15):\n",
    "            date_tmp = date + timedelta(hours=x, minutes=y)\n",
    "            url = base_url + date_tmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "            url_list[index].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966a9a0250206cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:18.447385400Z",
     "start_time": "2024-01-31T10:15:18.423523400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directories if they don't exist yet\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)\n",
    "\n",
    "if not os.path.isdir(parquet_path):\n",
    "    os.mkdir(parquet_path)\n",
    "    \n",
    "if not os.path.isdir(result_folder_path):\n",
    "    os.mkdir(result_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5837aac79f353d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:21.601666500Z",
     "start_time": "2024-01-31T10:15:18.429723500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a spark session (see config folder for spark config)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb63d2c2c61662b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:21.618485800Z",
     "start_time": "2024-01-31T10:15:21.603447600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define original data schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), True),\n",
    "    StructField(\"Day\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad726d18d2771288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:21.649243300Z",
     "start_time": "2024-01-31T10:15:21.607801500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    folder_location = os.path.join(local_path, fname[:4], fname[4:6])\n",
    "\n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(folder_location, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(folder_location, fname))\n",
    "\n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(folder_location, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(folder_location)\n",
    "\n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(folder_location, fname))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf453f8e79a0608a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:21.904517900Z",
     "start_time": "2024-01-31T10:15:21.620722400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download files and write them to parquet files in parallel for each month\n",
    "# This is done in batches to allow simple addition of new months to already existing data\n",
    "i = 0\n",
    "for month_list in url_list:\n",
    "    # Skip month if parquet file already exists\n",
    "    if os.path.exists(os.path.join(parquet_path, str(i) + \".parquet\")):\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    year_folder = os.path.join(local_path, month_list[0].split('/')[-1][:4])\n",
    "    month_folder = os.path.join(year_folder, month_list[0].split('/')[-1][4:6])\n",
    "\n",
    "    if not os.path.isdir(year_folder):\n",
    "        os.mkdir(year_folder)\n",
    "\n",
    "    if not os.path.isdir(month_folder):\n",
    "        os.mkdir(month_folder)\n",
    "\n",
    "    # Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_file, month_list)\n",
    "\n",
    "    # Read all csv files of one month into a spark dataframe\n",
    "    df = spark.read.csv(month_folder, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "\n",
    "    # Write the data of one month into a parquet file\n",
    "    df.write.parquet(os.path.join(parquet_path, str(i) + \".parquet\"), mode='overwrite')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    # Delete the csv files to free up disk space\n",
    "    shutil.rmtree(month_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e285222dfa5b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:24.971455900Z",
     "start_time": "2024-01-31T10:15:21.834389100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Get the csv and parquet file sizes via the input/output bytes of the stages from the spark rest api\n",
    "result_file_path = os.path.join(result_folder_path, 'data_size.csv')\n",
    "\n",
    "if not os.path.isdir(result_folder_path):\n",
    "    os.mkdir(result_folder_path)\n",
    "\n",
    "# Fetch the list of applications to get the application id\n",
    "apps_response = requests.get(spark_rest_api_url)\n",
    "apps = apps_response.json()\n",
    "app_id = apps[0]['id']\n",
    "\n",
    "# Get stages information for the application (1 stage per parquet file write)\n",
    "stages_response = requests.get(f\"{spark_rest_api_url}/{app_id}/stages\")\n",
    "stages_data = stages_response.json()\n",
    "\n",
    "stage_result = {}\n",
    "\n",
    "# Get necessary information of each stage\n",
    "for stage in stages_data:\n",
    "    stage_result[stage['stageId']] = {\n",
    "        'status': stage['status'],\n",
    "        'input_data': stage['inputBytes'],\n",
    "        'output_data': stage['outputBytes']\n",
    "    }\n",
    "    \n",
    "df_result = pd.DataFrame.from_dict(stage_result, orient='index')\n",
    "\n",
    "# Write the result to a csv file to use them later\n",
    "if not os.path.isfile(result_file_path):\n",
    "    df_result.to_csv(result_file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f9ccbf5639cd6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:24.976491300Z",
     "start_time": "2024-01-31T10:15:24.857102100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Method to run the non-aggregated version of the program repeatedly in the test loop\n",
    "def run_non_aggregated(df_base):\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_non_aggregated = df_base.join(broadcast(df_mapping), df_base['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                     'left_outer')\n",
    "\n",
    "    df_non_aggregated = df_non_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_non_aggregated.cache()\n",
    "    df_non_aggregated.count()\n",
    "    df_non_aggregated.createOrReplaceGlobalTempView(\"GDELT\")\n",
    "    \n",
    "    return df_non_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33308ce9115c6ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:25.004983700Z",
     "start_time": "2024-01-31T10:15:24.861281800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to run the aggregated version of the program repeatedly in the test loop\n",
    "def run_aggregated(df_base):\n",
    "    # Select only relevant columns for the aggregation\n",
    "    df_selection = df_base.select(\n",
    "        F.col('Day'),\n",
    "        F.col('ActionGeo_CountryCode'),\n",
    "        F.col('GoldsteinScale')\n",
    "    )\n",
    "\n",
    "    # Remove rows that contain null values, which would distort the aggregation results \n",
    "    df_selection = df_selection.na.drop()\n",
    "    \n",
    "    # Aggregate the values by date and country so there is only one value per country per day\n",
    "    df_aggregated = df_selection.groupBy('Day', 'ActionGeo_CountryCode').agg(\n",
    "        F.sum('GoldsteinScale').alias('GoldsteinScaleSum'),\n",
    "        F.count('*').alias('EventCount')\n",
    "    )\n",
    "\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_aggregated = df_aggregated.join(broadcast(df_mapping),\n",
    "                                       df_aggregated['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                       'left_outer')\n",
    "\n",
    "    df_aggregated = df_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_aggregated.cache()\n",
    "    df_aggregated.count()\n",
    "    df_aggregated.createOrReplaceGlobalTempView(\"GDELT_AGGR\")\n",
    "    \n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "395334747b2bf948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:25.054054500Z",
     "start_time": "2024-01-31T10:15:24.928405700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Method to get the current cache information from the spark rest api\n",
    "def get_cache_information():\n",
    "    \n",
    "    # Fetch the list of applications to get the application id\n",
    "    apps_response = requests.get(spark_rest_api_url)\n",
    "    apps = apps_response.json()\n",
    "    app_id = apps[0]['id']\n",
    "    \n",
    "    # Get storage information for the application\n",
    "    storage_response = requests.get(f\"{spark_rest_api_url}/{app_id}/storage/rdd\")\n",
    "    storage_data = storage_response.json()\n",
    "    \n",
    "    # Only one dataframe is cached at a time, so only the first entry is relevant\n",
    "    return storage_data[0]['memoryUsed'], storage_data[0]['diskUsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8440c03e9b407fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:25.205056900Z",
     "start_time": "2024-01-31T10:15:24.965735500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "\n",
    "def average_sql_request_response_time(sql, n=10):\n",
    "    # Create connection to thrift server\n",
    "    with hive.connect(host='localhost', port=10000, username='spark') as connection:\n",
    "        request_start_time = time.time()\n",
    "\n",
    "        for _ in range(n):\n",
    "            # Send request to thrift server\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(sql)\n",
    "            result = cursor.fetchall()\n",
    "\n",
    "        request_end_time = time.time()\n",
    "\n",
    "    return (request_end_time - request_start_time) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5089c9a753e4ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:25.224392700Z",
     "start_time": "2024-01-31T10:15:25.127868Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def write_result_to_csv(result_dict, file_name):\n",
    "    \n",
    "    result_file_path = os.path.join(result_folder_path, file_name)\n",
    "    \n",
    "    df_result = pd.DataFrame([result_dict])\n",
    "\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(result_file_path)\n",
    "    \n",
    "    # Write the test result to a csv file, append if file exists, create new if it doesn't\n",
    "    df_result.to_csv(result_file_path, sep=';', mode='a' if file_exists else 'w', header=not file_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93339d3044765fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:28.128380300Z",
     "start_time": "2024-01-31T10:15:25.132306500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc77b63692b1dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling Data Volume\n",
    "\n",
    "The following tests are performed to analyze how increasing the data volume affects the performance of the application.\n",
    "\n",
    "Both versions of the program (aggregated and non-aggregated) are repeatedly executed in a loop with an increasing amount of data. In each test run, performance metrics and cache information are collected and persisted to a csv file, to be used later for analysis.\n",
    "\n",
    "Contrary to `main.py` the aggregated and non-aggregated version are completely separated to test the individual performance of both versions.\n",
    "\n",
    "\n",
    "### Data Volume\n",
    "The data volume is increased by incrementing the number of parquet files (each representing a month of data) in every test run.\n",
    "\n",
    "There a are a total of 102 parquet files (8.5 years of data).\n",
    "\n",
    "Up to a full year of data (12 parquet files), tests are run in increments of 1 month (1 parquet file) to get a more granular view of the performance effects.\n",
    "Afterward, tests are run with increasing increments to analyze the performance effects at scale, while keeping the number of test runs low.\n",
    "The increment starts at 6 months and increases by 6 months with every test run (+6, +12, +18, ... parquet files), until the maximum of 8.5 years of data (102 parquet files) is reached.\n",
    "\n",
    "The tests are therefore conducted with the following numbers of parquet files: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 30, 48, 72, 102.\n",
    "\n",
    "### Performance\n",
    "The effect on the performance is analyzed by measuring different metrics:\n",
    "- **Pre-processing turnaround time**: The amount of time it takes to load the data into spark, conduct the necessary pre-processing & cache the results.\n",
    "- **Query response time**: The amount of time it takes to process and return the results of a single sql query, which is sent to the thrift server. An average of 10 sequential queries is calculated for more stable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1d9be6de28b6c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:28.157151200Z",
     "start_time": "2024-01-31T10:15:28.128297Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of all parquet file paths\n",
    "parquet_path_list = []\n",
    "\n",
    "# Number of parquet files in the directory\n",
    "for i in range(0, 102):\n",
    "    parquet_path_list.append(os.path.join(parquet_path, str(i) + \".parquet\"))\n",
    "\n",
    "parquet_file_test_cases = list(range(1, 13)) + [18, 30, 48, 72, 102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e6e395c751cd15e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T00:16:14.072588600Z",
     "start_time": "2024-01-30T23:32:26.400191200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 parquet files: duration: 17.910873889923096s, memory usage: 212624B, disk usage: 0B, avg req time: 0.3205996036529541s\n",
      "2 parquet files: duration: 11.786883115768433s, memory usage: 392008B, disk usage: 0B, avg req time: 0.21491703987121583s\n",
      "3 parquet files: duration: 14.532766342163086s, memory usage: 570008B, disk usage: 0B, avg req time: 0.22516844272613526s\n",
      "4 parquet files: duration: 20.72968816757202s, memory usage: 739032B, disk usage: 0B, avg req time: 0.2745798587799072s\n",
      "5 parquet files: duration: 24.38275408744812s, memory usage: 902656B, disk usage: 0B, avg req time: 0.2002878427505493s\n",
      "6 parquet files: duration: 27.8938992023468s, memory usage: 1078528B, disk usage: 0B, avg req time: 0.16792101860046388s\n",
      "7 parquet files: duration: 29.737545013427734s, memory usage: 1232880B, disk usage: 0B, avg req time: 0.13230364322662352s\n",
      "8 parquet files: duration: 30.786810159683228s, memory usage: 1387816B, disk usage: 0B, avg req time: 0.12997713088989257s\n",
      "9 parquet files: duration: 32.25099444389343s, memory usage: 1556184B, disk usage: 0B, avg req time: 0.12386527061462402s\n",
      "10 parquet files: duration: 34.38964653015137s, memory usage: 1727232B, disk usage: 0B, avg req time: 0.11478133201599121s\n",
      "11 parquet files: duration: 37.78990864753723s, memory usage: 1912824B, disk usage: 0B, avg req time: 0.11333491802215576s\n",
      "12 parquet files: duration: 40.895557165145874s, memory usage: 2063312B, disk usage: 0B, avg req time: 0.11834964752197266s\n",
      "18 parquet files: duration: 63.84113788604736s, memory usage: 2728784B, disk usage: 0B, avg req time: 0.11102850437164306s\n",
      "24 parquet files: duration: 86.43895173072815s, memory usage: 3451712B, disk usage: 0B, avg req time: 0.1050760269165039s\n",
      "30 parquet files: duration: 102.93086218833923s, memory usage: 4167904B, disk usage: 0B, avg req time: 0.12925372123718262s\n",
      "36 parquet files: duration: 91.01072597503662s, memory usage: 4846880B, disk usage: 0B, avg req time: 0.14287350177764893s\n",
      "42 parquet files: duration: 105.07836627960205s, memory usage: 5586328B, disk usage: 0B, avg req time: 0.15116634368896484s\n",
      "48 parquet files: duration: 120.36666941642761s, memory usage: 6250072B, disk usage: 0B, avg req time: 0.1306856870651245s\n",
      "54 parquet files: duration: 133.78375959396362s, memory usage: 6970952B, disk usage: 0B, avg req time: 0.1189932107925415s\n",
      "60 parquet files: duration: 148.54438853263855s, memory usage: 7654672B, disk usage: 0B, avg req time: 0.12518510818481446s\n",
      "66 parquet files: duration: 159.73500871658325s, memory usage: 8363992B, disk usage: 0B, avg req time: 0.10890445709228516s\n",
      "72 parquet files: duration: 173.06682920455933s, memory usage: 9001784B, disk usage: 0B, avg req time: 0.1138416051864624s\n",
      "78 parquet files: duration: 188.03518152236938s, memory usage: 9709040B, disk usage: 0B, avg req time: 0.11378064155578613s\n",
      "84 parquet files: duration: 202.07580828666687s, memory usage: 10364176B, disk usage: 0B, avg req time: 0.10736947059631348s\n",
      "90 parquet files: duration: 214.70373225212097s, memory usage: 11037120B, disk usage: 0B, avg req time: 0.10867528915405274s\n",
      "96 parquet files: duration: 229.84893202781677s, memory usage: 11718368B, disk usage: 0B, avg req time: 0.118764328956604s\n",
      "102 parquet files: duration: 243.43086171150208s, memory usage: 12407752B, disk usage: 0B, avg req time: 0.11687967777252198s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "exception_occurred = False\n",
    "\n",
    "# Run the aggregated version of main logic (data loading & pre-processing) in a loop with increasing data volume\n",
    "for i in parquet_file_test_cases:\n",
    "    if not exception_occurred:\n",
    "        try:    \n",
    "            # Start timer\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Read in the parquet files relevant for the current run\n",
    "            df = spark.read.parquet(*parquet_path_list[:i])\n",
    "                \n",
    "            # Run the aggregated version of the main logic\n",
    "            df_aggregated = run_aggregated(df)\n",
    "            \n",
    "            # Stop timer\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate duration of the run\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            # Get cache information\n",
    "            memory_usage, disk_usage = get_cache_information()\n",
    "            \n",
    "            # Get the average response time of a sql query\n",
    "            avg_req_time = average_sql_request_response_time(SQL_REQUEST_AGGREGATED)\n",
    "        \n",
    "            # Combine test results of the current run\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': i-1, # This is necessary to calculate the size of the data used for this test run\n",
    "                'duration': duration,\n",
    "                'memory_usage': memory_usage,\n",
    "                'disk_usage': disk_usage,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "            \n",
    "            print(f\"{i} parquet files: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "        \n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occurred = True\n",
    "    \n",
    "    if exception_occurred:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': i-1,\n",
    "            'duration': 0,\n",
    "            'memory_usage': 0,\n",
    "            'disk_usage': 0,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "        \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_data_volume_aggregated.csv')\n",
    "    \n",
    "    # Remove from cache to prevent interference with the next run\n",
    "    df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f61114596cbab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T00:16:13.666755700Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 parquet files: duration: 34.825024127960205s, memory usage: 1273540248B, disk usage: 0B, avg req time: 0.2940074443817139s\n",
      "2 parquet files: duration: 50.998690605163574s, memory usage: 2475682816B, disk usage: 0B, avg req time: 0.4139133930206299s\n",
      "3 parquet files: duration: 62.1660943031311s, memory usage: 3727293664B, disk usage: 0B, avg req time: 0.5287338018417358s\n",
      "4 parquet files: duration: 78.8647871017456s, memory usage: 4952817680B, disk usage: 0B, avg req time: 0.651429009437561s\n",
      "5 parquet files: duration: 96.7095856666565s, memory usage: 6191491184B, disk usage: 0B, avg req time: 0.7858332872390748s\n",
      "6 parquet files: duration: 114.00220370292664s, memory usage: 7367461568B, disk usage: 0B, avg req time: 0.9434693098068238s\n",
      "7 parquet files: duration: 129.62822437286377s, memory usage: 8589660192B, disk usage: 0B, avg req time: 1.1007545709609985s\n",
      "8 parquet files: duration: 150.91743516921997s, memory usage: 9859052432B, disk usage: 0B, avg req time: 1.905244731903076s\n",
      "9 parquet files: duration: 174.4083240032196s, memory usage: 9659728456B, disk usage: 1146662896B, avg req time: 2.625724506378174s\n",
      "10 parquet files: duration: 192.90184354782104s, memory usage: 9766891456B, disk usage: 2050651496B, avg req time: 3.322082018852234s\n",
      "11 parquet files: duration: 212.58268880844116s, memory usage: 9737988312B, disk usage: 3073940772B, avg req time: 3.8246331214904785s\n",
      "12 parquet files: duration: 237.58019280433655s, memory usage: 9707609416B, disk usage: 4072876510B, avg req time: 5.491439604759217s\n",
      "18 parquet files: duration: 360.038635969162s, memory usage: 9817580784B, disk usage: 9742041150B, avg req time: 12.842222476005555s\n",
      "24 parquet files: duration: 483.2054355144501s, memory usage: 9902984824B, disk usage: 15256685868B, avg req time: 16.397543025016784s\n",
      "30 parquet files: duration: 592.9723589420319s, memory usage: 9848049968B, disk usage: 20179497599B, avg req time: 21.608130478858946s\n",
      "36 parquet files: duration: 670.2361326217651s, memory usage: 9996854664B, disk usage: 25058018384B, avg req time: 25.070871567726137s\n",
      "42 parquet files: duration: 766.6207828521729s, memory usage: 9844528808B, disk usage: 29895272978B, avg req time: 28.738381767272948s\n",
      "48 parquet files: duration: 869.1880326271057s, memory usage: 9787184200B, disk usage: 34540547789B, avg req time: 33.59104568958283s\n",
      "54 parquet files: duration: 952.0325264930725s, memory usage: 9929612928B, disk usage: 38684723637B, avg req time: 35.70017638206482s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "exception_occurred = False\n",
    "\n",
    "# Run the non-aggregated version of main logic (data loading & pre-processing) in a loop with increasing data volume\n",
    "for i in parquet_file_test_cases:\n",
    "    if not exception_occurred:\n",
    "        try:\n",
    "            # Start timer\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Read in the parquet files relevant for the current run\n",
    "            df = spark.read.parquet(*parquet_path_list[:i])\n",
    "\n",
    "            # Run the non-aggregated version of the main logic\n",
    "            df_non_aggregated = run_non_aggregated(df)\n",
    "\n",
    "            # Stop timer\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate duration of the run\n",
    "            duration = end_time - start_time\n",
    "\n",
    "            # Get cache information\n",
    "            memory_usage, disk_usage = get_cache_information()\n",
    "\n",
    "            # Get the average response time of a sql query\n",
    "            avg_req_time = average_sql_request_response_time(SQL_REQUEST_NON_AGGREGATED)\n",
    "\n",
    "            # Combine test results of the current run\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': i - 1,  # This is necessary to calculate the size of the data used for this test run\n",
    "                'duration': duration,\n",
    "                'memory_usage': memory_usage,\n",
    "                'disk_usage': disk_usage,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "\n",
    "            print(f\"{i} parquet files: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "\n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occurred = True\n",
    "\n",
    "    if exception_occurred:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': i - 1,\n",
    "            'duration': 0,\n",
    "            'memory_usage': 0,\n",
    "            'disk_usage': 0,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "\n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_data_volume_non_aggregated.csv')\n",
    "\n",
    "    # Remove from cache to prevent interference with the next run\n",
    "    df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cb55f2702794b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling Load\n",
    "\n",
    "The following tests are performed to analyze how an increased load influences the performance of the application.\n",
    "\n",
    "In the given use case, an increasing amount of users using the dashboard at the same time or an increasing amount of dashboard elements (e.g. charts, tables, etc.) could be considered an increase in load. In both cases the number of concurrent queries, that are sent from superset to the thrift server, would increase. For that reason, we define load as the number of queries that are sent to the thrift server in parallel.\n",
    "\n",
    "Once the program is executed and the data is cached, an incrementing number of queries are sent to the thrift server in parallel. In each test run performance metrics are measured and persisted to a csv file. The tests are executed for both versions of the program (aggregated and non-aggregated) separately.\n",
    "\n",
    "\n",
    "### Load\n",
    "The number of queries sent to the thrift server in parallel is increased in increments of 10, starting at 10 up to 100. Afterward, it is increased in increments of 50, starting at 150 up to 500. Finally, the number of queries is increased in increments of 100, starting at 600 up to 1000.\n",
    "\n",
    "To simulate, that queries are sent in parallel (e.g. by different users), a separate thread is started for each query. Every thread establishes an isolated connection to the thrift server and sends a single query.\n",
    "\n",
    "\n",
    "### Performance\n",
    "The effect on the performance is analyzed by measuring the following metric:\n",
    "- **Average query response time**: The amount of time it takes to process and return the results of a sql query sent to the thrift server under the given load.\n",
    "\n",
    "### Data Volume\n",
    "The data volume is kept constant to test the effect of additional load in an isolated way. 1 parquet file (1 month of data) is used for all test runs to ensure that all cached data fits into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbbb7b579b5dccb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:37.062942400Z",
     "start_time": "2024-01-31T10:15:37.053014600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import queue\n",
    "\n",
    "\n",
    "def run_query(sql, data_queue, stop_event):\n",
    "    # Send a single query to the thrift server and store the response time\n",
    "    try:\n",
    "        if not stop_event.is_set():\n",
    "            req_time = average_sql_request_response_time(sql, 1)\n",
    "            data_queue.put(req_time)\n",
    "    # Stop other threads, when an exception occurs and save the exception\n",
    "    except Exception as e:\n",
    "        data_queue.put(e)\n",
    "        stop_event.set()\n",
    "\n",
    "\n",
    "def run_queries_parallel(sql, n):\n",
    "    threads = []\n",
    "    data_queue = queue.Queue()\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    # Start n threads to run n queries in parallel\n",
    "    for _ in range(n):\n",
    "        t = threading.Thread(target=run_query, args=(sql, data_queue, stop_event))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    # Wait until all threads are finished\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    # Calculate the average response time\n",
    "    total = 0\n",
    "    while not data_queue.empty():\n",
    "        result = data_queue.get()\n",
    "        if isinstance(result, Exception):\n",
    "            # If an exception occurred in one of the threads, the test run should fail\n",
    "            raise result\n",
    "        total += result\n",
    "\n",
    "    return total / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78ea03ea4e916485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T10:15:38.336632300Z",
     "start_time": "2024-01-31T10:15:38.316948Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of parquet files used in the test\n",
    "files = 1\n",
    "# Number of queries sent in parallel\n",
    "query_test_cases = list(range(10, 101, 10)) + list(range(150, 501, 50)) + list(range(600, 1001, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b82c0191a3b5a0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T09:42:18.869039400Z",
     "start_time": "2024-01-31T09:39:10.965052500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time with 10 parallel queries: 1.4298208475112915s\n",
      "Average response time with 20 parallel queries: 1.1105712413787843s\n",
      "Average response time with 30 parallel queries: 1.0981416781743367s\n",
      "Average response time with 40 parallel queries: 0.9521482467651368s\n",
      "Average response time with 50 parallel queries: 1.1046146535873413s\n",
      "Average response time with 60 parallel queries: 1.2085445046424865s\n",
      "Average response time with 70 parallel queries: 1.1775202478681293s\n",
      "Average response time with 80 parallel queries: 1.2359691202640533s\n",
      "Average response time with 90 parallel queries: 1.1312698708640205s\n",
      "Average response time with 100 parallel queries: 1.2913950562477112s\n",
      "Average response time with 150 parallel queries: 1.9098400942484537s\n",
      "Average response time with 200 parallel queries: 2.2729758834838867s\n",
      "Average response time with 250 parallel queries: 3.7793275508880617s\n",
      "Average response time with 300 parallel queries: 3.6516279363632203s\n",
      "Average response time with 350 parallel queries: 3.8707099206107003s\n",
      "Average response time with 400 parallel queries: 5.291167828440666s\n",
      "Average response time with 450 parallel queries: 5.258764734268189s\n",
      "Average response time with 500 parallel queries: 7.516808623313904s\n",
      "An error occurred: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Java heap space\\njava.lang.OutOfMemoryError: Java heap space\\n\\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\\n\\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:360)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:360)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1909/0x00007fbe74d949e0.apply(Unknown Source)\\n\\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\\n\\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\\n\\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\\n\\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\\n\\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1886)\\n\\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1857)\\n\\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1337)\\n\\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\\n\\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\\n\\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1912/0x00007fbe74d8d9f0.apply(Unknown Source)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\\n\\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\\n\\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\\n\\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)\\n\\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\\n\\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1580)\\n\\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\\n:36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor76:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:568', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'jdk.proxy2.$Proxy98:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:833', '*org.apache.spark.SparkException:Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Java heap space\\njava.lang.OutOfMemoryError: Java heap space\\n\\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\\n\\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:360)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:360)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1909/0x00007fbe74d949e0.apply(Unknown Source)\\n\\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\\n\\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\\n\\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\\n\\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\\n\\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1886)\\n\\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1857)\\n\\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1337)\\n\\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\\n\\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\\n\\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1912/0x00007fbe74d8d9f0.apply(Unknown Source)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\\n\\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\\n\\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\\n\\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)\\n\\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\\n\\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1580)\\n\\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\\n:75:40', 'org.apache.spark.scheduler.DAGScheduler:failJobAndIndependentStages:DAGScheduler.scala:2844', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$abortStage$2:DAGScheduler.scala:2780', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$abortStage$2$adapted:DAGScheduler.scala:2779', 'scala.collection.mutable.ResizableArray:foreach:ResizableArray.scala:62', 'scala.collection.mutable.ResizableArray:foreach$:ResizableArray.scala:55', 'scala.collection.mutable.ArrayBuffer:foreach:ArrayBuffer.scala:49', 'org.apache.spark.scheduler.DAGScheduler:abortStage:DAGScheduler.scala:2779', 'org.apache.spark.scheduler.DAGScheduler:submitMissingTasks:DAGScheduler.scala:1590', 'org.apache.spark.scheduler.DAGScheduler:submitStage:DAGScheduler.scala:1397', 'org.apache.spark.scheduler.DAGScheduler:handleJobSubmitted:DAGScheduler.scala:1332', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:doOnReceive:DAGScheduler.scala:2991', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:onReceive:DAGScheduler.scala:2982', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:onReceive:DAGScheduler.scala:2971', 'org.apache.spark.util.EventLoop$$anon$1:run:EventLoop.scala:49', 'org.apache.spark.scheduler.DAGScheduler:runJob:DAGScheduler.scala:984', 'org.apache.spark.SparkContext:runJob:SparkContext.scala:2398', 'org.apache.spark.SparkContext:runJob:SparkContext.scala:2419', 'org.apache.spark.SparkContext:runJob:SparkContext.scala:2438', 'org.apache.spark.SparkContext:runJob:SparkContext.scala:2463', 'org.apache.spark.rdd.RDD:$anonfun$collect$1:RDD.scala:1046', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:151', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:112', 'org.apache.spark.rdd.RDD:withScope:RDD.scala:407', 'org.apache.spark.rdd.RDD:collect:RDD.scala:1045', 'org.apache.spark.sql.execution.SparkPlan:executeCollect:SparkPlan.scala:448', 'org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec:$anonfun$executeCollect$1:AdaptiveSparkPlanExec.scala:374', 'org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec:withFinalPlanUpdate:AdaptiveSparkPlanExec.scala:402', 'org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec:executeCollect:AdaptiveSparkPlanExec.scala:374', 'org.apache.spark.sql.Dataset:collectFromPlan:Dataset.scala:4344', 'org.apache.spark.sql.Dataset:$anonfun$collect$1:Dataset.scala:3585', 'org.apache.spark.sql.Dataset:$anonfun$withAction$2:Dataset.scala:4334', 'org.apache.spark.sql.execution.QueryExecution$:withInternalError:QueryExecution.scala:546', 'org.apache.spark.sql.Dataset:$anonfun$withAction$1:Dataset.scala:4332', 'org.apache.spark.sql.execution.SQLExecution$:$anonfun$withNewExecutionId$6:SQLExecution.scala:125', 'org.apache.spark.sql.execution.SQLExecution$:withSQLConfPropagated:SQLExecution.scala:201', 'org.apache.spark.sql.execution.SQLExecution$:$anonfun$withNewExecutionId$1:SQLExecution.scala:108', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.SQLExecution$:withNewExecutionId:SQLExecution.scala:66', 'org.apache.spark.sql.Dataset:withAction:Dataset.scala:4332', 'org.apache.spark.sql.Dataset:collect:Dataset.scala:3585', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:236', '*java.lang.OutOfMemoryError:Java heap space:32:31', 'java.nio.HeapByteBuffer:<init>:HeapByteBuffer.java:64', 'java.nio.ByteBuffer:allocate:ByteBuffer.java:363', 'org.apache.spark.broadcast.TorrentBroadcast$:$anonfun$blockifyObject$1:TorrentBroadcast.scala:360', 'org.apache.spark.broadcast.TorrentBroadcast$:$anonfun$blockifyObject$1$adapted:TorrentBroadcast.scala:360', 'org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1909/0x00007fbe74d949e0:apply::-1', 'org.apache.spark.util.io.ChunkedByteBufferOutputStream:allocateNewChunkIfNeeded:ChunkedByteBufferOutputStream.scala:87', 'org.apache.spark.util.io.ChunkedByteBufferOutputStream:write:ChunkedByteBufferOutputStream.scala:75', 'net.jpountz.lz4.LZ4BlockOutputStream:flushBufferedData:LZ4BlockOutputStream.java:225', 'net.jpountz.lz4.LZ4BlockOutputStream:write:LZ4BlockOutputStream.java:178', 'java.io.ObjectOutputStream$BlockDataOutputStream:drain:ObjectOutputStream.java:1886', 'java.io.ObjectOutputStream$BlockDataOutputStream:write:ObjectOutputStream.java:1857', 'java.io.ObjectOutputStream:writeArray:ObjectOutputStream.java:1337', 'java.io.ObjectOutputStream:writeObject0:ObjectOutputStream.java:1177', 'java.io.ObjectOutputStream:writeObject:ObjectOutputStream.java:350', 'org.apache.spark.serializer.JavaSerializationStream:writeObject:JavaSerializer.scala:46', 'org.apache.spark.broadcast.TorrentBroadcast$:$anonfun$blockifyObject$4:TorrentBroadcast.scala:365', 'org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1912/0x00007fbe74d8d9f0:apply::-1', 'org.apache.spark.util.SparkErrorUtils:tryWithSafeFinally:SparkErrorUtils.scala:64', 'org.apache.spark.util.SparkErrorUtils:tryWithSafeFinally$:SparkErrorUtils.scala:61', 'org.apache.spark.util.Utils$:tryWithSafeFinally:Utils.scala:94', 'org.apache.spark.broadcast.TorrentBroadcast$:blockifyObject:TorrentBroadcast.scala:367', 'org.apache.spark.broadcast.TorrentBroadcast:writeBlocks:TorrentBroadcast.scala:161', 'org.apache.spark.broadcast.TorrentBroadcast:<init>:TorrentBroadcast.scala:99', 'org.apache.spark.broadcast.TorrentBroadcastFactory:newBroadcast:TorrentBroadcastFactory.scala:38', 'org.apache.spark.broadcast.BroadcastManager:newBroadcast:BroadcastManager.scala:78', 'org.apache.spark.SparkContext:broadcastInternal:SparkContext.scala:1662', 'org.apache.spark.SparkContext:broadcast:SparkContext.scala:1644', 'org.apache.spark.scheduler.DAGScheduler:submitMissingTasks:DAGScheduler.scala:1580', 'org.apache.spark.scheduler.DAGScheduler:submitStage:DAGScheduler.scala:1397', 'org.apache.spark.scheduler.DAGScheduler:handleJobSubmitted:DAGScheduler.scala:1332', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:doOnReceive:DAGScheduler.scala:2991', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:onReceive:DAGScheduler.scala:2982'], sqlState=None, errorCode=0, errorMessage='Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Java heap space\\njava.lang.OutOfMemoryError: Java heap space\\n\\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\\n\\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:360)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:360)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1909/0x00007fbe74d949e0.apply(Unknown Source)\\n\\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\\n\\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\\n\\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\\n\\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\\n\\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1886)\\n\\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1857)\\n\\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1337)\\n\\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\\n\\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\\n\\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1912/0x00007fbe74d8d9f0.apply(Unknown Source)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\\n\\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\\n\\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\\n\\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\\n\\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)\\n\\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\\n\\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1580)\\n\\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\\n'), operationHandle=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Day: date, ActionGeo_CountryCode: string, GoldsteinScaleSum: double, EventCount: bigint]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the aggregated version of the program as measurement object for the load tests\n",
    "df = spark.read.parquet(*parquet_path_list[:files])\n",
    "df_aggregated = run_aggregated(df)\n",
    "\n",
    "exception_occurred = False\n",
    "\n",
    "for i in query_test_cases:\n",
    "    if not exception_occurred:\n",
    "        # Execute n queries in parallel and calculate the average response time\n",
    "        try:\n",
    "            avg_req_time = run_queries_parallel(SQL_REQUEST_AGGREGATED, i)\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': files-1,\n",
    "                'parallel_queries': i,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "            print(f\"Average response time with {i} parallel queries: {avg_req_time}s\")\n",
    "            \n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occurred = True\n",
    "    \n",
    "    if exception_occurred:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': files-1,\n",
    "            'parallel_queries': i,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "        \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_load_aggregated.csv')\n",
    "    \n",
    "df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4737bdd8e2ac1bab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T10:15:46.782273Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time with 10 parallel queries: 4.136620140075683s\n",
      "Average response time with 20 parallel queries: 4.243908607959748s\n",
      "Average response time with 30 parallel queries: 5.094589932759603s\n",
      "Average response time with 40 parallel queries: 6.252254176139831s\n",
      "Average response time with 50 parallel queries: 6.963157968521118s\n",
      "Average response time with 60 parallel queries: 8.665546282132466s\n",
      "Average response time with 70 parallel queries: 10.911023283004761s\n",
      "Average response time with 80 parallel queries: 11.791859102249145s\n",
      "Average response time with 90 parallel queries: 12.981755465931363s\n",
      "Average response time with 100 parallel queries: 16.88657589197159s\n",
      "Average response time with 150 parallel queries: 36.35064530213674s\n",
      "Average response time with 200 parallel queries: 44.65883248925209s\n",
      "Average response time with 250 parallel queries: 70.1403224811554s\n",
      "Average response time with 300 parallel queries: 86.47629649162292s\n",
      "Average response time with 350 parallel queries: 101.65685324873243s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exception_occurred:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Execute n queries in parallel and calculate the average response time\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m         avg_req_time \u001b[38;5;241m=\u001b[39m \u001b[43mrun_queries_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSQL_REQUEST_NON_AGGREGATED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOMPLETE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_file_index\u001b[39m\u001b[38;5;124m'\u001b[39m: files\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparallel_queries\u001b[39m\u001b[38;5;124m'\u001b[39m: i,\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_req_time\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_req_time\n\u001b[1;32m     17\u001b[0m         }\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage response time with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parallel queries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_req_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m, in \u001b[0;36mrun_queries_parallel\u001b[0;34m(sql, n)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Wait until all threads are finished\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Calculate the average response time\u001b[39;00m\n\u001b[1;32m     34\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the non-aggregated version of the program as measurement objects for the load tests\n",
    "df = spark.read.parquet(*parquet_path_list[:files])\n",
    "df_non_aggregated = run_non_aggregated(df)\n",
    "\n",
    "exception_occurred = False\n",
    "\n",
    "for i in query_test_cases:\n",
    "    if not exception_occurred:\n",
    "        # Execute n queries in parallel and calculate the average response time\n",
    "        try:\n",
    "            avg_req_time = run_queries_parallel(SQL_REQUEST_NON_AGGREGATED, i)\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': files-1,\n",
    "                'parallel_queries': i,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "            print(f\"Average response time with {i} parallel queries: {avg_req_time}s\")\n",
    "            \n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occurred = True\n",
    "    \n",
    "    if exception_occurred:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': files-1,\n",
    "            'parallel_queries': i,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "            \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_load_non_aggregated.csv')\n",
    "\n",
    "df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96620b51c64648",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling Ressources\n",
    "\n",
    "The following tests are performed to analyze how increasing the ressources of the spark cluster influences the performance of the application.\n",
    "\n",
    "In the given case the relevant ressources are the number of CPU cores and the amount of RAM, which the spark cluster can use to process & cache the data.\n",
    "\n",
    "Scaling ressources in the big data context is usually achieved by adding additional nodes consisting of commodity hardware to a cluster (horizontal scaling). To simulate the effect of adding additional nodes to the cluster, both versions of the program (aggregated and non-aggregated) are executed (just like it was done in the data volume scalability tests) with an increasing number of worker nodes. In each test run, performance metrics and cache information are collected and persisted to a csv file.\n",
    "\n",
    "### Ressources\n",
    "The ressources are scaled by manually changing the number of worker nodes in `docker-compose.yaml` and adjusting the total number of executors in the cluster accordingly in 'spark-defaults.conf'. After changing the configuration the setup is restarted and the tests are executed again with the adapted spark cluster.\n",
    " \n",
    "The tests are conducted with the following ressource configurations:\n",
    "\n",
    "\n",
    "| No. Workers               | Total No. Executors (in cluster) | Total RAM (in cluster) | Total No. Cores (in cluster)       |\n",
    "|---------------------------|----------------------------------|------------------------------|------------------------------------|\n",
    "| 1                         | 2                                | 6 GB                         | 2                                  |\n",
    "| 2                         | 4                                | 12 GB                        | 4                                  |\n",
    "| 3                         | 6                                | 18 GB                        | 6                                  |\n",
    "\n",
    "### Performance\n",
    "The measured performance metric are the same as in the data volume scalability tests:\n",
    "- **Pre-processing turnaround time**\n",
    "- **Query response time**\n",
    "\n",
    "### Data Volume\n",
    "The data volume is kept constant to test the effect of adding additional ressources in an isolated way. 1 parquet file (1 month of data) is used for all test runs to ensure that all cached data fits into memory."
   ]
  },
  {
   "cell_type": "code",
   "id": "b2dbc683cb287043",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Manually adjusted every run\n",
    "number_of_workers = 1\n",
    "# Number of parquet files used in the test\n",
    "files = 1"
   ]
  },
  {
   "cell_type": "code",
   "id": "bbf2ca6450e4c906",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Run the aggregated version of main logic (data loading & pre-processing) with the current number of workers\n",
    "try:\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read in the parquet files\n",
    "    df = spark.read.parquet(*parquet_path_list[:files])\n",
    "\n",
    "    # Run the aggregated version of the main logic\n",
    "    df_aggregated = run_aggregated(df)\n",
    "\n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate duration of the run\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    # Get cache information\n",
    "    memory_usage, disk_usage = get_cache_information()\n",
    "\n",
    "    # Get the average response time of a sql query\n",
    "    avg_req_time = average_sql_request_response_time(SQL_REQUEST_AGGREGATED)\n",
    "\n",
    "    # Combine test results of the current run\n",
    "    result = {\n",
    "        'status': 'COMPLETE',\n",
    "        'workers': number_of_workers,\n",
    "        'last_file_index': i - 1,\n",
    "        'duration': duration,\n",
    "        'memory_usage': memory_usage,\n",
    "        'disk_usage': disk_usage,\n",
    "        'avg_req_time': avg_req_time\n",
    "    }\n",
    "\n",
    "    print(f\"{number_of_workers} workers: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "\n",
    "# If an exception occurs, the test run should fail \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    result = {\n",
    "        'status': 'FAILED',\n",
    "        'workers': number_of_workers,\n",
    "        'last_file_index': i - 1,\n",
    "        'duration': 0,\n",
    "        'memory_usage': 0,\n",
    "        'disk_usage': 0,\n",
    "        'avg_req_time': 0\n",
    "    }\n",
    "\n",
    "# Test results are persisted every run to avoid losing them in case of a crash\n",
    "write_result_to_csv(result, 'test_ressources_aggregated.csv')\n",
    "\n",
    "# Remove from cache to prevent interference with the next run\n",
    "df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "id": "83df5be09dea548f",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Run the non-aggregated version of main logic (data loading & pre-processing) with the current number of workers\n",
    "try:\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read in the parquet files\n",
    "    df = spark.read.parquet(*parquet_path_list[:files])\n",
    "\n",
    "    # Run the non-aggregated version of the main logic\n",
    "    df_non_aggregated = run_non_aggregated(df)\n",
    "\n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate duration of the run\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    # Get cache information\n",
    "    memory_usage, disk_usage = get_cache_information()\n",
    "\n",
    "    # Get the average response time of a sql query\n",
    "    avg_req_time = average_sql_request_response_time(SQL_REQUEST_NON_AGGREGATED)\n",
    "\n",
    "    # Combine test results of the current run\n",
    "    result = {\n",
    "        'status': 'COMPLETE',\n",
    "        'workers': number_of_workers,\n",
    "        'last_file_index': i - 1,\n",
    "        'duration': duration,\n",
    "        'memory_usage': memory_usage,\n",
    "        'disk_usage': disk_usage,\n",
    "        'avg_req_time': avg_req_time\n",
    "    }\n",
    "\n",
    "    print(f\"{number_of_workers} workers: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "\n",
    "# If an exception occurs, the test run should fail \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    result = {\n",
    "        'status': 'FAILED',\n",
    "        'workers': number_of_workers,\n",
    "        'last_file_index': i - 1,\n",
    "        'duration': 0,\n",
    "        'memory_usage': 0,\n",
    "        'disk_usage': 0,\n",
    "        'avg_req_time': 0\n",
    "    }\n",
    "\n",
    "# Test results are persisted every run to avoid losing them in case of a crash\n",
    "write_result_to_csv(result, 'test_ressources_non_aggregated.csv')\n",
    "\n",
    "# Remove from cache to prevent interference with the next run\n",
    "df_non_aggregated.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
