{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cd59b29db59877",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tests\n",
    "\n",
    "In this notebook several tests are performed to analyze the scalability and fault tolerance of the application.\n",
    "\n",
    "To achieve comparable test results, all tests (if not stated otherwise) are executed on a local spark cluster with the following allocation of ressources:\n",
    "\n",
    "| Item               | Ressources             | Total Ressources (in Cluster) |\n",
    "|--------------------|------------------------|------------------------------|\n",
    "| Workers            | 3                      | 3                            |\n",
    "| Executors          | 2 per Worker           | 6                            |\n",
    "| RAM (in GB)        | 3 per Executor         | 18                           |\n",
    "| Cores              | 1 per Executor         | 6                            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221ff96b1fbbb464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:12.253890400Z",
     "start_time": "2024-01-30T23:32:12.118372700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')\n",
    "parquet_path = os.path.join(local_path, 'parquet_test')\n",
    "result_folder_path = os.path.join(os.getcwd(), 'test_results')\n",
    "\n",
    "spark_rest_api_url = \"http://localhost:4040/api/v1/applications\"\n",
    "\n",
    "# The same sql query, which is used by superset to display the heatmap (aggregated version)\n",
    "SQL_REQUEST_AGGREGATED = \"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               SUM(GoldsteinScaleSum)/SUM(EventCount) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT_AGGR) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\"\n",
    "\n",
    "# The same sql query, which is used by superset to display the heatmap (non-aggregated version)\n",
    "SQL_REQUEST_NON_AGGREGATED = \"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               AVG(GoldsteinScale) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT\n",
    "           WHERE ActionGeo_CountryCode IS NOT NULL\n",
    "             AND GoldsteinScale IS NOT NULL) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2416bbad8d173cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:12.274266500Z",
     "start_time": "2024-01-30T23:32:12.262520600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Choose time period for which to download the data\n",
    "start_date = datetime.strptime('2015-07-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2023-12-31', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28da7b24aeff3418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:13.261709700Z",
     "start_time": "2024-01-30T23:32:12.310617700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "url_list = []\n",
    "index = 0\n",
    "url_list.append([])\n",
    "month = date_list[0].month\n",
    "\n",
    "# Create a nested list containing a list of months with the corresponding download urls\n",
    "for date in date_list:\n",
    "    if date.month != month:\n",
    "        month = date.month\n",
    "        index += 1\n",
    "        url_list.append([])\n",
    "\n",
    "    # Create the url and append it to the month list\n",
    "    for x in range(0, 24):\n",
    "        for y in range(0, 60, 15):\n",
    "            date_tmp = date + timedelta(hours=x, minutes=y)\n",
    "            url = base_url + date_tmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "            url_list[index].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966a9a0250206cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:13.277250300Z",
     "start_time": "2024-01-30T23:32:13.265096800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the local directories if they don't exist yet\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)\n",
    "\n",
    "if not os.path.isdir(parquet_path):\n",
    "    os.mkdir(parquet_path)\n",
    "    \n",
    "if not os.path.isdir(result_folder_path):\n",
    "    os.mkdir(result_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5837aac79f353d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:17.338615600Z",
     "start_time": "2024-01-30T23:32:13.274426200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a spark session (see config folder for spark config)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb63d2c2c61662b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:17.409710900Z",
     "start_time": "2024-01-30T23:32:17.360311Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define original data schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), True),\n",
    "    StructField(\"Day\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad726d18d2771288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:17.416664700Z",
     "start_time": "2024-01-30T23:32:17.369611300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    folder_location = os.path.join(local_path, fname[:4], fname[4:6])\n",
    "\n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(folder_location, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(folder_location, fname))\n",
    "\n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(folder_location, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(folder_location)\n",
    "\n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(folder_location, fname))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf453f8e79a0608a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:17.666730200Z",
     "start_time": "2024-01-30T23:32:17.389978800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download files and write them to parquet files in parallel for each month\n",
    "# This is done in batches to allow simple addition of new months to already existing data\n",
    "i = 0\n",
    "for month_list in url_list:\n",
    "    # Skip month if parquet file already exists\n",
    "    if os.path.exists(os.path.join(parquet_path, str(i) + \".parquet\")):\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    year_folder = os.path.join(local_path, month_list[0].split('/')[-1][:4])\n",
    "    month_folder = os.path.join(year_folder, month_list[0].split('/')[-1][4:6])\n",
    "\n",
    "    if not os.path.isdir(year_folder):\n",
    "        os.mkdir(year_folder)\n",
    "\n",
    "    if not os.path.isdir(month_folder):\n",
    "        os.mkdir(month_folder)\n",
    "\n",
    "    # Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_file, month_list)\n",
    "\n",
    "    # Read all csv files of one month into a spark dataframe\n",
    "    df = spark.read.csv(month_folder, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "\n",
    "    # Write the data of one month into a parquet file\n",
    "    df.write.parquet(os.path.join(parquet_path, str(i) + \".parquet\"), mode='overwrite')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    # Delete the csv files to free up disk space\n",
    "    shutil.rmtree(month_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9e285222dfa5b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:22.117622400Z",
     "start_time": "2024-01-30T23:32:17.676092700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Get the csv and parquet file sizes via the input/output bytes of the stages from the spark rest api\n",
    "result_file_path = os.path.join(result_folder_path, 'data_size.csv')\n",
    "\n",
    "if not os.path.isdir(result_folder_path):\n",
    "    os.mkdir(result_folder_path)\n",
    "\n",
    "# Fetch the list of applications to get the application id\n",
    "apps_response = requests.get(spark_rest_api_url)\n",
    "apps = apps_response.json()\n",
    "app_id = apps[0]['id']\n",
    "\n",
    "# Get stages information for the application (1 stage per parquet file write)\n",
    "stages_response = requests.get(f\"{spark_rest_api_url}/{app_id}/stages\")\n",
    "stages_data = stages_response.json()\n",
    "\n",
    "stage_result = {}\n",
    "\n",
    "# Get necessary information of each stage\n",
    "for stage in stages_data:\n",
    "    stage_result[stage['stageId']] = {\n",
    "        'status': stage['status'],\n",
    "        'input_data': stage['inputBytes'],\n",
    "        'output_data': stage['outputBytes']\n",
    "    }\n",
    "    \n",
    "df_result = pd.DataFrame.from_dict(stage_result, orient='index')\n",
    "\n",
    "# Write the result to a csv file to use them later\n",
    "if not os.path.isfile(result_file_path):\n",
    "    df_result.to_csv(result_file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f9ccbf5639cd6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:22.119781200Z",
     "start_time": "2024-01-30T23:32:22.100111600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Method to run the non aggregated version of the program repeatedly in the test loop\n",
    "def run_non_aggregated(df_base):\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_non_aggregated = df_base.join(broadcast(df_mapping), df_base['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                     'left_outer')\n",
    "\n",
    "    df_non_aggregated = df_non_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_non_aggregated.cache()\n",
    "    df_non_aggregated.count()\n",
    "    df_non_aggregated.createOrReplaceGlobalTempView(\"GDELT\")\n",
    "    \n",
    "    return df_non_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33308ce9115c6ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:22.121972500Z",
     "start_time": "2024-01-30T23:32:22.114290400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to run the aggregated version of the program repeatedly in the test loop\n",
    "def run_aggregated(df_base):\n",
    "    # Select only relevant columns for the aggregation\n",
    "    df_selection = df_base.select(\n",
    "        F.col('Day'),\n",
    "        F.col('ActionGeo_CountryCode'),\n",
    "        F.col('GoldsteinScale')\n",
    "    )\n",
    "\n",
    "    # Remove rows that contain null values, which would distort the aggregation results \n",
    "    df_selection = df_selection.na.drop()\n",
    "    \n",
    "    # Aggregate the values by date and country so there is only one value per country per day\n",
    "    df_aggregated = df_selection.groupBy('Day', 'ActionGeo_CountryCode').agg(\n",
    "        F.sum('GoldsteinScale').alias('GoldsteinScaleSum'),\n",
    "        F.count('*').alias('EventCount')\n",
    "    )\n",
    "\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_aggregated = df_aggregated.join(broadcast(df_mapping),\n",
    "                                       df_aggregated['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                       'left_outer')\n",
    "\n",
    "    df_aggregated = df_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_aggregated.cache()\n",
    "    df_aggregated.count()\n",
    "    df_aggregated.createOrReplaceGlobalTempView(\"GDELT_AGGR\")\n",
    "    \n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "395334747b2bf948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:22.179550100Z",
     "start_time": "2024-01-30T23:32:22.128453100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Method to get the current cache information from the spark rest api\n",
    "def get_cache_information():\n",
    "    \n",
    "    # Fetch the list of applications to get the application id\n",
    "    apps_response = requests.get(spark_rest_api_url)\n",
    "    apps = apps_response.json()\n",
    "    app_id = apps[0]['id']\n",
    "    \n",
    "    # Get storage information for the application\n",
    "    storage_response = requests.get(f\"{spark_rest_api_url}/{app_id}/storage/rdd\")\n",
    "    storage_data = storage_response.json()\n",
    "    \n",
    "    # Only one dataframe is cached at a time, so only the first entry is relevant\n",
    "    return storage_data[0]['memoryUsed'], storage_data[0]['diskUsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8440c03e9b407fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:22.182200100Z",
     "start_time": "2024-01-30T23:32:22.133054200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "\n",
    "def average_sql_request_response_time(sql, n=10):\n",
    "    # Create connection to thrift server\n",
    "    with hive.connect(host='localhost', port=10000, username='spark') as connection:\n",
    "        request_start_time = time.time()\n",
    "\n",
    "        for _ in range(n):\n",
    "            # Send request to thrift server\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(sql)\n",
    "            result = cursor.fetchall()\n",
    "\n",
    "        request_end_time = time.time()\n",
    "\n",
    "    return (request_end_time - request_start_time) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5089c9a753e4ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:22.211013700Z",
     "start_time": "2024-01-30T23:32:22.171968700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def write_result_to_csv(result_dict, file_name):\n",
    "    \n",
    "    result_file_path = os.path.join(result_folder_path, file_name)\n",
    "    \n",
    "    df_result = pd.DataFrame([result_dict])\n",
    "\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(result_file_path)\n",
    "    \n",
    "    # Write the test result to a csv file, append if file exists, create new if it doesn't\n",
    "    df_result.to_csv(result_file_path, sep=';', mode='a' if file_exists else 'w', header=not file_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93339d3044765fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:26.389974700Z",
     "start_time": "2024-01-30T23:32:22.177806800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc77b63692b1dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling Data Volume\n",
    "\n",
    "The following tests are performed to analyze how increasing the data volume affects the performance of the application.\n",
    "\n",
    "Both versions of the program (aggregated and non aggregated) are repeatedly executed in a loop with an increasing amount of data. In each test run, performance metrics and cache information are collected and persisted to a csv file, to be used later for analysis.\n",
    "\n",
    "Contrary to `main.py` the aggregated and non-aggregated version are completely separated to test the individual performance of both versions.\n",
    "\n",
    "\n",
    "### Data Volume\n",
    "The data volume is increased by incrementing the number of parquet files (each representing a month of data) in every test run.\n",
    "\n",
    "There a are a total of 102 parquet files (8.5 years of data).\n",
    "\n",
    "Up to a full year of data (12 parquet files), tests are run in increments of 1 month (1 parquet file) to get a more granular view of the performance effects.\n",
    "Afterward, tests are run with increasing increments to analyze the performance effects at scale, while keeping the number of test runs low.\n",
    "The increment starts at 6 months and increases by 6 months with every test run (+6, +12, +18, ... parquet files), until the maximum of 8.5 years of data (102 parquet files) is reached.\n",
    "\n",
    "The tests are therefore conducted with the following numbers of parquet files: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 30, 48, 72, 102.\n",
    "\n",
    "### Performance\n",
    "The effect on the performance is analyzed by measuring different metrics:\n",
    "- **Pre-processing turnaround time**: The amount of time it takes to load the data into spark, conduct the necessary pre-processing & cache the results.\n",
    "- **Query response time**: The amount of time it takes to process and return the results of a single sql query, which is sent to the thrift server. An average of 10 sequential queries is calculated for more stable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1d9be6de28b6c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T23:32:26.410859600Z",
     "start_time": "2024-01-30T23:32:26.392548100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of all parquet file paths\n",
    "parquet_path_list = []\n",
    "\n",
    "# Number of parquet files in the directory\n",
    "for i in range(0, 102):\n",
    "    parquet_path_list.append(os.path.join(parquet_path, str(i) + \".parquet\"))\n",
    "\n",
    "parquet_file_test_cases = list(range(1, 13)) + [18, 30, 48, 72, 102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e6e395c751cd15e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T00:16:14.072588600Z",
     "start_time": "2024-01-30T23:32:26.400191200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 parquet files: duration: 17.910873889923096s, memory usage: 212624B, disk usage: 0B, avg req time: 0.3205996036529541s\n",
      "2 parquet files: duration: 11.786883115768433s, memory usage: 392008B, disk usage: 0B, avg req time: 0.21491703987121583s\n",
      "3 parquet files: duration: 14.532766342163086s, memory usage: 570008B, disk usage: 0B, avg req time: 0.22516844272613526s\n",
      "4 parquet files: duration: 20.72968816757202s, memory usage: 739032B, disk usage: 0B, avg req time: 0.2745798587799072s\n",
      "5 parquet files: duration: 24.38275408744812s, memory usage: 902656B, disk usage: 0B, avg req time: 0.2002878427505493s\n",
      "6 parquet files: duration: 27.8938992023468s, memory usage: 1078528B, disk usage: 0B, avg req time: 0.16792101860046388s\n",
      "7 parquet files: duration: 29.737545013427734s, memory usage: 1232880B, disk usage: 0B, avg req time: 0.13230364322662352s\n",
      "8 parquet files: duration: 30.786810159683228s, memory usage: 1387816B, disk usage: 0B, avg req time: 0.12997713088989257s\n",
      "9 parquet files: duration: 32.25099444389343s, memory usage: 1556184B, disk usage: 0B, avg req time: 0.12386527061462402s\n",
      "10 parquet files: duration: 34.38964653015137s, memory usage: 1727232B, disk usage: 0B, avg req time: 0.11478133201599121s\n",
      "11 parquet files: duration: 37.78990864753723s, memory usage: 1912824B, disk usage: 0B, avg req time: 0.11333491802215576s\n",
      "12 parquet files: duration: 40.895557165145874s, memory usage: 2063312B, disk usage: 0B, avg req time: 0.11834964752197266s\n",
      "18 parquet files: duration: 63.84113788604736s, memory usage: 2728784B, disk usage: 0B, avg req time: 0.11102850437164306s\n",
      "24 parquet files: duration: 86.43895173072815s, memory usage: 3451712B, disk usage: 0B, avg req time: 0.1050760269165039s\n",
      "30 parquet files: duration: 102.93086218833923s, memory usage: 4167904B, disk usage: 0B, avg req time: 0.12925372123718262s\n",
      "36 parquet files: duration: 91.01072597503662s, memory usage: 4846880B, disk usage: 0B, avg req time: 0.14287350177764893s\n",
      "42 parquet files: duration: 105.07836627960205s, memory usage: 5586328B, disk usage: 0B, avg req time: 0.15116634368896484s\n",
      "48 parquet files: duration: 120.36666941642761s, memory usage: 6250072B, disk usage: 0B, avg req time: 0.1306856870651245s\n",
      "54 parquet files: duration: 133.78375959396362s, memory usage: 6970952B, disk usage: 0B, avg req time: 0.1189932107925415s\n",
      "60 parquet files: duration: 148.54438853263855s, memory usage: 7654672B, disk usage: 0B, avg req time: 0.12518510818481446s\n",
      "66 parquet files: duration: 159.73500871658325s, memory usage: 8363992B, disk usage: 0B, avg req time: 0.10890445709228516s\n",
      "72 parquet files: duration: 173.06682920455933s, memory usage: 9001784B, disk usage: 0B, avg req time: 0.1138416051864624s\n",
      "78 parquet files: duration: 188.03518152236938s, memory usage: 9709040B, disk usage: 0B, avg req time: 0.11378064155578613s\n",
      "84 parquet files: duration: 202.07580828666687s, memory usage: 10364176B, disk usage: 0B, avg req time: 0.10736947059631348s\n",
      "90 parquet files: duration: 214.70373225212097s, memory usage: 11037120B, disk usage: 0B, avg req time: 0.10867528915405274s\n",
      "96 parquet files: duration: 229.84893202781677s, memory usage: 11718368B, disk usage: 0B, avg req time: 0.118764328956604s\n",
      "102 parquet files: duration: 243.43086171150208s, memory usage: 12407752B, disk usage: 0B, avg req time: 0.11687967777252198s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "exception_occured = False\n",
    "\n",
    "# Run the aggregated version of main logic (data loading & pre-processing) in a loop with increasing data volume\n",
    "for i in parquet_file_test_cases:\n",
    "    if not exception_occured:\n",
    "        try:    \n",
    "            # Start timer\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Read in the parquet files relevant for the current run\n",
    "            df = spark.read.parquet(*parquet_path_list[:i])\n",
    "                \n",
    "            # Run the aggregated version of the main logic\n",
    "            df_aggregated = run_aggregated(df)\n",
    "            \n",
    "            # Stop timer\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate duration of the run\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            # Get cache information\n",
    "            memory_usage, disk_usage = get_cache_information()\n",
    "            \n",
    "            # Get the average response time of a sql query\n",
    "            avg_req_time = average_sql_request_response_time(SQL_REQUEST_AGGREGATED)\n",
    "        \n",
    "            # Combine test results of the current run\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': i-1, # This is necessary to calculate the size of the data used for this test run\n",
    "                'duration': duration,\n",
    "                'memory_usage': memory_usage,\n",
    "                'disk_usage': disk_usage,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "            \n",
    "            print(f\"{i} parquet files: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "        \n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occured = True\n",
    "    \n",
    "    if exception_occured:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': i-1,\n",
    "            'duration': 0,\n",
    "            'memory_usage': 0,\n",
    "            'disk_usage': 0,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "        \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_data_volume_aggregated.csv')\n",
    "    \n",
    "    # Remove from cache to prevent interference with the next run\n",
    "    df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f61114596cbab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T00:16:13.666755700Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 parquet files: duration: 34.825024127960205s, memory usage: 1273540248B, disk usage: 0B, avg req time: 0.2940074443817139s\n",
      "2 parquet files: duration: 50.998690605163574s, memory usage: 2475682816B, disk usage: 0B, avg req time: 0.4139133930206299s\n",
      "3 parquet files: duration: 62.1660943031311s, memory usage: 3727293664B, disk usage: 0B, avg req time: 0.5287338018417358s\n",
      "4 parquet files: duration: 78.8647871017456s, memory usage: 4952817680B, disk usage: 0B, avg req time: 0.651429009437561s\n",
      "5 parquet files: duration: 96.7095856666565s, memory usage: 6191491184B, disk usage: 0B, avg req time: 0.7858332872390748s\n",
      "6 parquet files: duration: 114.00220370292664s, memory usage: 7367461568B, disk usage: 0B, avg req time: 0.9434693098068238s\n",
      "7 parquet files: duration: 129.62822437286377s, memory usage: 8589660192B, disk usage: 0B, avg req time: 1.1007545709609985s\n",
      "8 parquet files: duration: 150.91743516921997s, memory usage: 9859052432B, disk usage: 0B, avg req time: 1.905244731903076s\n",
      "9 parquet files: duration: 174.4083240032196s, memory usage: 9659728456B, disk usage: 1146662896B, avg req time: 2.625724506378174s\n",
      "10 parquet files: duration: 192.90184354782104s, memory usage: 9766891456B, disk usage: 2050651496B, avg req time: 3.322082018852234s\n",
      "11 parquet files: duration: 212.58268880844116s, memory usage: 9737988312B, disk usage: 3073940772B, avg req time: 3.8246331214904785s\n",
      "12 parquet files: duration: 237.58019280433655s, memory usage: 9707609416B, disk usage: 4072876510B, avg req time: 5.491439604759217s\n",
      "18 parquet files: duration: 360.038635969162s, memory usage: 9817580784B, disk usage: 9742041150B, avg req time: 12.842222476005555s\n",
      "24 parquet files: duration: 483.2054355144501s, memory usage: 9902984824B, disk usage: 15256685868B, avg req time: 16.397543025016784s\n",
      "30 parquet files: duration: 592.9723589420319s, memory usage: 9848049968B, disk usage: 20179497599B, avg req time: 21.608130478858946s\n",
      "36 parquet files: duration: 670.2361326217651s, memory usage: 9996854664B, disk usage: 25058018384B, avg req time: 25.070871567726137s\n",
      "42 parquet files: duration: 766.6207828521729s, memory usage: 9844528808B, disk usage: 29895272978B, avg req time: 28.738381767272948s\n",
      "48 parquet files: duration: 869.1880326271057s, memory usage: 9787184200B, disk usage: 34540547789B, avg req time: 33.59104568958283s\n",
      "54 parquet files: duration: 952.0325264930725s, memory usage: 9929612928B, disk usage: 38684723637B, avg req time: 35.70017638206482s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "exception_occured = False\n",
    "\n",
    "# Run the non-aggregated version of main logic (data loading & pre-processing) in a loop with increasing data volume\n",
    "for i in parquet_file_test_cases:\n",
    "    if not exception_occured:\n",
    "        try:\n",
    "            # Start timer\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Read in the parquet files relevant for the current run\n",
    "            df = spark.read.parquet(*parquet_path_list[:i])\n",
    "\n",
    "            # Run the non aggregated version of the main logic\n",
    "            df_non_aggregated = run_non_aggregated(df)\n",
    "\n",
    "            # Stop timer\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate duration of the run\n",
    "            duration = end_time - start_time\n",
    "\n",
    "            # Get cache information\n",
    "            memory_usage, disk_usage = get_cache_information()\n",
    "\n",
    "            # Get the average response time of a sql query\n",
    "            avg_req_time = average_sql_request_response_time(SQL_REQUEST_NON_AGGREGATED)\n",
    "\n",
    "            # Combine test results of the current run\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': i - 1,  # This is necessary to calculate the size of the data used for this test run\n",
    "                'duration': duration,\n",
    "                'memory_usage': memory_usage,\n",
    "                'disk_usage': disk_usage,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "\n",
    "            print(f\"{i} parquet files: Duration: {duration}s, memory usage: {memory_usage}B, disk usage: {disk_usage}B, average response time: {avg_req_time}s\")\n",
    "\n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occured = True\n",
    "\n",
    "    if exception_occured:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': i - 1,\n",
    "            'duration': 0,\n",
    "            'memory_usage': 0,\n",
    "            'disk_usage': 0,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "\n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_data_volume_non_aggregated.csv')\n",
    "\n",
    "    # Remove from cache to prevent interference with the next run\n",
    "    df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cb55f2702794b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling Load\n",
    "\n",
    "The following tests are performed to analyze how an increased load influences the performance of the application.\n",
    "\n",
    "In the given use case, an increasing amount of users using the dashboard at the same time or an increasing amount of dashboard elements (e.g. charts, tables, etc.) could be considered an increase in load. In both cases the number of concurrent queries, that are sent from superset to the thrift server, would increase. For that reason, we define load as the number of queries that are sent to the thrift server in parallel.\n",
    "\n",
    "Once the program is executed and the data is cached, an incrementing number of queries are sent to the thrift server in parallel. In each test run performance metrics are measured and persisted to a csv file. The tests are executed for both versions of the program (aggregated and non aggregated) separately.\n",
    "\n",
    "\n",
    "### Load\n",
    "The number of queries sent to the thrift server in parallel is increased in increments of 10, starting at 10 up to 100. Afterward, it is increased in increments of 50, starting at 150 up to 500. Finally, the number of queries is increased in increments of 100, starting at 600 up to 1000.\n",
    "\n",
    "To simulate, that queries are sent in parallel (e.g. by different users), a separate thread is started for each query. Every thread establishes an isolated connection to the thrift server and sends a single query.\n",
    "\n",
    "\n",
    "### Performance\n",
    "The effect on the performance is analyzed by measuring the the **average query response time**, which is the amount of time it takes to process and return the results of a sql query sent to the thrift server under the given load.\n",
    "\n",
    "### Data Volume\n",
    "The data volume is kept constant to test the effect of additional load in an isolated way. 1 parquet file (1 month of data) is used for all test runs to ensure that all cached data fits into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb7b579b5dccb9",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import queue\n",
    "\n",
    "\n",
    "def run_query(sql, data_queue, stop_event):\n",
    "    # Send a single query to the thrift server and store the response time\n",
    "    try:\n",
    "        if not stop_event.is_set():\n",
    "            req_time = average_sql_request_response_time(sql, 1)\n",
    "            data_queue.put(req_time)\n",
    "    # Stop other threads, when an exception occurs and save the exception\n",
    "    except Exception as e:\n",
    "        data_queue.put(e)\n",
    "        stop_event.set()\n",
    "\n",
    "\n",
    "def run_queries_parallel(sql, n):\n",
    "    threads = []\n",
    "    data_queue = queue.Queue()\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    # Start n threads to run n queries in parallel\n",
    "    for _ in range(n):\n",
    "        t = threading.Thread(target=run_query, args=(sql, data_queue, stop_event))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    # Wait until all threads are finished\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    # Calculate the average response time\n",
    "    total = 0\n",
    "    while not data_queue.empty():\n",
    "        result = data_queue.get()\n",
    "        if isinstance(result, Exception):\n",
    "            # If an exception occured in one of the threads, the test run should fail\n",
    "            raise result\n",
    "        total += result\n",
    "\n",
    "    return total / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea03ea4e916485",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Number of parquet files used in the test\n",
    "files = 1\n",
    "\n",
    "# Number of queries sent in parallel\n",
    "query_test_cases = list(range(10, 101, 10)) + list(range(150, 501, 50)) + list(range(600, 1001, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82c0191a3b5a0b",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Run the aggregated version of the program as measurement object for the load tests\n",
    "df = spark.read.parquet(*parquet_path_list[:files])\n",
    "df_aggregated = run_aggregated(df)\n",
    "\n",
    "exception_occured = False\n",
    "\n",
    "for i in query_test_cases:\n",
    "    if not exception_occured:\n",
    "        # Execute n queries in parallel and calculate the average response time\n",
    "        try:\n",
    "            avg_req_time = run_queries_parallel(SQL_REQUEST_AGGREGATED, i)\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': files-1,\n",
    "                'parallel_queries': i,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "            print(f\"Average response time with {i} parallel queries: {avg_req_time}s\")\n",
    "            \n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occured = True\n",
    "    \n",
    "    if exception_occured:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': files-1,\n",
    "            'parallel_queries': i,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "        \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_load_aggregated.csv')\n",
    "    \n",
    "df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737bdd8e2ac1bab",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Run the non-aggregated version of the program as measurement objects for the load tests\n",
    "df = spark.read.parquet(*parquet_path_list[:files])\n",
    "df_non_aggregated = run_non_aggregated(df)\n",
    "\n",
    "exception_occured = False\n",
    "\n",
    "for i in query_test_cases:\n",
    "    if not exception_occured:\n",
    "        # Execute n queries in parallel and calculate the average response time\n",
    "        try:\n",
    "            avg_req_time = run_queries_parallel(SQL_REQUEST_NON_AGGREGATED, i)\n",
    "            result = {\n",
    "                'status': 'COMPLETE',\n",
    "                'last_file_index': files-1,\n",
    "                'parallel_queries': i,\n",
    "                'avg_req_time': avg_req_time\n",
    "            }\n",
    "            print(f\"Average response time with {i} parallel queries: {avg_req_time}s\")\n",
    "            \n",
    "        # If an exception occurs, the test run should fail \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            exception_occured = True\n",
    "    \n",
    "    if exception_occured:\n",
    "        result = {\n",
    "            'status': 'FAILED',\n",
    "            'last_file_index': files-1,\n",
    "            'parallel_queries': i,\n",
    "            'avg_req_time': 0\n",
    "        }\n",
    "            \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_load_non_aggregated.csv')\n",
    "\n",
    "df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96620b51c64648",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scaling Ressources"
   ]
  },
  {
   "cell_type": "code",
   "id": "83df5be09dea548f",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
