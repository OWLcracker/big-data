{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "221ff96b1fbbb464",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T12:29:08.320049900Z",
     "start_time": "2024-01-30T12:29:08.300747100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), 'data')\n",
    "parquet_path = os.path.join(local_path, 'parquet_test')\n",
    "result_folder_path = os.path.join(os.getcwd(), 'test_results')\n",
    "\n",
    "spark_rest_api_url = \"http://localhost:4040/api/v1/applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2416bbad8d173cf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T12:29:09.441877100Z",
     "start_time": "2024-01-30T12:29:09.438174200Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Choose time period for which to download the data\n",
    "start_date = datetime.strptime('2015-07-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2023-12-31', '%Y-%m-%d')\n",
    "\n",
    "# Create a list of dates between start_date and end_date\n",
    "date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28da7b24aeff3418",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T12:29:10.948187400Z",
     "start_time": "2024-01-30T12:29:09.701769300Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create a list containing download urls for each date\n",
    "base_url = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "url_list = []\n",
    "index = 0\n",
    "url_list.append([])\n",
    "month = date_list[0].month\n",
    "\n",
    "# Create a nested list containing a list of months with the corresponding download urls\n",
    "for date in date_list:\n",
    "    if date.month != month:\n",
    "        month = date.month\n",
    "        index += 1\n",
    "        url_list.append([])\n",
    "\n",
    "    # Create the url and append it to the month list\n",
    "    for x in range(0, 24):\n",
    "        for y in range(0, 60, 15):\n",
    "            date_tmp = date + timedelta(hours=x, minutes=y)\n",
    "            url = base_url + date_tmp.strftime('%Y%m%d%H%M%S') + '.export.CSV.zip'\n",
    "            url_list[index].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "966a9a0250206cf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T12:29:11.732973600Z",
     "start_time": "2024-01-30T12:29:11.721092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the local directories if they don't exist yet\n",
    "if not os.path.isdir(local_path):\n",
    "    os.mkdir(local_path)\n",
    "\n",
    "if not os.path.isdir(parquet_path):\n",
    "    os.mkdir(parquet_path)\n",
    "    \n",
    "if not os.path.isdir(result_folder_path):\n",
    "    os.mkdir(result_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5837aac79f353d0",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-30T12:29:12.507038600Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a spark session (see config folder for spark config)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Big Data Project') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb63d2c2c61662b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Define original data schema for csv files\n",
    "schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), True),\n",
    "    StructField(\"Day\", DateType(), True),\n",
    "    StructField(\"MonthYear\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", StringType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad726d18d2771288",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    folder_location = os.path.join(local_path, fname[:4], fname[4:6])\n",
    "\n",
    "    # Download file from the specified url, if it doesn't exist yet\n",
    "    if not os.path.isfile(os.path.join(folder_location, fname).replace(\".zip\", \"\")):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(folder_location, fname))\n",
    "\n",
    "            # Unzip zip file\n",
    "            with zipfile.ZipFile(os.path.join(folder_location, fname), 'r') as zip_ref:\n",
    "                zip_ref.extractall(folder_location)\n",
    "\n",
    "            # Delete zip file\n",
    "            os.remove(os.path.join(folder_location, fname))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with file {fname}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print('File ' + fname + ' already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf453f8e79a0608a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Download files and write them to parquet files in parallel for each month\n",
    "# This is done in batches to allow simple addition of new months to already existing data\n",
    "i = 0\n",
    "for month_list in url_list:\n",
    "    # Skip month if parquet file already exists\n",
    "    if os.path.exists(os.path.join(parquet_path, str(i) + \".parquet\")):\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    year_folder = os.path.join(local_path, month_list[0].split('/')[-1][:4])\n",
    "    month_folder = os.path.join(year_folder, month_list[0].split('/')[-1][4:6])\n",
    "\n",
    "    if not os.path.isdir(year_folder):\n",
    "        os.mkdir(year_folder)\n",
    "\n",
    "    if not os.path.isdir(month_folder):\n",
    "        os.mkdir(month_folder)\n",
    "\n",
    "    # Download all files from the url list in parallel (threads = no. processors on machine * 5)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_file, month_list)\n",
    "\n",
    "    # Read all csv files of one month into a spark dataframe\n",
    "    df = spark.read.csv(month_folder, sep='\\t', header=False, schema=schema, dateFormat='yyyyMMdd')\n",
    "\n",
    "    # Write the data of one month into a parquet file\n",
    "    df.write.parquet(os.path.join(parquet_path, str(i) + \".parquet\"), mode='overwrite')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    # Delete the csv files to free up disk space\n",
    "    shutil.rmtree(month_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e285222dfa5b51",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Get the csv and parquet file sizes via the input/output bytes of the stages from the spark rest api\n",
    "result_file_path = os.path.join(result_folder_path, 'data_size.csv')\n",
    "\n",
    "if not os.path.isdir(result_folder_path):\n",
    "    os.mkdir(result_folder_path)\n",
    "\n",
    "# Fetch the list of applications to get the application id\n",
    "apps_response = requests.get(spark_rest_api_url)\n",
    "apps = apps_response.json()\n",
    "app_id = apps[0]['id']\n",
    "\n",
    "# Get stages information for the application (1 stage per parquet file write)\n",
    "stages_response = requests.get(f\"{spark_rest_api_url}/{app_id}/stages\")\n",
    "stages_data = stages_response.json()\n",
    "\n",
    "stage_result = {}\n",
    "\n",
    "# Get necessary information of each stage\n",
    "for stage in stages_data:\n",
    "    stage_result[stage['stageId']] = {\n",
    "        'status': stage['status'],\n",
    "        'input_data': stage['inputBytes'],\n",
    "        'output_data': stage['outputBytes']\n",
    "    }\n",
    "    \n",
    "df_result = pd.DataFrame.from_dict(stage_result, orient='index')\n",
    "\n",
    "# Write the result to a csv file to use them later\n",
    "if not os.path.isfile(result_file_path):\n",
    "    df_result.to_csv(result_file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9ccbf5639cd6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Method to run the non aggregated version of the program repeatedly in the test loop\n",
    "def run_non_aggregated(df_base):\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_non_aggregated = df_base.join(broadcast(df_mapping), df_base['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                     'left_outer')\n",
    "\n",
    "    df_non_aggregated = df_non_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_non_aggregated.cache()\n",
    "    df_non_aggregated.count()\n",
    "    df_non_aggregated.createOrReplaceGlobalTempView(\"GDELT\")\n",
    "    \n",
    "    return df_non_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33308ce9115c6ec0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to run the aggregated version of the program repeatedly in the test loop\n",
    "def run_aggregated(df_base):\n",
    "    # Select only relevant columns for the aggregation\n",
    "    df_selection = df_base.select(\n",
    "        F.col('Day'),\n",
    "        F.col('ActionGeo_CountryCode'),\n",
    "        F.col('GoldsteinScale')\n",
    "    )\n",
    "\n",
    "    # Remove rows that contain null values, which would distort the aggregation results \n",
    "    df_selection = df_selection.na.drop()\n",
    "    \n",
    "    # Aggregate the values by date and country so there is only one value per country per day\n",
    "    df_aggregated = df_selection.groupBy('Day', 'ActionGeo_CountryCode').agg(\n",
    "        F.sum('GoldsteinScale').alias('GoldsteinScaleSum'),\n",
    "        F.count('*').alias('EventCount')\n",
    "    )\n",
    "\n",
    "    # CSV file containing a mapping from FIPS10-4 country codes to ISO 3166-1 alpha-2 country codes (necessary for superset heatmap)\n",
    "    mapping_file_path = os.path.join(os.getcwd(), 'util', 'country_code_mapping.csv')\n",
    "\n",
    "    # Load mapping file outside of spark (small dataset)\n",
    "    df_mapping = spark.read.csv(mapping_file_path, sep=';', header=True, inferSchema=True).select(\n",
    "        F.col('FIPS 10-4'),\n",
    "        F.col('ISO 3166-1')\n",
    "    )\n",
    "\n",
    "    # Map the country codes\n",
    "    df_aggregated = df_aggregated.join(broadcast(df_mapping),\n",
    "                                       df_aggregated['ActionGeo_CountryCode'] == df_mapping['FIPS 10-4'],\n",
    "                                       'left_outer')\n",
    "\n",
    "    df_aggregated = df_aggregated \\\n",
    "        .withColumn('ActionGeo_CountryCode', F.col('ISO 3166-1')) \\\n",
    "        .drop('ISO 3166-1') \\\n",
    "        .drop('FIPS 10-4')\n",
    "\n",
    "    # Load data, trigger caching and create a global temp view (as it would be necessary to use the data with superset)\n",
    "    df_aggregated.cache()\n",
    "    df_aggregated.count()\n",
    "    df_aggregated.createOrReplaceGlobalTempView(\"GDELT_AGGR\")\n",
    "    \n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395334747b2bf948",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Method to get the current cache information from the spark rest api\n",
    "def get_cache_information():\n",
    "    \n",
    "    # Fetch the list of applications to get the application id\n",
    "    apps_response = requests.get(spark_rest_api_url)\n",
    "    apps = apps_response.json()\n",
    "    app_id = apps[0]['id']\n",
    "    \n",
    "    # Get storage information for the application\n",
    "    storage_response = requests.get(f\"{spark_rest_api_url}/{app_id}/storage/rdd\")\n",
    "    storage_data = storage_response.json()\n",
    "    \n",
    "    # Only one dataframe is cached at a time, so only the first entry is relevant\n",
    "    return storage_data[0]['memoryUsed'], storage_data[0]['diskUsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440c03e9b407fb3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "\n",
    "def average_sql_request_execution_time(sql, n=10):\n",
    "    with hive.connect(host='localhost', port=10000, username='spark') as connection:\n",
    "        request_start_time = time.time()\n",
    "\n",
    "        for _ in range(n):\n",
    "            pd.read_sql(sql=sql, con=connection)\n",
    "\n",
    "        request_end_time = time.time()\n",
    "\n",
    "    return (request_end_time - request_start_time) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5089c9a753e4ba1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def write_result_to_csv(result_dict, file_name):\n",
    "    \n",
    "    result_file_path = os.path.join(result_folder_path, file_name)\n",
    "    \n",
    "    df_result = pd.DataFrame([result_dict])\n",
    "\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(result_file_path)\n",
    "    \n",
    "    # Write the test result to a CSV file, append if file exists, create new if it doesn't\n",
    "    df_result.to_csv(result_file_path, sep=';', mode='a' if file_exists else 'w', header=not file_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93339d3044765fd7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Retrieve the spark context from the current spark session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Import the HiveThriftServer2 class using the JVM instance of the spark context\n",
    "java_import(sc._jvm, \"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\")\n",
    "\n",
    "# Dummy java arguments for main method\n",
    "java_args = sc._gateway.new_array(sc._gateway.jvm.java.lang.String, 0)\n",
    "\n",
    "# Start the thrift server by calling the main method of the imported class\n",
    "sc._jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(java_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scaling Data Volume\n",
    "\n",
    "The following tests are performed to analyze how increasing the data volume affects the performance of the program.\n",
    "Both versions of the program (aggregated and non aggregated) are repeatedly executed in a loop with an increasing amount of data.\n",
    "In each test run, performance metrics and cache information are collected and persisted to a csv file, to be used later for analysis.\n",
    "\n",
    "Contrary to `main.py` the aggregated and non-aggregated version are completely separated to test the individual performance of both versions.\n",
    "\n",
    "### Ressource allocation\n",
    "To analyze how increasing the data volume affects the performance in an isolated way, the ressources allocated to the spark cluster remain constant through the test runs.\n",
    "The following ressources are allocated to the spark cluster:\n",
    "\n",
    "\n",
    "| Configuration Item    | Ressources             | Total Ressources (in Cluster) |\n",
    "|-----------------------|------------------------|-------------------------------|\n",
    "| Workers               | 3                      | 3                             |\n",
    "| Executors             | 2 per Worker           | 6                             |\n",
    "| RAM                   | 3 GB per Executor      | 18 GB                         |\n",
    "| Cores                 | 1 per Executor         | 6                             |\n",
    "\n",
    "\n",
    "### Data Volume\n",
    "The data volume is increased by incrementing the number of parquet files (each representing a month of data) in every test run.\n",
    "\n",
    "There a are a total of 102 parquet files (8.5 years of data).\n",
    "\n",
    "Up to a full year of data (12 parquet files), tests are run in increments of 1 month (1 parquet file) to get a more granular view of the performance effects.\n",
    "Afterwards, tests are run with increasing increments analyze the performance effects at scale, while keeping the number of test runs low.\n",
    "The increment starts a 6 months and increases by 6 months with every test run (+6, +12, +18 parquet files), until the maximum of 8.5 years of data (102 parquet files) is reached.\n",
    "\n",
    "The tests are therefore conducted with the following numbers of parquet files: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 30, 48, 72, 102."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73fc77b63692b1dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a list of all parquet file paths\n",
    "parquet_path_list = []\n",
    "\n",
    "# Number of parquet files in the directory (-1 because of .gitkeep)\n",
    "for i in range(0, 102):\n",
    "    parquet_path_list.append(os.path.join(parquet_path, str(i) + \".parquet\"))\n",
    "\n",
    "parquet_file_test_cases = list(range(1, 13)) + [18, 30, 48, 72, 102]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T12:27:16.432455200Z",
     "start_time": "2024-01-30T12:27:16.420292500Z"
    }
   },
   "id": "a1d9be6de28b6c84",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e395c751cd15e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Run the aggregated version of main logic (data loading & pre-processing) in a loop with increasing data volume\n",
    "for i in parquet_file_test_cases:\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read in the parquet files relevant for the current run\n",
    "    df = spark.read.parquet(*parquet_path_list[:i])\n",
    "    \n",
    "    # Run the aggregated version of the main logic\n",
    "    df_aggregated = run_aggregated(df)\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate duration of the run\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Get cache information\n",
    "    memory_usage, disk_usage = get_cache_information()\n",
    "    \n",
    "    # Get the average execution time of the sql query, which is used by superset to display the heatmap\n",
    "    avg_sql_time = average_sql_request_execution_time(\"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               SUM(GoldsteinScaleSum)/SUM(EventCount) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT_AGGR) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\")\n",
    "\n",
    "    # Combine test results of the current run\n",
    "    result = {\n",
    "        'last_file_index': i-1, # This is necessary to calculate the size of the data used for this test run\n",
    "        'duration': duration,\n",
    "        'memory_usage': memory_usage,\n",
    "        'disk_usage': disk_usage,\n",
    "        'avg_sql_time': avg_sql_time\n",
    "    }\n",
    "    \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_result_aggregated.csv')\n",
    "    \n",
    "    df_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f61114596cbab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "result_non_aggregated = {}\n",
    "\n",
    "# Run the non-aggregated version of main logic (data loading & pre-processing) in a loop with increasing data volume\n",
    "for i in parquet_file_test_cases:\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read in the parquet files relevant for the current run\n",
    "    df = spark.read.parquet(*parquet_path_list[:i])\n",
    "    \n",
    "    # Run the non aggregated version of the main logic\n",
    "    df_non_aggregated = run_non_aggregated(df)\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate duration of the run\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Get cache information\n",
    "    memory_usage, disk_usage = get_cache_information()\n",
    "    \n",
    "    # Get the average execution time of the sql query, which is used by superset to display the heatmap\n",
    "    avg_sql_time = average_sql_request_execution_time(\"\"\"\n",
    "        SELECT ActionGeo_CountryCode AS ActionGeo_CountryCode,\n",
    "               AVG(GoldsteinScale) AS GoldsteinScaleAvg\n",
    "        FROM\n",
    "          (SELECT *\n",
    "           FROM global_temp.GDELT\n",
    "           WHERE ActionGeo_CountryCode IS NOT NULL\n",
    "             AND GoldsteinScale IS NOT NULL) AS virtual_table\n",
    "        GROUP BY ActionGeo_CountryCode\n",
    "    \"\"\")\n",
    "    \n",
    "    # Combine test results of the current run\n",
    "    result = {\n",
    "        'last_file_index': i-1, # This is necessary to calculate the size of the data used for this test run\n",
    "        'duration': duration,\n",
    "        'memory_usage': memory_usage,\n",
    "        'disk_usage': disk_usage,\n",
    "        'avg_sql_time': avg_sql_time\n",
    "    }\n",
    "    \n",
    "    # Test results are persisted every run to avoid losing them in case of a crash\n",
    "    write_result_to_csv(result, 'test_result_non_aggregated.csv')\n",
    "\n",
    "    df_non_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b45d4f1fae73f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T21:34:29.493037400Z",
     "start_time": "2024-01-29T21:34:29.484413300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to calculate the size of the data used for the respective test run\n",
    "def calculate_data_size(row, df_data_size):\n",
    "    # Index of the last file used for this test\n",
    "    last_file_index = row['last_file_index']\n",
    "\n",
    "    # Filter for all files which were used to perform this test\n",
    "    relevant_data = df_data_size[df_data_size.index <= last_file_index]\n",
    "\n",
    "    csv_size = relevant_data['input_data'].sum()\n",
    "    parquet_size = relevant_data['output_data'].sum()\n",
    "\n",
    "    return csv_size, parquet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aecd9cefecbad20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T21:36:16.532485200Z",
     "start_time": "2024-01-29T21:36:16.516710300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_result_aggregated = pd.read_csv(os.path.join(result_folder_path, 'test_result_aggregated.csv'), sep=';', index_col=0)\n",
    "df_result_non_aggregated = pd.read_csv(os.path.join(result_folder_path, 'test_result_non_aggregated'), sep=';',\n",
    "                                       index_col=0)\n",
    "\n",
    "# Combine the results of the aggregated and non aggregated version into one dataframe\n",
    "df_result = pd.concat([df_result_aggregated, df_result_non_aggregated], axis=1, keys=['aggregated', 'non_aggregated'])\n",
    "\n",
    "df_data_size = pd.read_csv(os.path.join(result_folder_path, 'data_size.csv'), sep=';', index_col=0)\n",
    "\n",
    "# Calculate the size of the data used for each test run\n",
    "df_result[['csv_size', 'parquet_size']] = df_result.apply(lambda row: calculate_data_size(row, df_data_size), axis=1,\n",
    "                                                          result_type='expand')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
