{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:51.071396400Z",
     "start_time": "2023-12-26T09:54:50.137683200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "\n",
    "connection=hive.connect(host='localhost', port='10000',username='spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2b725f32b7bb4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T09:58:21.261747100Z",
     "start_time": "2023-12-26T09:58:21.224146300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_557/3825761271.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_tables=pd.read_sql(con=connection,sql=\"show tables\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [namespace, tableName, isTemporary]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tables=pd.read_sql(con=connection,sql=\"show tables\")\n",
    "df_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7bd42a806b0abad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_557/3848745893.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_cities=pd.read_sql(con=connection,sql=\"SELECT * FROM WorldCities\")\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql: SELECT * FROM WorldCities\nTExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `WorldCities` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [WorldCities], [], false\\n:37:36\", 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'jdk.internal.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:77', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:568', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'jdk.proxy2.$Proxy94:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:833', \"*org.apache.spark.sql.catalyst.ExtendedAnalysisException:[TABLE_OR_VIEW_NOT_FOUND] The table or view `WorldCities` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [WorldCities], [], false\\n:77:41\", 'org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt:tableNotFound:package.scala:87', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2:CheckAnalysis.scala:202', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2$adapted:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:244', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:243', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0$:CheckAnalysis.scala:164', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis0:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis:CheckAnalysis.scala:160', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis$:CheckAnalysis.scala:150', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:211', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:330', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:208', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:77', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:138', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$2:QueryExecution.scala:219', 'org.apache.spark.sql.execution.QueryExecution$:withInternalError:QueryExecution.scala:546', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:219', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:218', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:77', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:74', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:66', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:99', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:97', 'org.apache.spark.sql.SparkSession:$anonfun$sql$4:SparkSession.scala:691', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:682', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:713', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:744', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:227'], sqlState='42P01', errorCode=0, errorMessage=\"Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `WorldCities` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [WorldCities], [], false\\n\"), operationHandle=None)\nunable to rollback",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:2202\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2202\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyhive/hive.py:481\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mExecuteStatement(req)\n\u001b[0;32m--> 481\u001b[0m \u001b[43m_check_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operationHandle \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39moperationHandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyhive/hive.py:611\u001b[0m, in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mstatusCode \u001b[38;5;241m!=\u001b[39m ttypes\u001b[38;5;241m.\u001b[39mTStatusCode\u001b[38;5;241m.\u001b[39mSUCCESS_STATUS:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationalError(response)\n",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `WorldCities` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [WorldCities], [], false\\n:37:36\", 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'jdk.internal.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:77', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:568', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'jdk.proxy2.$Proxy94:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:833', \"*org.apache.spark.sql.catalyst.ExtendedAnalysisException:[TABLE_OR_VIEW_NOT_FOUND] The table or view `WorldCities` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [WorldCities], [], false\\n:77:41\", 'org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt:tableNotFound:package.scala:87', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2:CheckAnalysis.scala:202', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2$adapted:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:244', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:243', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0$:CheckAnalysis.scala:164', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis0:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis:CheckAnalysis.scala:160', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis$:CheckAnalysis.scala:150', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:211', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:330', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:208', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:77', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:138', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$2:QueryExecution.scala:219', 'org.apache.spark.sql.execution.QueryExecution$:withInternalError:QueryExecution.scala:546', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:219', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:218', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:77', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:74', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:66', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:99', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:97', 'org.apache.spark.sql.SparkSession:$anonfun$sql$4:SparkSession.scala:691', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:682', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:713', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:744', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:227'], sqlState='42P01', errorCode=0, errorMessage=\"Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `WorldCities` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [WorldCities], [], false\\n\"), operationHandle=None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:2206\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyhive/hive.py:358\u001b[0m, in \u001b[0;36mConnection.rollback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrollback\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotSupportedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHive does not have transactions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotSupportedError\u001b[0m: Hive does not have transactions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_cities\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM WorldCities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:635\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 635\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    643\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    647\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:2266\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2257\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2266\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2267\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:2211\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     ex \u001b[38;5;241m=\u001b[39m DatabaseError(\n\u001b[1;32m   2209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124munable to rollback\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2210\u001b[0m     )\n\u001b[0;32m-> 2211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2213\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql: SELECT * FROM WorldCities\nTExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `WorldCities` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [WorldCities], [], false\\n:37:36\", 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'jdk.internal.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:77', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:568', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'jdk.proxy2.$Proxy94:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:833', \"*org.apache.spark.sql.catalyst.ExtendedAnalysisException:[TABLE_OR_VIEW_NOT_FOUND] The table or view `WorldCities` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [WorldCities], [], false\\n:77:41\", 'org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt:tableNotFound:package.scala:87', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2:CheckAnalysis.scala:202', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2$adapted:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:244', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:243', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0$:CheckAnalysis.scala:164', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis0:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis:CheckAnalysis.scala:160', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis$:CheckAnalysis.scala:150', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:211', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:330', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:208', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:77', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:138', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$2:QueryExecution.scala:219', 'org.apache.spark.sql.execution.QueryExecution$:withInternalError:QueryExecution.scala:546', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:219', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:218', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:77', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:74', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:66', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:99', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:97', 'org.apache.spark.sql.SparkSession:$anonfun$sql$4:SparkSession.scala:691', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:682', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:713', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:744', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:227'], sqlState='42P01', errorCode=0, errorMessage=\"Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `WorldCities` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [WorldCities], [], false\\n\"), operationHandle=None)\nunable to rollback"
     ]
    }
   ],
   "source": [
    "df_cities=pd.read_sql(con=connection,sql=\"SELECT * FROM WorldCities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00b1e0c005a7b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T09:59:43.793938500Z",
     "start_time": "2023-12-26T09:59:43.766817700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e433d43b29afc7bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T10:29:03.432550100Z",
     "start_time": "2023-12-26T10:29:03.419445900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import *\n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy.schema import *\n",
    "\n",
    "engine = create_engine('hive://localhost:10000/default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c227288b61454bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T10:29:03.806934600Z",
     "start_time": "2023-12-26T10:29:03.698361100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with engine.connect() as con:\n",
    "    result = con.execute(text(\"show tables\"))\n",
    "result.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "873c6ed736374eac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T10:29:05.551893300Z",
     "start_time": "2023-12-26T10:29:05.514481600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e0e8c97402335",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
